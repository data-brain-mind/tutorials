<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://data-brain-mind.github.io/tutorials/feed.xml" rel="self" type="application/atom+xml" /><link href="https://data-brain-mind.github.io/tutorials/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-12-10T06:24:33+08:00</updated><id>https://data-brain-mind.github.io/tutorials/feed.xml</id><title type="html">Data on the Brain &amp;amp; Mind Tutorial Track (NeurIPS 2025)</title><subtitle>Home to the Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)
</subtitle><entry><title type="html">Accelerated Methods in {Multi-Modal, Multi-Metric, Many-Model} CogNeuroAI</title><link href="https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/" rel="alternate" type="text/html" title="Accelerated Methods in {Multi-Modal, Multi-Metric, Many-Model} CogNeuroAI" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>An exploration of the <em>Multi-Modal, Multi-Metric, Many-Model</em> computational modeling methods at the intersection of cognitive neuroscience and artificial intelligence – the emerging field of <strong>NeuroAI</strong>.</p>

<p>Subsequent, updated versions of this tutorial will be made available via GitHub at <a href="https://github.com/ColinConwell/DBM-Tutorial">github.com/ColinConwell/DBM-Tutorial</a>.</p>

<hr />

<p><strong>Background</strong>: Understanding how the brain transforms sensory input into representations that support adaptive, real-world behavior has long been a foundational goal of cognitive neuroscience. Developed with an industrial fervor scarcely seen since the days of steel and railroad, task-performant deep neural network (DNN) models have now become a central part of the neuroscientific toolkit – not just as artifacts of engineering, but as <em>theoretical objects</em> whose internal representations can be mapped systematically to biological neural activity <d-cite key="yamins2014performance,kriegeskorte2015deep,kanwisher2023using"></d-cite>. So deep now is the synchrony between AI and neuroscience that some have suggested the emergence of a new field altogether, aptly called <strong>NeuroAI</strong> <d-cite key="zador2023catalyzing"></d-cite>.</p>

<p>In this tutorial, we’ll explore three of NeuroAI’s most actively expanding frontiers: the study of representational alignment between natural and artificial neural systems <d-cite key="sucholutsky2023getting"></d-cite>, the similarity of representation in systems grounded in different modalities (e.g. vision and language) <d-cite key="radford2021learning,huh2024platonic"></d-cite>, and the interpretability of otherwise subsymbolic representations (vision) by way of symbolic references (natural language) <d-cite key="bau2017network"></d-cite>. Along the way, we’ll pay particular focus to the underlying gears and cogs of these methodologies, attempting to see if we can bring the same kinds of optimization to the science of neural modeling that engineers have brought to the development of neural models.</p>

<hr />

<p>The tutorial is organized (roughly) into two chapters:</p>

<ul>
  <li>
    <p><strong>Chapter 1: Intro to DeepJuice</strong><br /> An introduction to the DeepJuice library, and a reproduction of the main analysis in <d-cite key="conwell2024large"></d-cite>, which probes multiple forms of representational alignment between visual deep neural networks and ventral visual cortex activity in the widely used <a href="https://naturalscenesdataset.org/">Natural Scenes Dataset</a> <d-cite key="allen2022massive"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Chapter 2: Enter Now the LLM</strong><br /> Exploring the kinds of inferences and analysis made possible by language models, with a case study in cross-modal representational alignment, and language-specified, hypothesis-driven interpretability probes based on vector-semantic mapping with “relative representations” <d-cite key="moschella2022relative"></d-cite>.</p>
  </li>
</ul>

<div style="display: flex; align-items: center; gap: 12px;">
<img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/deepjuice-iconogram.png" alt="DeepJuice Logo" width="128" />
<h2>DeepJuice: High-Throughput (GPU-Accelerated) Brain &amp; Behavioral Modeling</h2>
</div>

<p>DeepJuice is a library for performing various kinds of readout on deep neural network models. It includes tools and functionality designed specifically for high-throughput model instrumentalization, feature extraction, dimensionality reduction, neural regression, transfer learning, manifold statistics, and mechanistic interpretability. It also includes a large collection of models (and relevant metadata) that allows for controlled experimental comparison across models that vary in theoretically interesting ways.</p>

<p><strong>The Controlled Comparison Approach</strong>: A key methodological insight underlying this work is that we can conceptualize each DNN as a different “model organism” – a unique artificial visual system with performant, human-relevant visual capacities. By comparing sets of models that vary only in one factor (e.g., architecture, task, or training data) while holding other factors constant, we can experimentally examine which inductive biases lead to more or less brain-predictive representations. This approach moves beyond simply ranking models on a leaderboard, toward understanding <em>why</em> certain representations align better with the brain than others.</p>

<p><strong>Target Brain Region</strong>: Our primary target is human <em>occipitotemporal cortex</em> (OTC), a broad swath of high-level ventral visual cortex encompassing category-selective regions for faces, bodies, scenes, and objects. But our interest extends beyond object recognition <em>per se</em> – OTC is increasingly understood as a “feature bank” whose representations support not just categorization but flexible, adaptive behavior across many tasks. The same representations that predict OTC activity may also predict human behavioral similarity judgments, generalization patterns, and semantic associations.</p>

<p>In this walkthrough, we demonstrate the methodology from <d-cite key="conwell2024large"></d-cite> (<a href="https://github.com/ColinConwell/DeepNSD">GitHub</a>), which examined representational alignment across 117 diverse DNNs. Here, we work through the analysis pipeline with a single example model – the methods are identical whether applied to one model or one hundred. Our target brain data comes from the <a href="https://naturalscenesdataset.org/">Natural Scenes Dataset</a> (NSD; <d-cite key="allen2022massive"></d-cite>) – currently the largest, highest-resolution fMRI dataset available for this purpose.</p>

<p>A reproduction of the first figure (showing an overview of the analysis pipeline) in <d-cite key="conwell2024large"></d-cite> is provided below.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/methods-overview-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/methods-overview-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/methods-overview-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/methods-overview.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Conwell et al. (2024) Figure 1: Methods Overview
</div>

<ul>
  <li>In <strong>(A)</strong>, we have an example of our target brain data: occipito-temporal cortex activity (in this case, from a single example subject, colored by a measure of reliability called <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1012092">NCSNR</a>).</li>
  <li>In <strong>(B)</strong>, we see a schematic of the underlying model repositories that power our controlled model comparison, grouped by specific axes of experimental interest with implications for the kinds of ‘representational pressures’ that could in theory have played a role in shaping the representations we read in the brain data.</li>
  <li>In <strong>(C)</strong>, we see a schematic of our representational alignment (“linking”) procedures (classic RSA, encoding RSA – with the figure-implicit encoding models that are fit in between). More detail on all 3 of these below!</li>
</ul>

<h2 id="step-0-environment-setup">Step 0: Environment Setup</h2>

<p>As <code class="language-plaintext highlighter-rouge">deepjuice</code> remains in active development, the particular setup steps may change over time. For the latest setup steps, please refer to the tutorial’s <a href="https://github.com/ColinConwell/DBM-Tutorial">GitHub repo</a>.</p>

<details class="code-fold">
<summary>Global environment setup</summary>

```python
import os, sys

tutorial_repo = 'https://github.com/ColinConwell/DBM-Tutorial'
notebook_path = os.getcwd() # current working directory

try: # to import deepjuice, then check for local clone
    from deepjuice import SystemStats
except ImportError as error:
    print('deepjuice installation not found. Checking for local clone...')


DEEPJUICE_DIR = os.path.join(notebook_path, 'DeepJuice')
if os.path.exists(DEEPJUICE_DIR):
    sys.path.insert(0, DEEPJUICE_DIR)

else: # raise error directing to repo
    raise RuntimeError(f"deepjuice not found. Please refer to {tutorial_repo} for setup instructions.")
```

</details>

<p>Working on a shared machine with multiple GPU devices? DeepJuice at the moment works sufficiently well with a single GPU for testing purposes. In the code below, we’ll modify the global environment to only “see” a single GPU, meaning all subsequent processes will default to using this GPU. Proactively, we’ll set this to be the last available GPU in the list, so as to minimize the likelihood of conflict with concurrent processes or traffic.</p>

<details class="code-fold">
<summary>GPU device selection</summary>

```python
from deepjuice.systemops.devices import count_cuda_devices

device_to_use = count_cuda_devices() - 1

if device_to_use &gt;= 0: # no update if no device
    os.environ['CUDA_VISIBLE_DEVICES'] = str(device_to_use)
```

</details>

<h2 id="conceptual-primer-representational-alignment">Conceptual Primer: Representational Alignment</h2>

<p>Before diving into the code, it is worth considering the conceptual foundations of what we are trying to measure. <em>Representational alignment</em> refers to the degree of correspondence between internal representations in two systems – in our case, artificial neural networks and the human brain. But how exactly should we quantify this correspondence? And what does it mean when two systems are “aligned”? A recent community effort <d-cite key="sucholutsky2023getting"></d-cite> provides a useful framework. They identify three core questions in alignment research:</p>
<ol>
  <li>How do we <em>measure</em> similarity between representations?</li>
  <li>Do similar representations lead to <em>similar behavior</em>?</li>
  <li>How can we <em>modify</em> representations to better align them?</li>
</ol>

<p>This tutorial focuses primarily on the first question, while keeping the others in mind.</p>

<p><strong>The Challenge of Comparison</strong>: Representations in DNNs and brains exist in different coordinate systems, with different dimensionalities, and are accessed through different measurement modalities. Any comparison requires assumptions about what aspects of representation matter. As <d-cite key="sucholutsky2023getting"></d-cite> emphasize, the field has developed many alignment measures, but there is limited consensus on which to use when, or how they relate to each other.</p>

<p><strong>Two Families of Methods</strong>: In this tutorial, we explore two complementary approaches:</p>

<ol>
  <li>
    <p><strong>Representational Similarity Analysis (RSA)</strong>: Compares the <em>geometry</em> of representations by asking whether stimuli that are represented as similar in the model are also represented as similar in the brain <d-cite key="kriegeskorte2008representational"></d-cite>. This approach abstracts away from individual features to focus on relational structure – the pattern of distances between stimuli.</p>
  </li>
  <li>
    <p><strong>Encoding Models</strong>: Learns a weighted mapping from model features to brain activity, then evaluates on held-out data <d-cite key="haxby2001distributed"></d-cite>. This approach allows the brain to “select” which model features are relevant, but introduces many degrees of freedom that can make even dissimilar representations appear aligned.</p>
  </li>
</ol>

<p>We will use both approaches, as well as hybrid methods (encoding RSA) that combine their strengths. The goal is not to find “the best” metric, but to understand what different metrics reveal about model-brain correspondence.</p>

<p>The powerhouse of all our comparisons will be DeepJuice: a single unified API that combines model zoology, feature extraction, dimensionality reduction, and alignment techniques together in a single, GPU-accelerated package, written in PyTorch, designed for cognitive scientists.</p>

<h2 id="intro-import-the-juice">Intro: Import the Juice</h2>

<p>To get started, let’s load all relevant deepjuice modules below:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepjuice</span> <span class="kn">import</span> <span class="o">*</span> <span class="c1"># imports all deepjuice modules
</span><span class="kn">from</span> <span class="n">deepjuice.first_steps</span> <span class="kn">import</span> <span class="o">*</span> <span class="c1"># tutorial helpers
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Welcome to DeepJuice!
</code></pre></div></div>

<h2 id="step-1-selecting-a-model">Step 1: Selecting a Model</h2>

<h3 id="model-options">Model Options</h3>

<p>The first part of most DeepJuice analyses will involve loading a pretrained deep neural network model. In the DeepJuice model zoo (which we affectionally refer to as “the orchard”), you’ll find a large number of different, <em>registered</em> models. <strong>get_model_options()</strong> (the main function from deepjuice/model_zoo) will return a pandas dataframe of the various models we’ve already implemented. Note that while these are the models that will be easiest to use with deepjuice, almost any PyTorch model will work just as well.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># here's a sample of 5 deepjuice models from the model_zoo
</span><span class="nf">get_model_options</span><span class="p">().</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<p>The output of <strong>get_model_options()</strong> can be parsed as a regular pandas dataframe, but you can also throw various arguments in there to return specific subsets of the model zoo, by passing a dictionary of the form {metadata_column_name: search_query}. And if you just want the <strong>unique ID</strong> (<strong>uid</strong>) of the models that can be used to load them, you can ask get_model_options for a list.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># here are a sample of 5 deepjuice models from OpenCLIP
</span><span class="nf">get_model_options</span><span class="p">({</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">openclip</span><span class="sh">'</span><span class="p">}).</span><span class="nf">sample</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># here is a list of all available OpenAI clip models in deepjuice:
</span><span class="nf">get_model_options</span><span class="p">({</span><span class="sh">'</span><span class="s">source</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">clip</span><span class="sh">'</span><span class="p">},</span> <span class="n">output</span><span class="o">=</span><span class="sh">'</span><span class="s">list</span><span class="sh">'</span><span class="p">,</span> <span class="n">exact_match</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['clip_rn50',
 'clip_rn101',
 'clip_rn50x4',
 'clip_rn50x16',
 'clip_rn50x64',
 'clip_vit_b_32',
 'clip_vit_b_16',
 'clip_vit_l_14',
 'clip_vit_l_14_336px']
</code></pre></div></div>

<h3 id="load-the-model">Load the Model</h3>

<p>Once you’ve decided on a model to use, just pass the <strong>model_uid</strong> to the following function. Crucially, this function by default will return both the pretrained model and the preprocessing function that should be applied to the data you intend to perform readout over. (Note: If using certain models – like OpenCLIP – in a Colab environment, you may be required to install the relevant packages, e.g. <code class="language-plaintext highlighter-rouge">pip install open_clip_torch</code>.)</p>

<p>For this walkthrough, we’ll be using a pretrained AlexNet model from Harvard Vision Science Laboratory’s <a href="https://github.com/harvard-visionlab/open_ipcl">OpenIPCL</a> repository. This model is a self-supervised contrastive learning model trained over the images (but not the labels) of the ImageNet1K dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># list all (Harvard) IPCL models available in deepjuice
</span><span class="nf">get_model_options</span><span class="p">({</span><span class="sh">'</span><span class="s">model_uid</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">ipcl</span><span class="sh">'</span><span class="p">},</span> <span class="n">output</span><span class="o">=</span><span class="sh">'</span><span class="s">list</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['ipcl_alexnet_gn_imagenet1k',
 'ipcl_alexnet_gn_openimagesv6',
 'ipcl_alexnet_gn_places2',
 'ipcl_alexnet_gn_vggface2',
 'ipcl_alexnet_gn_mixedx3']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model_uid</span> <span class="o">=</span> <span class="sh">'</span><span class="s">ipcl_alexnet_gn_imagenet1k</span><span class="sh">'</span>
<span class="n">model</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="nf">get_deepjuice_model</span><span class="p">(</span><span class="n">model_uid</span><span class="p">)</span>
</code></pre></div></div>

<p>Critically, almost all deepjuice functionality is predicated on the model object having a forward function that is called directly over an input tensor as follows: model(inputs). All <em>registered</em> models in DeepJuice have this property implicitly specified, but if you’re using a custom model, or want a variation of the function, you can always specify your own!</p>

<h2 id="step-2-feature-extraction">Step 2: Feature Extraction</h2>

<p>You’ve now loaded a model and you want to know how the model responds to a given set of inputs – potentially at more than one stage of the model’s information-processing hierarchy. This procedure is typically called feature extraction, and involves saving the intermediate outputs of one or more of a model’s many layers.</p>

<p>In the example below, we’ll grab some sample images, and pass them through our loaded model, collecting both features and feature metadata as we do. This is a relatively small set of inputs, but just note that feature extraction procedures can get very computationally expensive very fast. We’ll discuss how to manage this overhead in the <strong>Memory Management</strong> subsection below.</p>

<h3 id="datasets--metadata">DataSets + MetaData</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">juicyfruits</span> <span class="kn">import</span> <span class="n">get_sample_images</span>

<span class="c1"># let's grab a quick sample of 5 images
</span><span class="n">sample_image_paths</span> <span class="o">=</span> <span class="nf">get_sample_images</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="sh">"</span><span class="s">   Sample image paths:</span><span class="sh">"</span><span class="p">,</span> <span class="o">*</span><span class="p">[</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">relpath</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">os</span><span class="p">.</span><span class="nf">getcwd</span><span class="p">())</span> <span class="k">for</span> <span class="n">path</span> <span class="ow">in</span> <span class="n">sample_image_paths</span><span class="p">]]))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Initializing DeepJuice Benchmarks
   Sample image paths:
DeepJuice/juicyfruits/quick_data/image/vislab_logo.jpg
DeepJuice/juicyfruits/quick_data/image/william_james.jpg
DeepJuice/juicyfruits/quick_data/image/grace_hopper.jpg
DeepJuice/juicyfruits/quick_data/image/xaesthetics_logo.jpg
DeepJuice/juicyfruits/quick_data/image/viriginia_woolf.jpg
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's get our dataloader now, which takes our image_paths
# and! our model preprocessing function, returning tensors
</span><span class="n">dataloader</span> <span class="o">=</span> <span class="nf">get_data_loader</span><span class="p">(</span><span class="n">sample_image_paths</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's start by getting all our model's feature maps, since we have only a small number of images:
# get_feature_maps requires only two arguments in this case: model, inputs (in this case, our dataloader)
</span>
<span class="n">feature_maps</span> <span class="o">=</span> <span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">flatten</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="c1">#flatten=False for visualization
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracting sample maps with torchinfo:
  (Moving tensors from CUDA:0 to CPU)
DeepJuice:INFO (_log) - Keeping 26 / 36 total maps (10 duplicates removed).
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># note that we can also add the argument dry_run=True
# to get a quick report about our extraction
</span><span class="nf">get_feature_maps</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">dry_run</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracting sample maps with torchinfo:
  (Moving tensors from CUDA:0 to CPU)
DeepJuice:INFO (_log) - Keeping 26 / 36 total maps (10 duplicates removed).
get_feature_maps() Dry Run Information
  # Inputs: 5; # Feature Maps: 26
  # Duplicates (Removed): 10
  Total memory required: 40.02 MB
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># what do our feature_maps look like?
</span><span class="k">for</span> <span class="n">layer_index</span><span class="p">,</span> <span class="p">(</span><span class="n">layer_name</span><span class="p">,</span> <span class="n">feature_map</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">.</span><span class="nf">items</span><span class="p">()):</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">layer_index</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">layer_name</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">feature_map</span><span class="p">.</span><span class="n">shape</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1 Conv2d-2-1 [5, 96, 55, 55]
2 GroupNorm-2-2 [5, 96, 55, 55]
3 ReLU-2-3 [5, 96, 55, 55]
4 MaxPool2d-2-4 [5, 96, 27, 27]
5 Conv2d-2-5 [5, 256, 27, 27]
...
24 ReLU-2-24 [5, 4096]
25 Linear-2-25 [5, 128]
26 Normalize-1-10 [5, 128]
</code></pre></div></div>

<p>Now that we’ve had a first look at our feature_map extraction procedure, let’s have a look at our feature_map metadata – which gives us other key information about we might need later on.</p>

<p>Note that a key argument for all of Deepjuice’s <em>metadata</em> operations is the <em>input_dim</em> argument (sometimes called batch_dim by other packages like TorchInfo). This tells us which dimension of the input corresponds to the number of stimuli in our dataset. This is almost always 0 (the DeepJuice default), but not always, so caveat emptor! Specifying the <em>input_dim</em> allows us to do things like flattening in further downstream processing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">feature_map_metadata</span> <span class="o">=</span> <span class="nf">get_feature_map_metadata</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="memory-management">Memory Management</h3>

<p>Due to the memory limitations of most machines, it will in the vast majority of cases be impossible to extract all the feature maps from a candidate model all at once. For this reason, DeepJuice is built with a number of tools that help manage the memory load of the feature extraction procedure.</p>

<p>The primary tool in this toolkit is the <strong>FeatureExtractor</strong> class.</p>

<p>The FeatureExtractor class works by taking a model, inputs combination and precomputing how much necessary is necessary to extract each feature map. It automatically batches these maps according either to an automated procedure or a user specified <em>memory_limit</em>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># let's imagine here that you have a system with EXTREMELY low RAM available
</span><span class="n">feature_extractor</span> <span class="o">=</span> <span class="nc">FeatureExtractor</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">memory_limit</span><span class="o">=</span><span class="sh">'</span><span class="s">12MB</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Extracting sample maps with torchinfo:
  (Moving tensors from CUDA:0 to CPU)
FeatureExtractor Handle for alexnet_gn
  36 feature maps (+10 duplicates); 5 inputs
  Memory required for full extraction: 45.21 MB
  Memory usage limiting device set to: cpu
  Memory usage limit currently set to: 12.00 MB
  5 batches required for current memory limit 
   Batch-001: 3 feature maps; 6.88 MB 
   Batch-002: 2 feature maps; 11.10 MB 
   Batch-003: 4 feature maps; 9.28 MB 
   Batch-004: 8 feature maps; 11.83 MB 
   Batch-005: 19 feature maps; 6.12 MB
</code></pre></div></div>

<p>Given that the vast majority of users will need the kind of memory management facilitated by FeatureExtractor, let’s see how it works in action below. Once instantiated with a model and inputs, FeatureExtractor acts as a generator, and can be called in a for loop as with any other generator.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepjuice.tensorops</span> <span class="kn">import</span> <span class="n">flatten_along_axis</span> <span class="k">as</span> <span class="n">flatten_tensor</span>

<span class="n">total_feature_count</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># across the nonduplicate layers
</span><span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">feature_maps</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">feature_extractor</span><span class="p">):</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Batch </span><span class="si">{</span><span class="n">index</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">feature_maps</span><span class="p">)</span><span class="si">}</span><span class="s"> Maps</span><span class="sh">'</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">uid</span><span class="p">,</span> <span class="n">feature_map</span> <span class="ow">in</span> <span class="n">feature_maps</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">feature_map</span> <span class="o">=</span> <span class="nf">flatten_tensor</span><span class="p">(</span><span class="n">feature_map</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">total_feature_count</span> <span class="o">+=</span> <span class="n">feature_map</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">  Total Feature Count:</span><span class="sh">'</span><span class="p">,</span> <span class="n">total_feature_count</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Batch 0: 3 Maps
Batch 1: 2 Maps
Batch 2: 4 Maps
Batch 3: 8 Maps
Batch 4: 19 Maps
  Total Feature Count: 2367456
</code></pre></div></div>

<h2 id="step-3-benchmark-preparation">Step 3: Benchmark Preparation</h2>

<p>So far, we’ve been extracting our <em>feature_maps</em> on a random sample of images. In reality, what we’ll more typically be doing is extracting our feature_maps over a stimulus set designed for a candidate brain or behavioral experiment we want to model. Below, we’ll use the real-world case of the <a href="https://naturalscenesdataset.org/">7T fMRI Natural Scenes Dataset (NSD)</a> as an example.</p>

<h3 id="benchmark-classes">Benchmark Classes</h3>

<p>The easiest way to deal with benchmark data (the target brain or behavioral data that you’ll be comparing your extracted feature_maps against) is (in our humble opinion) with a class object. Here’s an example of one such object below, which loads some sample data from one subject’s early visual and occitemporal cortex in response to 1000 images.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">juicyfruits</span> <span class="kn">import</span> <span class="n">NSDBenchmark</span>
<span class="n">benchmark</span> <span class="o">=</span> <span class="nc">NSDBenchmark</span><span class="p">()</span> <span class="c1">#load brain data benchmark
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Loading DeepJuice NSDBenchmark: 
  Image Set: shared1000
  Voxel Set: ['EVC', 'OTC']
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">benchmark</span> <span class="c1"># general info about the benchmark
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>NSD Human fMRI Benchmark Data
 Macro ROI(s): ['EVC', 'OTC']
 SubjectID(s): [1]
 # Probe Stimuli: 1000
 # Responding Voxels: 11967
 Largest ROI Constituents:
   OTC: 7310 Voxels
   EVC: 4657 Voxels
   EBA: 2525 Voxels
</code></pre></div></div>

<p>The pleasantry of a benchmark class is that you can do all sorts of intuitive things with it – without actually having to wrangle the underlying data components each time you want to do something.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># want a random sample stimulus?
</span><span class="n">benchmark</span><span class="p">.</span><span class="nf">get_stimulus</span><span class="p">()</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-36-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-36-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-36-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-36-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># want a target sample stimulus?
</span><span class="n">benchmark</span><span class="p">.</span><span class="nf">get_stimulus</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-37-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-37-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-37-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-37-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>A typically crucial piece of any brain dataset (and especially fMRI) are ‘regions of interests’ (ROIs). Our benchmark class here catalogues these automatically, and comes equipped with key functions that allow us to subset those parts of the brain data that correspond to each ROI.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">benchmark</span><span class="p">.</span><span class="nf">get_roi_structure</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="sh">'</span><span class="s">global</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Level 0:ROI EVC, OTC, V1v, V1d, V2v, V2d, V3v, V3d, hV4, FFA-1, FFA-2
              OFA, EBA, FBA-1, FBA-2, OPA, PPA, VWFA-1, VWFA-2, OWFA
Level 1:SubjectID 1
</code></pre></div></div>

<h3 id="core-components">Core Components</h3>

<p>Admittedly, though, classes can be hard to work with. For that reason, let’s break down the 3 core components to any benchmark:</p>
<ol>
  <li>Response Data: (BrainUnitID x StimulusID)</li>
  <li>Metadata: (BrainUnitID x …)</li>
  <li>Stimulus Data: (StimulusID x…)</li>
</ol>

<p><em>response_data</em> is the most important of these 3 components. In the case of a <strong>BrainBenchmark</strong> (like <strong>NSDBenchmark</strong>), the rows of this dataframe are the IDs of a target brain units (sometimes called “neuroids” by benchmarking platforms like BrainScore), the columns of this dataframe are the IDs of our target stimuli, and each cell is the response of a given brain unit (in this case voxels) to a target stimulus (in this case, a natural image).</p>

<h2 id="step-4-alignment-procedures">Step 4: Alignment Procedure(s)</h2>

<p>You have your benchmark data; you have your features. Now, it is time to put the two together. While many people refer to this step with many different names – correspondence test, encoding or decoding, representational similarity analysis – in DeepJuice, we tend to call it the “alignment procedure.” We use this term with the most general connotation possible, or at least the one we hope generalizes over the many different forms this particular step can take.</p>

<p><strong>Why Multiple Metrics?</strong> A striking finding in recent work is that many qualitatively different DNN models – with different architectures, tasks, and training data – achieve comparably high alignment scores with human visual cortex. This relative parity suggests that standard alignment metrics may be capturing broad, shared structure rather than the specific computational principles that distinguish models. Using multiple metrics with different assumptions can help reveal whether apparent alignment reflects genuine correspondence or simply the flexibility of the linking procedure.</p>

<p>In the example below (which directly follows the analyses from <d-cite key="conwell2024large"></d-cite>), we run three styles of alignment procedure:</p>

<ul>
  <li>
    <p><strong>Classical RSA (cRSA)</strong>: A paradigmatic representational similarity analysis that directly computes the representational geometry of the target brain and model data (using the Pearson distance), then directly compares the resultant representational dissimilarity matrices (RDMs) with no intermediate reweighting (again using the Pearson distance). This analysis assumes a fully-emergent fit between model and brain that weights all model features equally, and in this sense is one of the stricter tests of alignment one can use.</p>
  </li>
  <li>
    <p><strong>Encoding Regression (eReg)</strong>: This alignment procedure unfurls in multiple steps. First, for computational efficiency and to control <em>explicit</em> degrees of freedom, we use a dimensionality reduction technique called sparse random projection to project each of our feature maps into a lower-dimensional space. This step relies on the <strong>Johnson-Lindenstrauss Lemma</strong> <d-cite key="johnson1984extensions"></d-cite>, a theorem guaranteeing that points in a high-dimensional space can be embedded into a lower-dimensional space while approximately preserving pairwise distances. Crucially, the target dimension depends logarithmically on the number of samples (images), not the original feature dimension. After reducing the dimensionality of our feature space, we apply ridge regression to the reduced features.</p>
  </li>
  <li>
    <p><strong>Encoding RSA (eRSA)</strong>: Elsewhere developed as <strong>feature-reweighted RSA</strong> <d-cite key="kaniuth2022feature"></d-cite>, this alignment procedure is a way of taking the encoding models from the ridge regression procedure above, and using them to build RDMs. This procedure liberates our RSA from the assumption that all features must be weighted equally, and leverages the trimming and redistribution of feature importances done by the encoding model to give us a more explicitly brain-aligned representational geometry.</p>
  </li>
</ul>

<p>Below is a more detailed visual schematic of these methods:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/metrics-schematic-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/metrics-schematic-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/metrics-schematic-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/metrics-schematic.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Alignment Methods Schematic
</div>

<p>Of course, scoring the alignment between a given model’s feature space and the brain doesn’t necessarily tell us all that we’d like to know. Once we’ve scored the alignment across many feature spaces from a single model (or from many models), we’d ideally like to know why certain feature spaces score higher than others.</p>

<p>There are obviously many theories for this, but in the example below, we’ll look at one increasingly popular candidate called <strong>effective dimensionality</strong>, which quantifies how the variance in a given feature space is distributed across its principal components. This metric has been linked to generalization performance in neural networks and the dimensionality of neural representations in the brain <d-cite key="elmoznino2024high"></d-cite>.</p>

<h3 id="prepare-benchmark-data">Prepare Benchmark Data</h3>

<p>The first thing we’ll need to do to be able to run our alignment procedure is to compute the target RDMs from our target brain data. If we’ve already specified ROIs in this data, we can compute these RDMs simply by passing an RDM function (i.e. a squareform distance metric) to a convenience function from our benchmark class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepjuice.alignment</span> <span class="kn">import</span> <span class="n">compute_rdm</span>
<span class="n">benchmark</span><span class="p">.</span><span class="nf">build_rdms</span><span class="p">(</span><span class="n">compute_rdm</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">pearson</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="score-the-feature-maps">Score the Feature Maps</h3>

<p>And now, in one grand swoop, we’re going to use our <strong>Benchmark()</strong> and our <strong>FeatureExtractor()</strong> to loop through all <em>feature_maps</em> and score them on the alignment procedures outlined above.</p>

<details class="code-fold">
<summary>Define: get_benchmarking_results() + helpers</summary>

```python
from deepjuice.alignment import get_scoring_method

def effective_dimensionality(feature_map):
    # first, we define a GPU-capable PCA
    pca = TorchPCA(device='cuda:0')

    # then fit the PCA...
    pca.fit(feature_map)

    # then extract the eigenspectrum
    eigvals = pca.explained_variance_

    # then return effective dimensionality (on CPU)
    return (eigvals.sum() ** 2 / (eigvals ** 2).sum()).item()

def get_benchmarking_results(benchmark, feature_extractor,
                             layer_index_offset = 0,
                             metrics = ['cRSA','eReg','eRSA'],
                             rdm_distance = 'pearson',
                             rsa_distance = 'pearson',
                             score_types = ['pearsonr'],
                             stack_final_results = True,
                             feature_map_stats = None,
                             alpha_values = np.logspace(-1,5,7).tolist(),
                             regression_means = True, device='auto'):
    # ... (full implementation in notebook)
    pass
```

</details>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stats</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">effective_dimensionality</span><span class="sh">'</span><span class="p">:</span> <span class="n">effective_dimensionality</span><span class="p">}</span> <span class="c1"># stats to compute over features
</span><span class="n">results</span> <span class="o">=</span> <span class="nf">get_benchmarking_results</span><span class="p">(</span><span class="n">benchmark</span><span class="p">,</span> <span class="n">feature_extractor</span><span class="p">,</span> <span class="n">feature_map_stats</span> <span class="o">=</span> <span class="n">stats</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="parse-results-scores">Parse Results (Scores)</h3>

<p>The results you now have are comprehensive, but complicated: one score per feature map (layer) per ROI per subject per train-test split per metric. If you’re doing analyses across multiple models, you’ll then multiply these combinatorics even further with scores per model. Generally speaking – and while this choice comes with assumptions the field should probably start examining a bit more closely – most model-to-brain alignment procedures include the taking of a max over layers.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepjuice.benchmark</span> <span class="kn">import</span> <span class="n">get_results_max</span>

<span class="c1"># variables over which we'll take the max:
</span><span class="n">max_over</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">model_layer</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># all variables for which we want the max:
</span><span class="n">group_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">metric</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">subj_id</span><span class="sh">'</span><span class="p">]</span>

<span class="c1"># criterion is the column in results that is used to select the maxs
</span><span class="nf">get_results_max</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span> <span class="n">group_vars</span><span class="p">,</span> <span class="n">max_over</span><span class="p">,</span>
                <span class="n">criterion</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">},</span> <span class="n">filters</span><span class="o">=</span><span class="p">{</span><span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">:</span> <span class="p">[</span><span class="sh">'</span><span class="s">EVC</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">OTC</span><span class="sh">'</span><span class="p">]})</span>
</code></pre></div></div>

<h2 id="step-5-visualize-your-results">Step 5: Visualize your Results</h2>

<p>So now, we have some results! What do you do with them? Contribute to the advancement of knowledge, ideally. But first! Let’s start with some plots, and a bit of analysis.</p>

<details class="code-fold">
<summary>Import plotnine (ggplot)</summary>

```python
from plotnine import * # python's ggplot
import plotnine.options as ggplot_opts
```

</details>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># these are the results columns we'll need for plot
</span><span class="n">target_cols</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">model_layer</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">,</span>
               <span class="sh">'</span><span class="s">subj_id</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">effective_dimensionality</span><span class="sh">'</span><span class="p">]</span>

<span class="n">target_region</span> <span class="o">=</span> <span class="sh">'</span><span class="s">OTC</span><span class="sh">'</span> <span class="c1"># first, we'll look at occipitotemporal cortex
</span>
<span class="c1"># we subset our results for our region of interest:
</span><span class="n">plot_data</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sh">'</span><span class="s">region == @target_region</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># here, we convert model_layer_index into a relative depth (0 to 1)
</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span>
                                  <span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">].</span><span class="nf">max</span><span class="p">())</span>

<span class="n">ggplot_opts</span><span class="p">.</span><span class="n">figure_size</span> <span class="o">=</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># set figure size
</span>
<span class="c1"># this defines our plotting geometry / aesthetics
</span><span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span>
           <span class="sh">'</span><span class="s">group</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">color</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">}</span>

<span class="c1"># and now, we invoke python's ggplot via plotnine!
</span><span class="p">(</span><span class="nf">ggplot</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="o">**</span><span class="n">mapping</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_line</span><span class="p">()</span> <span class="o">+</span>
 <span class="nf">facet_wrap</span><span class="p">(</span><span class="sh">'</span><span class="s">~metric</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span> <span class="nf">theme_bw</span><span class="p">()).</span><span class="nf">draw</span><span class="p">()</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-82-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-82-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-82-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-82-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Here, we see that (pretty much up until the last layers) our scores for occipitotemporal cortex (OTC) tend to increase. (The dropoff in the last layers is a byproduct of the fact that we’re working with a self-supervised model in this demo, and these layers correspond to the projection head of the model – whose features aren’t always particularly useful or predictive). Now, let’s look at the same plot with the addition of early visual cortex (EVC):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_region</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">EVC</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">OTC</span><span class="sh">'</span><span class="p">]</span> <span class="c1"># now, we add early visual cortex
</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sh">'</span><span class="s">region == @target_region</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span>
                                  <span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">].</span><span class="nf">max</span><span class="p">())</span>

<span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span>
           <span class="sh">'</span><span class="s">group</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">color</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">}</span>

<span class="n">mapping</span><span class="p">[</span><span class="sh">'</span><span class="s">linetype</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="sh">'</span><span class="s">region</span><span class="sh">'</span> <span class="c1"># for comparison
</span>
<span class="p">(</span><span class="nf">ggplot</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="o">**</span><span class="n">mapping</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_line</span><span class="p">()</span> <span class="o">+</span>
 <span class="nf">facet_grid</span><span class="p">(</span><span class="sh">'</span><span class="s">cv_split~metric</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span> <span class="nf">theme_bw</span><span class="p">()).</span><span class="nf">draw</span><span class="p">()</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-83-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-83-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-83-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-83-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Here, we see that predictions for EVC tend to peak earlier than they do for late-stage visual cortex (OTC) – though not by as much as you might otherwise expect… (While we don’t have time to go too deeply into this result here, if you’re interested in following-up on this, a good place to start would be to consider receptive field sizes!)</p>

<p>Finally, let’s look at the relationship between effective dimensionality and score across layers:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_region</span> <span class="o">=</span> <span class="sh">'</span><span class="s">OTC</span><span class="sh">'</span> <span class="c1"># just occipitotemporal cortex again
</span>
<span class="c1"># we subset our results for our region of interest:
</span><span class="n">plot_data</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">query</span><span class="p">(</span><span class="sh">'</span><span class="s">region == @target_region</span><span class="sh">'</span><span class="p">)</span>

<span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">effective_dimensionality</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span>
           <span class="sh">'</span><span class="s">group</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">color</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">}</span>

<span class="p">(</span><span class="nf">ggplot</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="o">**</span><span class="n">mapping</span><span class="p">))</span> <span class="o">+</span> <span class="nf">geom_point</span><span class="p">()</span> <span class="o">+</span>
 <span class="nf">geom_smooth</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="sh">'</span><span class="s">lm</span><span class="sh">'</span><span class="p">)</span> <span class="o">+</span> <span class="nf">facet_wrap</span><span class="p">(</span><span class="sh">'</span><span class="s">~metric</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span> <span class="nf">theme_bw</span><span class="p">()).</span><span class="nf">draw</span><span class="p">()</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-84-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-84-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-84-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-84-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Here, we see effectively no relationship. Why? Well, the reasons are complex, and full enumeration thereof is beyond the scope of this tutorial – but one function it serves here, at least, is to highlight that downstream alignment (at least to this very popular visual brain data) is not always 1:1 with what you might expect from the effective “degrees of freedom” inherent to the feature space of a candidate model.</p>

<hr />

<h1 id="enter-now-the-llms">Enter Now the LLMs</h1>

<h2 id="cross-modal-brain-alignment">Cross-Modal Brain Alignment</h2>

<p>Recent success predicting human ventral visual system responses from large language model (LLM) representations of image captions has sparked renewed interest in the possibility that high-level visual representations are “aligned to language” <d-cite key="wang2023better,doerig2025high"></d-cite>. This finding is striking: models trained <em>only</em> on text, with no visual input whatsoever, can predict activity in the <em>visual</em> cortex as well as models trained directly on images.</p>

<p><strong>What might explain this convergence?</strong> One hypothesis, articulated in the “Platonic Representation Hypothesis” <d-cite key="huh2024platonic"></d-cite>, suggests that neural networks trained on different modalities may be converging toward a shared statistical model of reality – not because they learn from each other, but because they are all learning to represent the same underlying world structure. An alternative interpretation is more deflationary: perhaps alignment procedures exploit the many degrees of freedom in large models, and apparent convergence reflects shared co-occurrence statistics rather than deeper representational similarity. This echoes the classic <em>symbol-grounding problem</em> <d-cite key="harnad1990symbol"></d-cite>, questioning whether models learn “meaning” or merely statistical patterns in symbols.</p>

<p>Recent work has begun to dissect this question more carefully. <d-cite key="shoham2024using"></d-cite> provide evidence that suggests language-vision alignment in neural networks emerges primarily through relational structure (how concepts relate to each other) rather than low-level feature similarity. <d-cite key="xu2025large"></d-cite> demonstrate that LLMs without visual grounding can recover <em>non-sensorimotor</em> features of human concepts (e.g., abstract relations) but struggle with <em>sensorimotor</em> features (e.g., shape, texture) – suggesting that cross-modal alignment may be partial and selective.</p>

<p>These findings raise important questions: Which aspects of representation are shared across modalities, and which are modality-specific? In this chapter, we demonstrate techniques for probing cross-modal alignment using the same NSD data from Chapter 1.</p>

<p>In this section, we walk through an example of cross-modal alignment using the same subset of the NSD dataset as before. Language models can be loaded through DeepJuice, but for demonstration purposes, we load a sentence transformer model from Huggingface directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>

<span class="n">model_uid</span> <span class="o">=</span> <span class="sh">'</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">'</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_uid</span><span class="p">)</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="p">.</span><span class="nf">from_pretrained</span><span class="p">(</span><span class="n">model_uid</span><span class="p">)</span>
</code></pre></div></div>

<p>Each of the images in the NSD dataset is actually an image from the COCO dataset, and each image in the COCO dataset comes with 5-6 human annotations (captions). By default (at least in the NSD metadata), the COCO captions are stored as stringified lists.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepjuice.structural</span> <span class="kn">import</span> <span class="n">flatten_nested_list</span> <span class="c1"># utility for list flattening
</span>
<span class="n">all_captions</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">.</span><span class="n">stimulus_data</span><span class="p">.</span><span class="n">coco_captions</span><span class="p">.</span><span class="nf">tolist</span><span class="p">()</span> <span class="c1"># list of strings
</span>
<span class="c1"># the listification and flattening of our 5 captions per image into one big list:
</span><span class="n">captions</span> <span class="o">=</span> <span class="nf">flatten_nested_list</span><span class="p">([</span><span class="nf">eval</span><span class="p">(</span><span class="n">captions</span><span class="p">)[:</span><span class="mi">5</span><span class="p">]</span> <span class="k">for</span> <span class="n">captions</span> <span class="ow">in</span> <span class="n">all_captions</span><span class="p">])</span>

<span class="nf">assert</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">captions</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1000</span> <span class="o">*</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># assertion to ensure each image has 5 captions
</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">captions</span><span class="p">,</span> <span class="mi">5</span><span class="p">).</span><span class="nf">tolist</span><span class="p">()</span> <span class="c1"># a sample showing what our captions look like
</span></code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>['Two children standing next to a yellow fire hydrant',
 'Skier skiing down a hill near a guard rail ',
 'A suitcase on a bed with a cat sitting inside of it.',
 'A man riding a surfboard on top of a wave in the ocean.',
 'a train being worked on in a train manufacturer']
</code></pre></div></div>

<p>Pretty much everything from here forward matches exactly the benchmarking procedure above! In this example, we’ll calculate the cRSA, SRPR, and eRSA score for our target set of language embeddings, saving the results for two key ROIs (early visual and occipitotemporal cortex) for the purposes of downstream comparison:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_region</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">EVC</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">OTC</span><span class="sh">'</span><span class="p">]</span> <span class="c1"># early and late visual cortex
</span>
<span class="n">plot_data</span> <span class="o">=</span> <span class="n">results</span><span class="p">.</span><span class="nf">copy</span><span class="p">()[</span><span class="n">results</span><span class="p">[</span><span class="sh">'</span><span class="s">cv_split</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="sh">'</span><span class="s">test</span><span class="sh">'</span><span class="p">].</span><span class="nf">query</span><span class="p">(</span><span class="sh">'</span><span class="s">region == @target_region</span><span class="sh">'</span><span class="p">)</span>

<span class="c1"># this gives us a "depth ratio" from 0 (earliest layer) to 1 (latest)
</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">]</span> <span class="o">/</span> 
                                  <span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">model_layer_index</span><span class="sh">'</span><span class="p">].</span><span class="nf">max</span><span class="p">())</span>

<span class="c1"># putting our metrics in order as a factor (Categorical)
</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">metric</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">Categorical</span><span class="p">(</span><span class="n">plot_data</span><span class="p">[</span><span class="sh">'</span><span class="s">metric</span><span class="sh">'</span><span class="p">],</span> 
                                     <span class="n">categories</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">cRSA</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">eReg</span><span class="sh">'</span><span class="p">,</span><span class="sh">'</span><span class="s">eRSA</span><span class="sh">'</span><span class="p">])</span>

<span class="kn">from</span> <span class="n">plotnine</span> <span class="kn">import</span> <span class="o">*</span> <span class="c1"># ggplot functionality
</span>
<span class="c1"># define an aesthetic mapping for ggplot call:
</span><span class="n">mapping</span> <span class="o">=</span> <span class="p">{</span><span class="sh">'</span><span class="s">x</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">model_layer_depth</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">y</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">score</span><span class="sh">'</span><span class="p">,</span> 
           <span class="sh">'</span><span class="s">group</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">color</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">region</span><span class="sh">'</span><span class="p">}</span>

<span class="p">(</span><span class="nf">ggplot</span><span class="p">(</span><span class="n">plot_data</span><span class="p">,</span> <span class="nf">aes</span><span class="p">(</span><span class="o">**</span><span class="n">mapping</span><span class="p">))</span> <span class="o">+</span> 
 <span class="nf">geom_line</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span><span class="nf">facet_wrap</span><span class="p">(</span><span class="sh">'</span><span class="s">~metric</span><span class="sh">'</span><span class="p">)</span><span class="o">+</span> <span class="nf">theme_bw</span><span class="p">()).</span><span class="nf">draw</span><span class="p">()</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-94-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-94-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-94-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-94-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Notice here that language models do exceptionally well in predicting late-stage visual cortex (OTC), but not so well in predicting early visual cortex (EVC)! This is a nice sanity check, and suggests the high scores in late-stage visual cortex are indeed the result of shared representational structure, as opposed to a mere statistical fluke.</p>

<h2 id="vector-semantic-mapping">Vector-Semantic Mapping</h2>

<p><em>Hypothesis-driven interpretation via “relative representations”</em> <d-cite key="moschella2022relative"></d-cite></p>

<p>The use of deep neural network models to predict brain and behavioral phenomena typically involves procedures that operate over dense, high-dimensional vectors whose underlying structure is largely opaque. However accurate these procedures may be in terms of raw predictive power, this ambiguity leaves open fundamental questions about what drives the measured correspondence between model and brain.</p>

<p><strong>The Core Idea</strong>: <d-cite key="moschella2022relative"></d-cite> proposed <em>relative representations</em> as an alternative to working with absolute embedding coordinates. The key insight is elegant: rather than describing each data point by its coordinates in a high-dimensional space, describe it by its <em>similarity to a fixed set of anchor points</em>. In their words:</p>
<blockquote>
  <p>“We propose the latent similarity between each sample and a fixed set of anchors as an alternative data representation.”</p>
</blockquote>

<p>This move toward <strong>interpretable</strong> alignment allows us to bridge the gap between opaque neural features and grounded semantic concepts.</p>

<p><strong>Anchor Points as a Coordinate System</strong>: Think of this as defining a new coordinate system where each axis is “similarity to anchor X.” If we choose our anchors wisely – using natural language queries that correspond to interpretable concepts – then the resulting representation is both lower-dimensional and human-readable. Each dimension has a clear meaning: “how much does this image resemble [query]?”</p>

<p><strong>Connection to Zero-Shot Classification</strong>: This technique is closely related to how CLIP performs zero-shot classification <d-cite key="radford2021learning"></d-cite>. CLIP classifies images by computing similarity to text embeddings of category labels, then taking the argmax. Here, we extend this idea: instead of taking an argmax for classification, we use the full vector of similarities as features for downstream prediction.</p>

<p>We will use this technique to probe what kinds of semantic information (expressible in natural language) drives the predictive power of LLMs for visual brain activity.</p>

<h2 id="classification-warmup-example">Classification Warmup Example</h2>

<p>Below is a schematic of the basic idea of vector-semantic mapping using relative representations, applied to the binary classification task of <a href="https://huggingface.co/datasets/cats_vs_dogs">cat and dog images</a> based on the embeddings of a CLIP-like vision encoder.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-classifier-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-classifier-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-classifier-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-classifier.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Relative Representations Classifier
</div>

<p>Relative representations are a way to quickly warp high-dimensional embeddings into new (lower-dimensional) coordinates defined by a set of custom queries (also called anchor points).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="nf">get_available_device</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Using device: </span><span class="si">{</span><span class="n">device</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="n">model</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">preprocess</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">create_model_and_transforms</span><span class="p">(</span><span class="sh">'</span><span class="s">ViT-B-16-SigLIP</span><span class="sh">'</span><span class="p">,</span> <span class="n">pretrained</span><span class="o">=</span><span class="sh">'</span><span class="s">webli</span><span class="sh">'</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">open_clip</span><span class="p">.</span><span class="nf">get_tokenizer</span><span class="p">(</span><span class="sh">'</span><span class="s">ViT-B-16-SigLIP</span><span class="sh">'</span><span class="p">)</span>

<span class="n">hf_data</span> <span class="o">=</span> <span class="nf">load_dataset</span><span class="p">(</span><span class="sh">'</span><span class="s">cats_vs_dogs</span><span class="sh">'</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="sh">'</span><span class="s">../datasets</span><span class="sh">'</span><span class="p">)[</span><span class="sh">'</span><span class="s">train</span><span class="sh">'</span><span class="p">]</span>
<span class="n">image_dataset</span> <span class="o">=</span> <span class="nc">ImageDataset</span><span class="p">(</span><span class="n">hf_data</span><span class="p">,</span> <span class="n">preprocess</span><span class="p">,</span> <span class="n">image_key</span><span class="o">=</span><span class="sh">'</span><span class="s">image</span><span class="sh">'</span><span class="p">,</span> <span class="n">label_key</span><span class="o">=</span><span class="sh">'</span><span class="s">labels</span><span class="sh">'</span><span class="p">)</span>

<span class="n">image_loader</span> <span class="o">=</span> <span class="nc">DataLoader</span><span class="p">(</span><span class="n">image_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using device: cuda
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">A sample of dog and cat images:</span><span class="sh">'</span><span class="p">)</span>
<span class="n">image_dataset</span><span class="p">.</span><span class="nf">show_sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-98-output-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-98-output-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-98-output-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-98-output-2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="binary-classification-with-full-embeddings">Binary Classification with Full Embeddings</h3>

<p>We now have the dataset we can use to perform our binary classification task. First, we’ll split our embeddings (and their associated category labels) into train and test sets…</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">embeds</span> <span class="o">=</span> <span class="n">embeds</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span><span class="p">)</span> <span class="c1"># move to cpu, just in case
</span>
<span class="c1"># use odd / even indices for train / test split:
</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span> <span class="o">=</span> <span class="n">embeds</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">embeds</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div>

<p>… then, we fit our classifier (a regularized logistic regression with internal cross-validation to select the regularization parameter, alpha).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="nc">RidgeClassifierCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">])</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Binary Classification Accuracy:</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Binary Classification Accuracy:
0.9976078598889363
</code></pre></div></div>

<h3 id="classification-with-relative-representations">Classification with Relative Representations</h3>

<p>Now we arrive at the key technique. Instead of classifying over the full 768-dimensional embedding space, we project images into a lower-dimensional space defined by <em>similarity to language queries</em>. As <d-cite key="moschella2022relative"></d-cite> demonstrated, this relative representation has a powerful property: it is invariant to isometric transformations of the original embedding space.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query_strings</span> <span class="o">=</span> <span class="p">[</span><span class="sh">'</span><span class="s">has fur</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">has scales</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">has wings</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">has pointy ears</span><span class="sh">'</span><span class="p">,</span>
                 <span class="sh">'</span><span class="s">is bigger</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is smaller</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">lives indoors</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">lives outdoors</span><span class="sh">'</span><span class="p">]</span>

<span class="n">query_embeds</span> <span class="o">=</span> <span class="nf">get_query_embeds</span><span class="p">(</span><span class="n">query_strings</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Query Embeddings Shape:</span><span class="sh">'</span><span class="p">,</span> <span class="n">query_embeds</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query Embeddings Shape: torch.Size([8, 768])
</code></pre></div></div>

<p>After embedding our language queries, we compute their cosine similarity to each image embedding. But now, instead of using softmax over similarity scores, we use them as features in a regularized classifier.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">similarity</span> <span class="o">=</span> <span class="n">embeds</span> <span class="o">@</span> <span class="n">query_embeds</span><span class="p">.</span><span class="n">T</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">similarity</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">similarity</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">,</span> <span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">clf</span> <span class="o">=</span> <span class="nc">RidgeClassifierCV</span><span class="p">(</span><span class="n">alphas</span><span class="o">=</span><span class="p">[</span><span class="mf">1e-3</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e3</span><span class="p">])</span>
<span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Binary Classification Accuracy:</span><span class="sh">'</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="nf">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Binary Classification Accuracy:
0.9119179837676207
</code></pre></div></div>

<p>Notice now that the number of coefficients in our classifier corresponds 1:1 with the number of queries we made… And because our queries were made with natural language, we can actually interpret these coefficients in terms of their ‘importance’ in determining the correct label!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-110-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-110-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-110-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-110-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Query-Based Classifier Coefficients
</div>

<h2 id="brain-alignment-with-relative-representations">Brain Alignment with Relative Representations</h2>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-alignment-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-alignment-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-alignment-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/relative-reps-alignment.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Relative Representations for Brain Alignment
</div>

<p>The classification example above demonstrates that relative representations can preserve task-relevant structure while providing interpretable coefficients. But the more topic-relevant question motivating the use of this technique in this tutorial is: <strong>Can we use relative representations to understand what drives model-brain alignment?</strong></p>

<p>When we fit a standard encoding model, we learn weights over thousands of opaque features. We can measure alignment, but we cannot easily interpret <em>why</em> certain images are predicted well or poorly. Relative representations offer a path forward: by projecting both model features and brain activity into a shared space defined by interpretable language queries, we can ask which semantic dimensions are most important for predicting neural responses.</p>

<p>This is a form of <em>hypothesis-driven alignment analysis</em>. Rather than asking “how well does this model predict the brain?” we ask “which semantic concepts, expressible in language, mediate the correspondence between model and brain?”</p>

<h3 id="defining-the-semantic-basis">Defining the Semantic Basis</h3>

<p>The power of relative representations depends critically on the choice of anchor points. In <d-cite key="moschella2022relative"></d-cite>, anchors were often chosen from the data itself. Here, we take a different approach: we define anchors using natural language queries that span semantic dimensions we hypothesize might be relevant for visual brain activity.</p>

<details class="code-fold">
<summary>Define hypothesized semantic queries (73 anchor points)</summary>

```python
prompt_options = {} # initialize a dictionary for our query options (by categories, for clarity)

prompt_options['camera'] = ['a picture', 'a drawing', 'a rendering']

prompt_options['quality'] = ['high resolution', 'low resolution', 'high quality', 'low quality',
                             'dark', 'bright', 'cluttered', 'clean', 'happy', 'sad',
                             'colorful', 'black-and-white', 'professional', 'candid', 'artistic']

prompt_options['agent'] = ['a person', 'a group of people',
                           'an animal', 'a group of animals']

prompt_options['object'] = ['an object', 'a group of objects', 'furniture',
                            'food', 'a plant', 'plants', 'a vehicle', 'vehicles']

prompt_options['adjective'] = ['big','small','boxy','curvy','rural', 'urban',
                               'slow', 'fast', 'dangerous', 'safe', 'indoors','outdoors']

prompt_options['places'] = ['work', 'a desk', 'a room', 'a building', 'a city', 'a park', 'a field',
                             'a beach', 'the water', 'the sky', 'the snow', 'the desert', 'the mountains']

prompt_options['time'] = ['in the morning', 'at night', 'during the day', 
                          'in winter', 'in spring', 'in summer', 'in autumn']

prompt_options['action'] = ['playing', 'working', 'fighting', 'dancing', 'jumping', 
                            'sitting', 'standing', 'running', 'walking', 'swimming', 'flying']

# flatten our categorized query options into a single list
all_prompts = list(chain(*prompt_options.values()))
print('Number of prompts (hypothesized anchor points): {}'.format(len(all_prompts)))
```

</details>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Number of prompts (hypothesized anchor points): 73
</code></pre></div></div>

<p>To see how well these queries define the representational alignment of our vision model to the visual brain data, we simply need again to compute the similarity of each query to each image embedding, then fit a regression over the resultant similarity scores.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">query_embeds</span> <span class="o">=</span> <span class="nf">get_query_embeds</span><span class="p">(</span><span class="n">all_prompts</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Shape of query embeddings:</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="n">el</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">query_embeds</span><span class="p">.</span><span class="n">shape</span><span class="p">])</span>

<span class="n">image_query_sims</span> <span class="o">=</span> <span class="n">image_embeds</span> <span class="o">@</span> <span class="n">query_embeds</span><span class="p">.</span><span class="n">T</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Shape of similarity matrix:</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="n">el</span> <span class="k">for</span> <span class="n">el</span> <span class="ow">in</span> <span class="n">image_query_sims</span><span class="p">.</span><span class="n">shape</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Shape of query embeddings: [73, 768]
Shape of similarity matrix: [1000, 73]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">type_label</span> <span class="o">=</span> <span class="sh">'</span><span class="s">Regression on Hypothesized Query Similarities</span><span class="sh">'</span>
<span class="n">hypothesis_query_results</span> <span class="o">=</span> <span class="nf">compute_alignment</span><span class="p">(</span><span class="n">image_query_sims</span><span class="p">,</span> <span class="n">type_label</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Scoresheet for our hypothesized query regression:</span><span class="sh">'</span><span class="p">)</span>
<span class="n">hypothesis_query_results</span><span class="p">[</span><span class="sh">'</span><span class="s">scoresheet</span><span class="sh">'</span><span class="p">]</span>
</code></pre></div></div>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-119-output-3.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dimensionality reduction: 768 -&gt; 73 (9.5% of original)
EVC: 108.3% performance with 90.5% fewer dimensions
OTC: 103.5% performance with 90.5% fewer dimensions
</code></pre></div></div>

<p>And so – What do we see here? Well, first and foremost we see that with only 73 queries (73 total predictors, 73 dimensions) we can recover the full predictive power of the underlying 768-dimensional embedding space!</p>

<p>And now, taking the mean absolute value of the coefficients for each query, we can see which of our 73 dimensions weighed most on our downstream brain data prediction!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-120-output-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-120-output-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-120-output-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-120-output-2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="visualization-coverage-of-the-manifold">Visualization: Coverage of the Manifold</h3>

<p>To better understand how our query-based representations relate to the brain representational space, we can visualize both spaces using dimensionality reduction techniques. These visualizations help illustrate:</p>
<ol>
  <li>How query similarities “cover” the brain representational space</li>
  <li>The geometric correspondence between query and brain spaces</li>
</ol>

<h3 id="prediction-error-on-the-brain-manifold">Prediction Error on the Brain Manifold</h3>

<p>A key insight from <d-cite key="moschella2022relative"></d-cite> is that anchor points define a <strong>coordinate system</strong>: each image is “triangulated” by its similarity to the anchors. The quality of this coordinate system depends on how well the anchors span the relevant variation in the target space.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-125-output-2.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="comparison-random-vs-hypothesized-queries">Comparison: Random vs. Hypothesized Queries</h3>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-131-output-1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-131-output-1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-131-output-1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-accelerated-methods-in-multi-modal-multi-metric-many-model-cogneuroai/cell-131-output-1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>… what do we find? On the one hand, our hypothesized queries do indeed consistently (uniformly, in fact) outperform the random queries of the same size…</p>

<p>… On the other hand, they do so by a much smaller margin than you might expect if our hypothesized queries were indeed exceptional in a deeply meaningful way…</p>

<p>Further interpretation of these results is… an active area of research! And as such (albeit very much a regrettable cliffhanger), a topic we leave for you as food-for-thought. So as not to leave you fully hanging, though, here’s some questions we’ve been pondering:</p>

<ul>
  <li>What do these results tell us (if anything) about the nature of cross-modal alignment between language models and the visual brain? Is it a fluke? Or more evidence of ‘platonic’ representational structure?</li>
  <li>By what mechanism could the random queries perform as well as they do?</li>
  <li>What, if anything, might this finding have to do with effective dimensionality?</li>
  <li>What, if anything, could we do to improve the hypothesized queries?
    <ul>
      <li>Is there more variance left to capture? If so, how much, and with what kinds of queries?</li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="conclusion">Conclusion</h1>

<p>Our tutorial has now taken us on a somewhat whirlwind tour through the core methods of representational alignment research in computational cognitive neuroscience: extracting features from deep neural networks, comparing them to brain activity using multiple metrics (RSA and encoding models), and probing cross-modal alignment between vision and language representations.</p>

<p><strong>A Note on Scope</strong>: Throughout this tutorial, we have demonstrated these methods on a small number of example models. The findings from any single model (or handful of models) should be interpreted cautiously – the real power of this approach emerges when applied systematically across many models, as in <d-cite key="conwell2024large"></d-cite>. The code and methods here are designed to scale: the same functions that process one model can process hundreds, enabling the kind of controlled comparisons that distinguish modern alignment research from earlier single-model studies.</p>

<p><strong>Key Takeaways</strong>:</p>

<ol>
  <li>
    <p><strong>Different metrics reveal different aspects of alignment</strong>: Classical RSA provides a strict test of emergent geometric correspondence, while encoding models allow flexible feature reweighting. The choice of metric embodies assumptions about what constitutes a “good” match between model and brain.</p>
  </li>
  <li>
    <p><strong>Many different models achieve comparable alignment</strong>: A striking finding here and in the broader literature is that qualitatively different models (even those trained on entirely different modalities of data) often achieve similar brain-predictivity scores. This parity – not visible when testing single models – motivates the multi-model, many-metric (but single pipeline!) approach demonstrated here.</p>
  </li>
  <li>
    <p><strong>Cross-modal alignment raises deep questions</strong>: The fact that language models can predict visual cortex activity challenges simple modular accounts of brain organization. Whether this reflects genuine shared representations or statistical artifacts remains actively investigated.</p>
  </li>
  <li>
    <p><strong>Interpretability is a moving target</strong>: The interpretability of DNNs is a moving target, circumscribed by every choice in our experimental pipelines: the data we use as probes, the model populations we sample to assess divergence, and the metrics we use to quantify greater and lesser alignment to the downstream brain and behavioral phenomena that are our ultimate targets.</p>
  </li>
</ol>

<p><strong>Looking Forward</strong>: The broader vision of NeuroAI <d-cite key="zador2023catalyzing"></d-cite> is a bidirectional exchange: insights from neuroscience inspire more capable AI, while AI tools help us understand the brain. The goal of using DNNs to understand computational principles – not just predict brain activity – requires moving beyond single-model demonstrations to systematic, many-model comparisons. We hope this tutorial provides the methodological foundation for such investigations.</p>

<p>Feel free to reuse, remix, and extend this code. For questions or suggestions, please reach out via the associated GitHub repository.</p>

<hr />

<h1 id="related-software">Related Software</h1>

<p>This tutorial uses DeepJuice for high-throughput neural network analysis. We gratefully acknowledge the broader ecosystem of tools for representational alignment research:</p>

<p><strong>Neural Benchmarking Platforms</strong>:</p>
<ul>
  <li><a href="https://www.brain-score.org/">Brain-Score</a> - Integrative benchmarks for brain-like AI <d-cite key="schrimpf2018brainscore"></d-cite></li>
  <li><a href="https://github.com/cvai-roig-lab/Net2Brain">Net2Brain</a> - Toolbox for comparing DNNs to brain data</li>
</ul>

<p><strong>Representational Similarity and Alignment</strong>:</p>
<ul>
  <li><a href="https://github.com/rsagroup/rsatoolbox">RSAToolbox</a> - Representational Similarity Analysis in Python</li>
  <li><a href="https://github.com/ahwillia/netrep">NetRep</a> - Metrics for comparing neural network representations</li>
  <li><a href="https://github.com/nacloos/similarity-repository">Similarity</a> - Repository of similarity measures</li>
  <li><a href="https://gallantlab.org/himalaya/">Himalaya</a> - Fast kernel methods for encoding models (Gallant Lab)</li>
</ul>]]></content><author><name>Colin Conwell</name></author><summary type="html"><![CDATA[A tutorial showcasing a number of (GPU-accelerated) methods for probing the representational alignment of brains, minds, and machines. An exploration of computational modeling methods at the intersection of cognitive neuroscience and artificial intelligence.]]></summary></entry><entry><title type="html">An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute</title><link href="https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/" rel="alternate" type="text/html" title="An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/"><![CDATA[<p>The following tutorial will provide a brief introduction to this dataset then demonstrate how to load an example session and do some simple analysis. We’ll cover the following topics:</p>

<ul>
  <li>Introduction to the experiment</li>
  <li>Accessing the dataset</li>
  <li>Reading the project metadata</li>
  <li>Loading an experiment for analysis</li>
  <li>Plotting neural activity aligned to stimuli</li>
  <li>Using optotagging to infer cell type</li>
  <li>Accessing LFP data</li>
  <li>Analyzing behavioral data during the visual change detection task</li>
</ul>

<h2 id="introduction-to-the-experiment">Introduction to the experiment</h2>

<p>Our ability to perceive the sensory environment and flexibly interact with the world requires the coordinated action of neuronal populations distributed throughout the brain. To further our understanding of the neural basis of behavior, the Visual Behavior project leveraged the Allen Brain Observatory pipeline (diagrammed below; gray panels refer to the companion <a href="https://portal.brain-map.org/circuits-behavior/visual-behavior-2p">Visual Behavior Optical Physiology dataset</a>) to collect a large-scale, highly standardized dataset consisting of recordings of neural activity in mice that have learned to perform a visual change detection task. This dataset can be used to investigate how patterns of spiking activity across the visual cortex and thalamus are related to behavior and also how these activity dynamics are influenced by task-engagement and prior visual experience. </p>

<p>The Visual Behavior Neuropixels dataset includes 153 sessions from 81 mice. These data are made openly accessible, with all recorded timeseries, behavioral events, and experimental metadata conveniently packaged in Neurodata Without Borders (NWB) files that can be accessed and analyzed using our open Python software package, the  <a href="https://github.com/AllenInstitute/AllenSDK">AllenSDK</a>.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="the-visual-change-detection-task">The visual change detection task</h3>

<p>The Visual Behavior Optical Physiology and Visual Behavior Neuropixels projects are built upon a change detection behavioral task. Briefly, in this go/no-go task, mice are shown a continuous series of briefly presented visual images and they earn water rewards by correctly reporting when the identity of the image changes (diagrammed below). Five percent of images are omitted, allowing for analysis of expectation signals.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="neuropixels-recordings-throughout-visual-cortex-and-thalamus">Neuropixels recordings throughout visual cortex and thalamus</h3>

<p>This dataset includes multi-regional Neuropixels recordings from up to 6 probes at once. The probes target six visual cortical areas including VISp, VISl, VISal, VISrl, VISam, and VISpm. In addition, multiple subcortical areas are also typically measured, including visual thalamic areas LGd and LP as well as units in the hippocampus and midbrain. In addition to spiking activity, NWB files contain local field potential (LFP) data from each probe insertion.</p>

<p>Recordings were made in three genotypes: C57BL6J, Sst-IRES-Cre; Ai32, and Vip-IRES-Cre; Ai32. By crossing Sst and Vip lines to the Ai32 ChR2 reporter mouse, we were able to activate putative Sst+ and Vip+ cortical interneurons by stimulating the cortical surface with blue light during an optotagging protocol at the end of each session.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/probe_diagram-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/probe_diagram-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/probe_diagram-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/probe_diagram.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="investigating-the-impact-of-stimulus-novelty-on-neural-responses-and-behavior">Investigating the impact of stimulus novelty on neural responses and behavior</h3>

<p>To allow analysis of stimulus novelty on neural responses and behavior, two different images sets were used in the recording sessions: G and H (diagrammed below). Both image sets comprised 8 natural images. Two images were shared across the two image sets (purple in diagram), enabling within session analysis of novelty effects. Mice took one of the following three trajectories through training and the two days of recording:</p>

<p>1) Train on G; see G on the first recording day; see H on the second recording day</p>

<p>2) Train on G; see H on the first recording day; see G on the second recording day</p>

<p>3) Train on H; see H on the first recording day; see G on the second recording day</p>

<p>The numbers in the <em>Training and Recording Workflow</em> bubble below give the total recording sessions of each type in the dataset.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h4 id="tutorial-link"><a href="https://github.com/AllenNeuralDynamics/neurips-vbn-tutorial/blob/main/code/vbn_neurips_tutorial.ipynb">Tutorial link</a></h4>

<h3 id="resources">Resources</h3>

<h4 id="data-resources-associated-with-this-dataset">Data Resources associated with this dataset</h4>

<table>
  <thead>
    <tr>
      <th>Data Resource</th>
      <th>Format</th>
      <th>Uniform Resource ID (URI)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Behavior + spike data</td>
      <td>NWB</td>
      <td>s3://visual-behavior-neuropixels-data/visual-behavior-neuropixels/, https://doi.org/10.48324/dandi.000713/0.240702.1725</td>
    </tr>
    <tr>
      <td>Behavior training data</td>
      <td>NWB</td>
      <td>s3://visual-behavior-neuropixels-data/visual-behavior-neuropixels/, https://doi.org/10.48324/dandi.000713/0.240702.1725</td>
    </tr>
    <tr>
      <td>Local Field Potential (LFP) data</td>
      <td>NWB</td>
      <td>s3://visual-behavior-neuropixels-data/visual-behavior-neuropixels/, https://doi.org/10.48324/dandi.000713/0.240702.1725</td>
    </tr>
    <tr>
      <td>Raw AP band data</td>
      <td>Binary</td>
      <td>s3://allen-brain-observatory/visual-behavior-neuropixels/raw-data/</td>
    </tr>
    <tr>
      <td>Raw LFP band data</td>
      <td>Binary</td>
      <td>s3://allen-brain-observatory/visual-behavior-neuropixels/raw-data/</td>
    </tr>
    <tr>
      <td>Eye, face, body videos</td>
      <td>MP4</td>
      <td>s3://allen-brain-observatory/visual-behavior-neuropixels/raw-data/</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="technical-resources-associated-with-this-dataset">Technical Resources associated with this dataset</h4>

<table>
  <thead>
    <tr>
      <th>Technical Resource</th>
      <th>Format</th>
      <th>Uniform Resource ID (URI)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>AllenSDK tutorials</td>
      <td>Jupyter Notebooks</td>
      <td>https://allensdk.readthedocs.io/en/latest/visual_behavior_neuropixels.html</td>
    </tr>
    <tr>
      <td>Visual Behavior Neuropixels Databook</td>
      <td>Jupyter Book</td>
      <td>https://allenswdb.github.io/physiology/ephys/visual-behavior/VB-Neuropixels.html</td>
    </tr>
    <tr>
      <td>Visual Behavior Neuropixels Technical Whitepaper</td>
      <td>PDF</td>
      <td>https://brainmapportal-live-4cc80a57cd6e400d854-f7fdcae.divio-media.net/filer_public/f7/06/f706855a-a3a1-4a3a-a6b0-3502ad64680f/visualbehaviorneuropixels_technicalwhitepaper.pdf</td>
    </tr>
  </tbody>
</table>

<p><em>AllenSDK tutorials:</em> Jupyter notebooks demonstrating how to access data with the AllenSDK and perform basic analysis on behavior, spike and LFP data.</p>

<p><em>Visual Behavior Neuropixels Databook:</em> Jupyter Book providing background information about the experimental design and change detection task, as well as in-depth descriptions of all metadata tables in the dataset.</p>

<p><em>Visual Behavior Neuropixels Technical Whitepaper:</em> Document providing detailed information about the Allen Brain Observatory data collection pipeline and rig hardware.</p>

<p>In addition to these materials, please refer to the following manuscript, which provides a basic characterization of the dataset and which we intend as the primary citation for those using this resource: https://www.biorxiv.org/content/10.1101/2025.10.17.683190v1.</p>]]></content><author><name>Corbett Bennett</name></author><summary type="html"><![CDATA[Tutorial on analyzing the Visual Behavior Neuropixels dataset from the Allen Institute]]></summary></entry><entry><title type="html">Neural Keyword Spotting on LibriBrain</title><link href="https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain/" rel="alternate" type="text/html" title="Neural Keyword Spotting on LibriBrain" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/neural-keyword-spotting-on-libribrain/"><![CDATA[<h2 id="introduction">Introduction</h2>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        



<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
      
        <source class="responsive-img-srcset" srcset="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-480.webp 480w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-800.webp 800w,/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic-1400.webp 1400w," type="image/webp" sizes="95vw" />
      
    
    <img src="/tutorials/assets/img/2025-11-24-neural-keyword-spotting-on-libribrain/task-graphic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />
  </picture>

  
</figure>
    </div>
</div>
<div class="caption">
    Overview of the neural keyword spotting task: A participant listens to audiobook speech while MEG sensors record brain activity. The system processes neural signals to detect when specific keywords (like "Watson") are heard, producing a probability score for each word window.
</div>

<blockquote>
  <p><strong>Note</strong>: This tutorial is released in conjunction with our DBM workshop paper <em>“Elementary, My Dear Watson: Non-Invasive Neural Keyword Spotting in the LibriBrain Dataset”</em><d-cite key="elvers2025elementary"></d-cite>. The tutorial provides a comprehensive introduction as well as a hands-on, pedagogical walkthrough of the methods and concepts presented in the paper.</p>
</blockquote>

<p><strong>Neural Keyword Spotting (KWS)</strong> from brain signals presents a promising direction for non-invasive brain–computer interfaces (BCIs), with potential applications in assistive communication technologies for individuals with speech impairments. While invasive BCIs have achieved remarkable success in speech decoding<d-cite key="willett2023speech,metzger2023neuroprosthesis"></d-cite>, non-invasive approaches using magnetoencephalography (MEG) or electroencephalography (EEG) remain challenging due to lower signal-to-noise ratios and the difficulty of detecting brief, rare events in continuous neural recordings.</p>

<p>This tutorial demonstrates how to build and evaluate a neural keyword spotting system using the <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite>—a large-scale MEG corpus with over 50 hours of recordings from a single participant listening to audiobooks. We focus on the practical challenges of extreme class imbalance, appropriate evaluation metrics, and techniques for training models that can distinguish keyword occurrences from the continuous stream of speech.</p>

<h2 id="motivation-and-context">Motivation and Context</h2>

<h3 id="why-keyword-spotting">Why Keyword Spotting?</h3>

<p>Full speech decoding from non-invasive brain signals remains an open challenge. However, <strong>keyword spotting</strong>—detecting specific words of interest—offers a more tractable goal that could still enable meaningful communication. Even detecting a single keyword reliably (a “1-bit channel”) could significantly improve quality of life for individuals with severe communication disabilities, allowing them to:</p>

<ul>
  <li>Answer yes/no questions</li>
  <li>Signal alerts or requests</li>
  <li>Control devices through specific command words</li>
  <li>Maintain basic communication when other channels fail</li>
</ul>

<h3 id="the-challenge-rare-events-in-noisy-data">The Challenge: Rare Events in Noisy Data</h3>

<p>Keyword spotting from MEG presents two fundamental challenges:</p>

<ol>
  <li>
    <p><strong>Extreme Class Imbalance</strong>: Even short, common words like “the” represent only ~5.5% of all words in naturalistic speech. Target keywords like “Watson” appear in just 0.12% of word windows, creating a severe imbalance.</p>
  </li>
  <li>
    <p><strong>Low Signal-to-Noise Ratio</strong>: Unlike invasive recordings with electrode arrays placed directly on the cortex, non-invasive MEG/EEG sensors sit outside the skull, capturing attenuated and spatially blurred neural signals mixed with physiological and environmental noise.</p>
  </li>
</ol>

<p>These challenges require specialized techniques, which we cover in this tutorial.</p>

<h2 id="dataset-and-methodology">Dataset and Methodology</h2>

<h3 id="the-libribrain-dataset">The LibriBrain Dataset</h3>

<p>The <strong>LibriBrain dataset</strong><d-cite key="ozdogan2025libribrain"></d-cite> is a publicly available MEG corpus featuring over 50 hours of continuous recordings from a single participant listening to Sherlock Holmes audiobooks. The dataset is released as a set of preprocessed HDF5 files with word- and phoneme-level event annotation for each session, collected using a MEGIN Triux™ Neo system. The dimension of the currently released data is 306 sensor channels x 250 Hz.</p>

<h3 id="task-formulation">Task Formulation</h3>

<p>We frame keyword detection as <strong>event-referenced binary classification</strong>:</p>

<ul>
  <li><strong>Input</strong>: MEG signals (306 channels × T timepoints) windowed around word onsets</li>
  <li><strong>Output</strong>: Probability p ∈ [0, 1] that the target keyword occurs in this window</li>
  <li><strong>Window length</strong>: Keyword duration + buffers (pre-/post-onset)</li>
</ul>

<p>This differs from continuous detection by:</p>
<ol>
  <li>Focusing on word boundaries (where linguistic information peaks)</li>
  <li>Avoiding the combinatorial explosion of sliding windows</li>
  <li>Leveraging precise temporal alignment from annotations</li>
</ol>

<p><strong>Data Splits</strong>: We use multiple training sessions and dynamically select validation/test sessions based on keyword prevalence to ensure sufficient positive examples in held-out sets.</p>

<h3 id="model-architecture">Model Architecture</h3>

<p>The tutorials baseline model addresses the challenges through three components:</p>

<blockquote>
  <p><strong>Note</strong>: The notebook first demonstrates individual components with simplified examples (e.g., <code class="language-plaintext highlighter-rouge">ConvTrunk</code> with stride-2), then presents the full training architecture below.</p>
</blockquote>

<h4 id="1-convolutional-trunk">1. Convolutional Trunk</h4>
<p>The model begins with a Conv1D layer projecting the 306 MEG channels to 128 dimensions, followed by a residual block<d-cite key="he2016deep"></d-cite>. A key design choice is <strong>aggressive temporal downsampling</strong>: a stride-25 convolution with kernel size 50 reduces the sequence length by ~25× while expanding the receptive field. Two additional Conv1D layers refine the 128-dimensional representation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">trunk</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">306</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="nc">ResNetBlock1D</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">25</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>  <span class="c1"># stride-25 downsampling
</span>    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">same</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="nc">ELU</span><span class="p">(),</span>
<span class="p">)</span>
</code></pre></div></div>

<h4 id="2-temporal-attention">2. Temporal Attention</h4>
<p>The trunk output is projected to 512 dimensions before splitting into two parallel 1×1 convolution heads: one producing per-timepoint logits, the other producing attention scores. The attention mechanism<d-cite key="ilse2018attention"></d-cite> learns to focus on brief, informative time windows (e.g., around keyword onsets) while down-weighting noise.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">self</span><span class="p">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">))</span>
<span class="n">self</span><span class="p">.</span><span class="n">logits_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">self</span><span class="p">.</span><span class="n">attn_t</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Conv1d</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">head</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">trunk</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">logit_t</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">logits_t</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">attn_t</span><span class="p">(</span><span class="n">h</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">logit_t</span> <span class="o">*</span> <span class="n">attn</span><span class="p">).</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="3-loss-functions-for-extreme-imbalance">3. Loss Functions for Extreme Imbalance</h4>
<p>Standard cross-entropy fails under extreme class imbalance. We employ two complementary losses:</p>

<ul>
  <li>
    <p><strong>Focal Loss</strong><d-cite key="lin2017focal"></d-cite>: Down-weights easy negatives by $(1-p_t)^\gamma$, with class prior $\alpha=0.95$ matching the &lt;1% base rate. This prevents “always negative” collapse.</p>
  </li>
  <li>
    <p><strong>Pairwise Ranking Loss</strong><d-cite key="burges2010ranknet"></d-cite>: Directly optimizes the ordering of positive vs. negative scores, improving precision-recall trade-offs:</p>
  </li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pairwise_logistic_loss</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">pos_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">1</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="n">neg_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">targets</span> <span class="o">==</span> <span class="mi">0</span><span class="p">).</span><span class="nf">nonzero</span><span class="p">()</span>
    <span class="c1"># Sample pairs and penalize inversions
</span>    <span class="n">margins</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">pos_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">scores</span><span class="p">[</span><span class="n">sampled_neg_idx</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log1p</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="o">-</span><span class="n">margins</span><span class="p">)).</span><span class="nf">mean</span><span class="p">()</span>
</code></pre></div></div>

<h3 id="training-strategy">Training Strategy</h3>

<p><strong>Balanced Sampling</strong>: We construct training batches with ~10% positive rate (vs. natural &lt;1%) by:</p>
<ol>
  <li>Including most/all positive examples</li>
  <li>Subsampling negatives proportionally</li>
  <li>Shuffling each batch</li>
</ol>

<p>This ensures gradients aren’t starved by all-negative batches while keeping evaluation on natural class priors for realistic metrics.</p>

<p><strong>Preprocessing</strong>: The dataset applies per-channel z-score normalization and clips outliers beyond ±10σ before feeding data to the model.</p>

<p><strong>Data Augmentation</strong><d-cite key="buda2018systematic"></d-cite> (applied during training only):</p>
<ul>
  <li>Temporal shifts: randomly roll each sample by ±4 timepoints (±16ms at 250 Hz)</li>
  <li>Additive Gaussian noise: σ=0.01 added to normalized signals</li>
</ul>

<p><strong>Regularization</strong>: Dropout (p=0.5), weight decay<d-cite key="loshchilov2019decoupled"></d-cite> (1e-4), and early stopping on validation loss.</p>

<h2 id="evaluation-metrics">Evaluation Metrics</h2>

<p>Traditional accuracy is meaningless under extreme imbalance (always predicting “no keyword” yields &gt;99% accuracy). We employ metrics that reflect real-world BCI deployment:</p>

<h3 id="threshold-free-metrics">Threshold-Free Metrics</h3>

<p><strong>Area Under Precision-Recall Curve (AUPRC)</strong><d-cite key="saito2015precision,davis2006relationship"></d-cite>:</p>
<ul>
  <li>Baseline equals positive class prevalence (~0.001 for “Watson” on the full dataset, ~0.005 on the test set chosen to maximize prevalence)</li>
  <li>Aim for 2–10× improvement over chance</li>
  <li>More informative than AUROC under heavy imbalance</li>
</ul>

<p><strong>Precision-Recall Trade-off</strong>:</p>
<ul>
  <li><strong>Precision</strong>: Fraction of predicted keywords that are correct (controls false alarms)</li>
  <li><strong>Recall</strong>: Fraction of true keywords detected</li>
</ul>

<h3 id="user-facing-deployment-metrics">User-Facing Deployment Metrics</h3>

<p><strong>False Alarms per Hour (FA/h)</strong>:</p>
<ul>
  <li>Practical constraint: target &lt;10 FA/h for usability</li>
  <li>Computed as: <code class="language-plaintext highlighter-rouge">(False Positives / total_seconds) × 3600</code></li>
  <li>Evaluated at fixed recall targets (e.g., 0.2, 0.4, 0.6)</li>
</ul>

<p><strong>Operating Point Selection</strong>:
Choose threshold on validation to meet FA/h or precision targets; report test results at that fixed threshold.</p>

<h3 id="performance-interpretation">Performance Interpretation</h3>

<ul>
  <li><strong>Chance</strong>: Prevalence (% of words matching the keyword)</li>
  <li><strong>2–5× Chance</strong>: Modest but meaningful improvement</li>
  <li><strong>&gt;10× Chance</strong>: Strong performance for this challenging task</li>
</ul>

<h2 id="computational-requirements">Computational Requirements</h2>
<ul>
  <li><strong>GPU</strong>: Google Colab free tier (T4/L4 GPU) sufficient</li>
  <li><strong>Training Time</strong>: ~30 minutes for the baseline on default configuration</li>
  <li><strong>Memory</strong>: &lt;16 GB GPU RAM with batch size 64</li>
  <li><strong>Dataset</strong>: Automatically downloaded by the <code class="language-plaintext highlighter-rouge">pnpl</code> library (~50 GB for the full set, ~5GB for the default subset)</li>
</ul>

<p>The tutorial is designed to run on consumer hardware by training on a subset of data. To scale to the full 50+ hours of data, increase training sessions in the configuration and use a higher-tier GPU (V100/A100).</p>

<h2 id="learning-goals">Learning Goals</h2>

<p>By working through this tutorial, you will:</p>

<ol>
  <li><strong>Frame KWS from continuous MEG</strong> as a rare-event detection problem with event-referenced windowing</li>
  <li><strong>Handle extreme class imbalance</strong> through balanced sampling, focal loss, and pairwise ranking</li>
  <li><strong>Build a lightweight temporal model</strong> (Conv1D + attention) trainable on consumer GPUs</li>
  <li><strong>Evaluate with appropriate metrics</strong>: AUPRC, FA/h at fixed recall, precision-recall curves</li>
  <li><strong>Understand trade-offs</strong> between sensitivity (recall), false alarm rate, and practical usability</li>
  <li><strong>Gain hands-on experience</strong> with a real-world non-invasive BCI dataset</li>
</ol>

<h2 id="tutorial-structure">Tutorial Structure</h2>

<p>The accompanying Jupyter notebook provides a complete, executable walkthrough:</p>

<ol>
  <li><strong>Setup &amp; Configuration</strong> — Install dependencies, configure paths and hyperparameters</li>
  <li><strong>Dataset Exploration</strong> — Inspect HDF5 files (MEG signals) and TSV files (annotations)</li>
  <li><strong>Problem Formulation</strong> — Visualize challenges (class imbalance, signal noise)</li>
  <li><strong>Model Components</strong> — Interactive demos of each architectural component:
    <ul>
      <li>Convolutional trunk (spatial-temporal processing)</li>
      <li>Temporal attention (adaptive pooling)</li>
      <li>Focal loss (imbalance handling)</li>
      <li>Pairwise ranking (order-based training)</li>
      <li>Balanced sampling (batch composition)</li>
    </ul>
  </li>
  <li><strong>Training</strong> — Full training loop with PyTorch Lightning, early stopping, logging</li>
  <li><strong>Evaluation</strong> — AUPRC, ROC, FA/h curves, confusion matrices, threshold analysis</li>
  <li><strong>Next Steps</strong> — Suggested experiments (different keywords, architectures, augmentations)</li>
</ol>

<h2 id="notebook-access">Notebook Access</h2>

<p>Access the full interactive tutorial:</p>

<div style="display: flex; gap: 10px; margin: 20px 0;">
  <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" />
  </a>
  <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb" target="_blank">
    <img src="https://img.shields.io/badge/GitHub-View%20Source-blue?logo=github" alt="View on GitHub" />
  </a>
</div>

<p><strong>Links</strong>:</p>
<ul>
  <li><strong>Interactive (Colab)</strong>: <a href="https://colab.research.google.com/github/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb">Open in Google Colab</a></li>
  <li><strong>Source (GitHub)</strong>: <a href="https://github.com/neural-processing-lab/libribrain-keyword-experiments/blob/main/tutorial/Keyword_Spotting_Tutorial.ipynb">View on GitHub</a></li>
  <li><strong>Workshop Paper</strong>: <a href="https://arxiv.org/abs/2510.21038">arXiv:2510.21038</a></li>
  <li><strong>LibriBrain Dataset</strong>: <a href="https://huggingface.co/datasets/pnpl/LibriBrain">View on HuggingFace</a></li>
</ul>

<p><strong>Requirements</strong>: A Google account for Colab, or local Jupyter Notebook install with Python 3.10+</p>

<hr />

<p>Besides the accompanying workshop paper <d-cite key="elvers2025elementary"></d-cite>, this tutorial builds on work from the 2025 LibriBrain Competition<d-cite key="landau2025pnpl"></d-cite> centered around the LibriBrain dataset<d-cite key="ozdogan2025libribrain"></d-cite>. These papers contain more comprehensive bibliographies which might be helpful for readers seeking additional context.</p>]]></content><author><name>Gereon Elvers</name></author><summary type="html"><![CDATA[End-to-end walkthrough for Neural Keyword Spotting (KWS) on the LibriBrain MEG corpus: load data, frame the task, train a compact baseline, and evaluate with precision–recall metrics tailored to extreme class imbalance.]]></summary></entry><entry><title type="html">NLDisco: A Pipeline for Interpretable Neural Latent Discovery</title><link href="https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery/" rel="alternate" type="text/html" title="NLDisco: A Pipeline for Interpretable Neural Latent Discovery" /><published>2025-11-24T00:00:00+08:00</published><updated>2025-11-24T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/nldisco-a-pipeline-for-interpretable-neural-latent-discovery/"><![CDATA[<p>Large-scale neural recordings contain rich structure, but identifying the underlying representations remains difficult without tools that produce interpretable, neuron-level features. This tutorial introduces <strong>NLDisco</strong> (<strong>N</strong>eural <strong>L</strong>atent <strong>Disco</strong>very pipeline), which uses sparse encoder–decoder (SED) models to identify meaningful latent dimensions that correspond to specific behavioural or environmental variables.</p>

<h2 id="goal">Goal</h2>

<p>Discover interpretable latents (i.e., <em>features</em>) in high-dimensional neural data.</p>

<h2 id="terminology">Terminology</h2>

<ul>
  <li><em>Neural / Neuronal:</em> Refers to biological neurons. Distinguished from <em>model neurons</em> (see below).</li>
  <li><em>Units:</em> Putative biological neurons - the output from spikesorting extracellular electrophysiological data.</li>
  <li><em>Model neurons:</em> Neurons in a neural network model (aka <em>latents</em>)</li>
  <li><em>Features:</em> Interpretable latents (latent dimensions that align with meaningful behavioural or environmental variables)</li>
</ul>

<h2 id="method-overview">Method overview</h2>

<h3 id="sparse-autoencoders">Sparse Autoencoders</h3>

<p>Motivated by successful applications of sparse dictionary learning in AI mechanistic interpretability <d-cite key="lindsey_2024_crosscoders,cunningham_2023_saes,bricken_2023_towards_monosemanticity,templeton_2024_scaling_monosemanticity,dunefsky_2024_transcoders,ameisen_2025_circuit_tracing,lindsey_2025_biology_llm"></d-cite>, NLDisco trains overcomplete sparse encoder-decoder (SED) models to reconstruct neural activity based on a set of sparsely active dictionary elements (i.e. latents), implemented as hidden layer neurons. In the figure below, this is illustrated as reconstructing target neural activity \(z\) from input neural activity \(y\) via dictionary elements \(d\). Sparsity in the latent space encourages a monosemantic dictionary, where each hidden layer neuron corresponds to a single neural representation that can be judged for interpretability, making SEDs a simple but effective tool for neural latent discovery.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/sae-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/sae-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/sae-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/sae.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<p>For example, in a monkey reaching task, you might find a latent that becomes active mainly during high-velocity hand movements, and this latent can then be traced back to the subset of biological neurons whose activity consistently drives it.</p>

<p>These SEDs can be configured as autoencoders (SAEs) if the target for \(z\) is \(y\) (e.g. M1 activity based on M1 activity), or as transcoders if the target for \(z\) is dependent on or related to \(y\) (e.g. M1 activity based on M2 activity, or M1 activity on day 2 based on M1 activity on day 1). In this tutorial, we will work exclusively with the autoencoder variant, specifically Matryoshka SAEs (MSAEs).</p>

<h3 id="matryoshka-architecture">Matryoshka Architecture</h3>

<p>The Matryoshka architecture segments the latent space into multiple levels, each of which attempts a full reconstruction of the target neural activity <d-cite key="bussmann_2025_msae"></d-cite>. In the figure below, black boxes indicate the latents (model neurons) involved in a given level, while light-red boxes indicate additional latents recruited at lower levels. A top-\(k\) selection is used to choose which latents to recruit for reconstruction at each level (yellow neuron within each light-red box - \(k=1\) for each level in this example).</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/msae-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/msae-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/msae-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-nldisco-a-pipeline-for-interpretable-neural-latent-discovery/msae.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<p>This nested arrangement is motivated by the idea that multi-scale feature learning can mitigate “feature absorption” (a common issue where a more specific feature subsumes a portion of a more general feature), allowing both coarse and detailed representations to emerge simultaneously.</p>
<ul>
  <li>Latents in the highest level (\(L_1\)) typically correspond to broad, high-level features (e.g., a round object),</li>
  <li>Latents exclusive to the lowest level (\(L_3\)) often correspond to more specific, fine-grained features (e.g., a basketball)</li>
</ul>

<h2 id="code">Code</h2>

<p>The code for the full tutorial showcasing the NLDisco pipeline can be accessed <a href="https://github.com/jkbhagatio/nldisco/blob/nldisco_tutorial/notebooks/NLDisco_tutorial.ipynb">here</a>. The notebook contains step-by-step instructions and descriptions for each stage of the pipeline. It follows this structure:</p>
<ol>
  <li><strong>Load and prepare data</strong> - Load neural spike data from the Churchland MC_Maze dataset (center-out reaching task with motor cortex recordings) <d-cite key="churchland_2012_churchland_datasets,nlb_mcmaze"></d-cite>, pre-process into binned spike counts, and prepare behavioural/environmental metadata variables (hand position, velocity, maze conditions, etc.) for later feature interpretation.</li>
  <li><strong>Train models</strong> - Train MSAE models to reconstruct neural activity patterns. Multiple instances are trained with identical configurations for comparison to ensure convergence, with hyperparameters for sparsity and reconstruction quality. Validation checks examine decoder weight distributions, sparsity levels (L0), and reconstruction performance.</li>
  <li><strong>Save or load the model activations</strong> - Save trained SAE latent activations for efficient reuse, or load pre-computed activations to skip directly to feature interpretation.</li>
  <li><strong>Find features</strong> - Automatically map latents to behavioural and environmental metadata by computing selectivity scores that measure how strongly each latent activates during specific conditions (e.g., particular maze configurations, velocity ranges). Use an interactive dashboard to explore promising latent-metadata mappings and identify which biological neurons contribute most to interpretable features.</li>
</ol>]]></content><author><name>Anaya Gaelle Pouget</name></author><summary type="html"><![CDATA[Large-scale neural recordings contain rich structure, but identifying the underlying representations remains difficult without tools that produce interpretable, neuron-level features. This tutorial introduces NLDisco (Neural Latent Discovery pipeline), which uses sparse encoder–decoder (SED) models to identify meaningful latent dimensions that correspond to specific behavioural or environmental variables.]]></summary></entry><entry><title type="html">CONFORM: A Project to Create Crowd-Sourced Open Neuroscience fMRI Foundation Models</title><link href="https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/" rel="alternate" type="text/html" title="CONFORM: A Project to Create Crowd-Sourced Open Neuroscience fMRI Foundation Models" /><published>2025-10-09T00:00:00+08:00</published><updated>2025-10-09T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/"><![CDATA[<h2 id="background-and-introduction">Background and Introduction</h2>

<p>Creating a foundational human fMRI model is a critical next step for extending modern NeuroAI<d-cite key="gifford2024opportunities"></d-cite>. To achieve this, the model must generalize across both individuals and tasks, which requires a large volume of data with many participants, observations, and diverse stimuli.</p>

<p>Historically, a significant impediment has been that most fMRI studies have small sample sizes and a low number of observations per session; the latter also leading to poor stimulus diversity. As a result, typical fMRI experiments sample only a tiny fraction of the human population and the vast space of real-world visual, auditory, or linguistic inputs. These limitations impeded efforts to draw robust conclusions from fMRI data and to integrate insights from modern AI systems into our understanding of the human brain—a challenge that is exacerbated by the inherently noisy BOLD signal.</p>

<p>In visual neuroscience, a first step in meeting this challenge has already been taken through the collection of large-scale fMRI datasets, which typically include brain responses from a small number of participants each scanned over many repeated sessions (15-40 hours-long sessions), who view a large number of stimuli (5000-10,000 stimuli per participant)<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023"></d-cite>. This approach of “deeply sampling” a small number of participants increases the statistical power of experiments <d-cite key="NASELARIS2021sampling,baker2021power"></d-cite>, and enables powerful parameter-rich, within-subject models. While this approach of collecting large datasets from small groups of participants has led to hundreds of publications and impactful discoveries, even this strategy is neither sustainable nor scalable for both scientific and practical reasons:</p>

<ol>
  <li>
    <p><strong>Participant burden and attrition</strong>: Successful data collection at this scale depends on <em>heroic</em> efforts by both experimenters and participants. The time commitment and scheduling complexities are onerous: participants, experimenters, and scanners must remain consistently healthy and available (e.g., in both the BOLD5000 and NSD datasets at least one participant failed to complete the study<d-cite key="chang2019bold5000,allen2022massive"></d-cite>; in the THINGS dataset one participant was canceled due to “technical issues”<d-cite key="things2023"></d-cite>).</p>
  </li>
  <li>
    <p><strong>Limited sample size</strong>: Even with this extraordinary amount of effort, data was collected from only 3-8 participants—a small number that does not support the hoped-for population diversity expected of human neural foundation models.</p>
  </li>
  <li>
    <p><strong>Constrained stimulus diversity</strong>: Stimulus diversity is necessarily limited by small participant pools and the need for stimulus repeats and/or overlap across participants<d-cite key="Prince2022"></d-cite>. Even within a single recurring participant, only a limited number of observations are possible. Moreover, controlled tasks and stimulus selection methods have further reduced diversity in the visual images included in each dataset: NSD uses only COCO images (only 80 object categories<d-cite key="lin2014microsoft"></d-cite>, which leave gaps in many regions of natural image space<d-cite key="rothsample"></d-cite>), BOLD5000 uses COCO as well as SUN<d-cite key="SUN_Xiao2010"></d-cite> and ImageNet<d-cite key="russakovsky2015imagenet"></d-cite> images, and THINGS uses a larger number of “concepts”, but depicted as single cropped objects that show little context<d-cite key="things2023"></d-cite>.</p>
  </li>
  <li>
    <p><strong>Infrastructure challenges</strong>: Creating the infrastructure for data management and distribution is a considerable technical challenge. Short-term it requires a robust and replicable data processing pipeline and a reliable platform for data distribution. Long-term it requires stability—years later the distribution website should remain readily accessible.</p>
  </li>
  <li>
    <p><strong>Financial barriers</strong>: The monetary costs of collecting data can present a challenge to any single lab (e.g., five participants across 25 × one hour scans could easily cost on the order of $100,000) and risks over-representing the interests of the small number of labs with the necessary resources.</p>
  </li>
</ol>

<p>Despite their increased scale relative to standard fMRI studies, these datasets still present significant challenges in the construction of NeuroAI models. The number of observations and participants is still small for purposes of model training, and data quality is dependent on preprocessing methods. More importantly, prediction accuracy and decoding performance are typically high only when trained and tested within the same participant—due to inherent structural and functional differences between individual brains and, at present, weak methods for generalizing across them. Consequently, when models are applied across participants, even within the same study, their performance and decoding capabilities decrease dramatically.</p>

<p><img src="/tutorials/assets/img/2025-11-24-conform-a-project-to-create-crowd-sourced-open-neuroscience-fmri-foundation-models/Workshop-NeurIPS25.png" alt="CONFORM workflow" style="width: 80%; margin: auto;" /></p>

<p><strong>Figure 1: CONFORM workflow.</strong> A single, optimized experimental design is distributed to multiple sites for data collection. The collected data is then centralized for preprocessing, alignment, and integration into a foundational dataset. This process creates a continuous feedback loop, allowing the dataset to grow in size and diversity, which informs future experimental design and provides the basis for a strong foundation model.</p>

<h2 id="towards-a-dynamic-foundation-model-for-visual-fmri">Towards a Dynamic Foundation Model for Visual fMRI</h2>

<p>We propose <strong>CONFORM</strong> (Crowd-Sourced Open Neuroscience fMRI Foundation Model)—a strategy for building foundational human visual fMRI models through community-contributed datasets and models. Following previous efforts in systems neuroscience <d-cite key="internationalbrain2017laboratory"></d-cite>, we propose to leverage multi-site crowd-sourcing to enable collection of larger and more diverse datasets, along with new computational advances to facilitate coherent analysis. As detailed below, the building blocks of CONFORM are already in place, spanning four key domains:</p>

<ol>
  <li>
    <p><strong>A larger-scale and highly diverse dataset</strong> that aggregates close to 100 participants and 100,000s of natural scenes depicting 1000’s of object categories/concepts in context. “MOSAIC”<d-cite key="lahner_mosaic_2025"></d-cite> is a scalable framework for combining extant fMRI datasets<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023,lahner_modeling_2024,zhou2023large,shen2019deep,horikawa2017generic"></d-cite>, using common preprocessing and registration, into a single, extremely large-scale and extensible vision dataset. MOSAIC Repository: <a href="https://registry.opendata.aws/mosaic/">https://registry.opendata.aws/mosaic/</a></p>
  </li>
  <li>
    <p><strong>Higher data quality</strong> through an enhanced preprocessing pipeline to improve the signal-to-noise ratio of measured BOLD responses. Building on <em>GLMSingle</em><d-cite key="Prince2022"></d-cite> and Generative Modeling of Signal and Noise (<em>GSN</em><d-cite key="kay2025disentangling"></d-cite>), we are developing <em>PSN</em> (Partitioning of Signal and Noise)—a powerful low-rank denoising method that optimally separates signal from noise in neural data, outperforming trial-averaging and PCA, especially when noise is structured or complex (as in fMRI). PSN Repository: <a href="https://github.com/jacob-prince/PSN">https://github.com/jacob-prince/PSN</a></p>
  </li>
  <li>
    <p><strong>Enhanced generalization</strong> to new participants from outside-of-dataset studies using “BraInCoRL”—a meta-learned in-context foundation model that enables generalization using only a small amount of additional data<d-cite key="yu2025-braInCoRL"></d-cite>. BrainCoRL Repository: <a href="https://github.com/leomqyu/BraInCoRL">https://github.com/leomqyu/BraInCoRL</a></p>
  </li>
  <li>
    <p><strong>Crowd-sourcing infrastructure</strong> to support the continuous integration of data from new studies across unique participants and data collection sites.</p>
  </li>
</ol>

<p>Building on these methodological advances and the lessons learned from distributed large-scale fMRI datasets<d-cite key="chang2019bold5000,allen2022massive,things2023,gong_large-scale_2023,lahner_modeling_2024"></d-cite>, CONFORM will be a unique collaborative modeling strategy that will enable the creation of large-scale vision foundation fMRI models on datasets with improved signal quality, more participants, greater stimulus diversity, and which, critically, generalizes to new participants and studies in low data regimes. Longer term—across labs, participants, and MRI systems, we further propose a “crowd-sourced” community-driven effort to collect and integrate new data, thereby continuously improving the models. Given the challenges of collecting ever-larger and more diverse datasets at a single site, we suggest that crowd-sourcing is the only tenable solution for building appropriate-scale, truly foundational neural datasets. However, developing a viable crowd-sourcing infrastructure at this scale remains an unsolved challenge with a very high risk/reward tradeoff.</p>

<p>We are taking on this challenge by integrating and further developing recent advances in fMRI preprocessing, data aggregation, and generalization. CONFORM will also include the infrastructure for continuously expanding the dataset’s size and the diversity of its stimuli<d-cite key="wang2022incorporating"></d-cite>. Our project will use a two-pronged approach for data contributions: locally directed and globally directed.</p>

<p>The <strong>locally directed</strong> model is straightforward: the CONFORM distribution website will also accept contributions. In contrast to other neural data repositories<d-cite key="markiewicz2021openneuro"></d-cite>, we will provide detailed specifications for the acceptable designs, stimuli, tasks, and data formats to ensure submissions can be seamlessly integrated into CONFORM with high data quality. One attractive aspect of a locally directed model is that CONFORM may be able to re-purpose extant data that was already collected for a different purpose, thus giving new life to data that may have been otherwise dormant for years. At the same time, processing all available public data is not feasible. As an alternative, we will facilitate researchers re-analyzing their datasets with our pipeline. Our goal with the locally directed model is to be as inclusive as possible with stimuli and tasks, even with necessary limitations.</p>

<p>The <strong>globally directed</strong> model is more ambitious and forward-looking, and offers a greater potential payoff. We will provide a complete, turn-key study design to participating research sites, streamlining the data collection process. We will optimize the selection of stimulus images to achieve the best possible distribution of images within natural image space across many participants<d-cite key="rothsample"></d-cite>. We will also optimize for repeated stimuli and partial stimulus overlap across the population. Similarly, we will optimize the study design with respect to scanning parameters and trial structure. Collaborators will be able to specify both the length of scan sessions and the total number of participants they contribute. They will then be provided with complete scan protocols, experimental control files, and stimulus images. An interface on the same website used for distribution will allow them to download these files and upload their collected data for incorporation into the dataset.</p>

<p>CONFORM’s framework towards a scalable foundation fMRI model will enable powerful insights into human vision. Datasets within CONFORM will continue to grow in size and stimulus diversity as the community contributes data. Critically, the resultant models will achieve improved generalization to new participants across diverse subpopulations, requiring only a relatively small amount of data per individual. As such, CONFORM will dramatically broaden the accessibility of NeuroAI methods, empowering researchers in a much wider range of scientific domains to make new discoveries.</p>

<h3 id="improving-data-qualitypsn">Improving Data Quality—PSN</h3>

<p>The recently introduced <em>GLMSingle</em> preprocessing pipeline dramatically improves the signal-to-noise ratio of measured BOLD responses acquired using standard fMRI methods<d-cite key="Prince2022"></d-cite>. In parallel, the Generative Modeling of Signal and Noise technique (<em>GSN</em><d-cite key="kay2025disentangling"></d-cite>) has established a new paradigm for accurately estimating the parameters of the signal and noise distributions that give rise to the observed measurements. We are building upon the GLMSingle and GSN approaches in developing <em>PSN</em> (Partitioning of Signal and Noise)—a low-rank denoising method that optimally separates signal from noise in neural data, improving the performance and interpretability of downstream computational models.</p>

<p>PSN addresses a core challenge in building a truly foundational fMRI dataset by maximizing the amount of stimulus-driven information (signal) that can be recovered from each participant’s measurements, while partitioning out the influence of other sources of variability (noise). Conventional denoising strategies such as trial averaging are straightforward and widely used, but they rely on the assumptions that noise is independent across trials and uncorrelated between voxels. In actuality, these assumptions are often violated in fMRI data, where noise can be structured, spatially correlated, and non-stationary. Similarly, PCA-based low-rank denoising identifies directions of highest variance but does not explicitly distinguish between signal and noise, leading to bias when noise variance is large or when signal and noise share overlapping subspaces<d-cite key="kay2025disentangling,pospisil2024revisiting"></d-cite>.</p>

<p>PSN addresses these limitations by extending the GSN framework<d-cite key="kay2025disentangling"></d-cite> to produce denoised trial-averaged data that are optimized for downstream modeling. GSN first estimates separate covariance structures for the signal and noise directly from repeated-trial measurements. These estimates define a signal-aware basis for low-rank reconstruction, allowing us to then selectively preserve dimensions most likely to reflect stimulus-driven activity while discarding those dominated by noise.</p>

<p>Critically, PSN relies on cross-validation to determine the optimal number of signal dimensions to retain, with thresholds chosen either at the multi-voxel or single-voxel level, depending on the data’s heterogeneity in feature tuning and signal-to-noise ratio. This cross-validated tailoring of denoising parameters will be particularly important given CONFORM’s aim of integrating large, multi-site datasets, where measurement quality can vary widely across participants, scanners, and brain regions.</p>

<p>In simulations with known ground truth, PSN consistently recovers more accurate signal estimates than trial averaging or PCA-based methods, achieving lower variance without introducing substantial bias. Applied to real datasets, including primate electrophysiology and human fMRI, PSN yields substantial gains in cross-validated encoding model performance and improves the interpretability of model-derived feature visualization (manuscript in preparation). In the context of CONFORM, applying PSN to every contributed dataset ensures that all data entering the foundation model are maximally informative, optimized for data quality and reliability, and robust to the structured noise sources inherent in large-scale, crowd-sourced fMRI. Finally, because non-stimulus-driven sources of neural variability may themselves be of scientific interest, PSN also enables these components to be cleanly separated for downstream analyses that focus on modeling noise rather than signal.</p>

<h3 id="integration-of-fmri-data-across-studiesmosaic">Integration of fMRI Data Across Studies—MOSAIC</h3>

<p>Individual fMRI experiments face practical constraints that create trade-offs between the number of participants, the number of experimental trials, and stimulus diversity. Any resulting conclusions are thus limited in scope. However, the aggregation of existing fMRI datasets, here called MOSAIC (Meta-Organized Stimuli And fMRI Imaging data for Computational modeling), achieves a vastly larger scale useful for measuring cross-dataset and cross-subject generalization and training of high-parameter artificial neural networks.</p>

<p>MOSAIC<d-cite key="lahner_mosaic_2025"></d-cite> currently preprocesses eight event-related fMRI vision datasets (Natural Scenes Dataset<d-cite key="allen2022massive"></d-cite>, Natural Object Dataset<d-cite key="gong_large-scale_2023"></d-cite>, BOLD Moments Dataset<d-cite key="lahner_modeling_2024"></d-cite>, BOLD5000<d-cite key="chang2019bold5000"></d-cite>, Human Actions Dataset, Deeprecon<d-cite key="shen2019deep"></d-cite>, Generic Object Decoding<d-cite key="horikawa2017generic"></d-cite>, and THINGS<d-cite key="things2023"></d-cite>) with a shared pipeline and registers all data to the same cortical surface space. Single-trial beta values in MOSAIC are estimated using GLMsingle and a high integrity test-train split is curated across datasets.</p>

<p>At present, MOSAIC contains 430,007 fMRI-stimulus pairs from 93 participants across 162,839 unique image stimuli. The stimuli are further divided into 144,360 training stimuli, 18,145 test stimuli, and 334 synthetic stimuli for rigorous model training and evaluation. Their shared preprocessing pipeline uses open source frameworks and is thus compatible with methods advancements such as PSN and expansion to other registration spaces such as subject native. Crucial to CONFORM, datasets can be added to MOSAIC <em>post-hoc</em> regardless of experimental design, acquisition, and size.</p>

<p>MOSAIC is a critical first step to enable researchers to overcome individual dataset limitations and tackle complex research questions at an unprecedented scale. The MOSAIC dataset and preprocessing code will be available soon for download. In tandem with the MOSAIC team, the larger CONFORM community will work to leverage MOSAIC’s extensible design to allow the seamless integration of new datasets, creating an evolving foundation for collaborative human vision research.</p>

<h3 id="generalizing-across-participants-and-studies-in-a-low-data-regimebraincorl">Generalizing Across Participants and Studies in a Low Data Regime—BraInCoRL</h3>

<p>Different datasets may utilize different stimuli, employ different scanning parameters, and collect data from diverse populations. This makes it challenging to build generalizable models that predict neural activity across diverse participants. Traditional approaches require large, participant-specific fMRI datasets, limiting their scalability for clinical and research applications. This variability in cortical organization—driven by anatomical and functional differences, developmental experiences, and learning—necessitates a framework that can adapt to new individuals with minimal data while capturing shared functional principles of visual processing.</p>

<p>To address this, BrainCoRL (Brain In-Context Representation Learning)<d-cite key="yu2025-braInCoRL"></d-cite> leverages meta-learning and transformer-based in-context learning to predict voxelwise neural responses from few-shot examples without fine-tuning. Inspired by how large language models adapt to new tasks through contextual examples, BrainCoRL treats each voxel’s response function as a learnable mapping that can be inferred from limited data. The model is trained across multiple participants to discover shared functional principles of visual processing, enabling it to rapidly adapt to new individuals without additional fine-tuning.</p>

<p>BrainCoRL outperforms traditional voxelwise encoding models in low-data regimes, generalizes to entirely new fMRI datasets acquired with different scanners and protocols, and provides interpretable insights into cortical selectivity through its attention mechanisms. Notably, the framework can also link neural responses to natural language descriptions, opening new possibilities for query-driven functional mapping of the visual cortex. By dramatically reducing the data requirements for accurate neural encoding models, this work paves the way for more scalable and personalized applications in both basic neuroscience and clinical settings, where understanding individual differences in brain organization is crucial for diagnosis and treatment.</p>

<h2 id="impact-and-conclusions">Impact and Conclusions</h2>

<p>Although existing large-scale fMRI datasets have been valuable, used in hundreds of studies to support a wide range of novel scientific discoveries, they are limited by their single-site, small-N approach. To move beyond this, we propose CONFORM—a unique crowd-sourcing strategy that leverages recent advances in data processing, data aggregation, analysis, and a new crowd-sourced infrastructure. This new approach directly addresses the financial and logistical challenges of collecting large datasets while enabling unprecedented stimulus diversity. However, simply crowd-sourcing data is not enough; CONFORM’s success will be predicated on the specific data and modeling optimizations we introduce to handle the multifaceted noise inherent in fMRI. Moreover, by creating models that can effectively predict new data with only a small amount of information, we will dramatically broaden the accessibility of NeuroAI methods. This will empower a much wider range of researchers to leverage the power of modern AI using the typical scale of data they collect, ultimately accelerating scientific discovery.</p>

<p>Critically, generalizing across individuals requires addressing both biological differences and technical noise sources, such as artifacts from different scanners and motion. We directly tackle these challenges through a three-pronged approach:</p>

<ol>
  <li>
    <p><strong>Data Acquisition</strong>: Collect a limited amount of data from each participant, including repeated and partially overlapping stimuli across the population, to boost both data quality and stimulus diversity.</p>
  </li>
  <li>
    <p><strong>Denoising</strong>: Apply a two-level denoising strategy. Use GLMsingle to optimize the signal-to-noise ratio within each subject and, then, apply PSN to separate stimulus-related variance from idiosyncratic noise, improving data quality and interpretability.</p>
  </li>
  <li>
    <p><strong>Alignment</strong>: Learn a mapping from the denoised data into a shared representational space, thereby allowing us to make accurate predictions across individuals. This can be achieved through advanced methods such as BrainCoRL, which does not require overlapping stimuli, or using standard functional alignment techniques that rely on overlapping stimuli in the denoised data.</p>
  </li>
</ol>

<p>By integrating and advancing these tools to create a true foundational model, we can answer downstream questions using the dataset population to make predictions about new individuals or clinical populations. For example, recent advances in visualizing and labeling neural representations of object categories<d-cite key="luo2023brain,luo2024brainscuba"></d-cite> could be extended to autistic individuals, thereby providing a much clearer picture of the encoding of atypically processed visual information (e.g., human faces). Thus, a wide range of research domains will have access to modern AI methods using only the scale of data they typically collect. Ultimately, this generalizability will enable the next generation of insights into brain function across a much wider range of populations.</p>]]></content><author><name>Michael J. Tarr</name></author><summary type="html"><![CDATA[We propose CONFORM (Crowd-Sourced Open Neuroscience fMRI Foundation Model), a project that will bring together recent advances in neural data processing and analysis with a novel, crowd-sourced infrastructure. This transformative approach will overcome several current challenges in creating a foundational human fMRI model for vision: collecting massive amounts of data from a handful of participants is neither scalable nor sustainable; the number of participants is small for such datasets; stimulus diversity is limited; and generalizability to different populations is poor. CONFORM will overcome these limitations by combining a powerful denoising method (PSN), a scalable framework for aggregating existing fMRI datasets (MOSAIC), and a meta-learning model that enables generalization with much smaller data from new participants (BraInCoRL). Our collaborative effort will produce models built on unprecedented scale and diversity—ultimately with hundreds of participants and hundreds of thousands of naturalistic image and movie stimuli—and provide the tools for continuous expansion of the underlying dataset. This ``crowd-sourced'' approach will allow many more researchers to leverage state-of-the-art NeuroAI methods using the scale of data they typically collect, democratizing access to powerful models and accelerating scientific discovery for a wide range of neuroscientific domains and populations.]]></summary></entry><entry><title type="html">Distill Example3</title><link href="https://data-brain-mind.github.io/tutorials/distill-example3/" rel="alternate" type="text/html" title="Distill Example3" /><published>2025-09-08T00:00:00+08:00</published><updated>2025-09-08T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example3</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example3/"><![CDATA[]]></content><author><name>Anonymous</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations</title><link href="https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/" rel="alternate" type="text/html" title="Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations" /><published>2025-09-05T00:00:00+08:00</published><updated>2025-09-05T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/"><![CDATA[<h2 id="tldr">TL;DR</h2>
<p><strong>ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes to advance research in attention, language, and machine learning.</strong></p>

<h2 id="1-introduction-and-motivation">1. Introduction and motivation</h2>
<h3 id="11-neural-decoding-models-from-science-fiction-to-reality">1.1 Neural decoding models: from science fiction to reality</h3>
<p>Many of us have seen movies or read stories where people can “mind read,” such as Professor X, an exceptionally powerful telepath who can read and control the thoughts of others. While this belongs to science fiction, advances in technology and computation are bringing us closer to decoding aspects of human thought in real life. Neural decoding models powered by modern natural language processing and large language models have begun to approximate how humans process and generate language. These models, in turn, have helped researchers to understand how human brains accomplish the same tasks.</p>
<h3 id="12-simultaneous-eeg-and-eye-tracking-for-cognitive-dataset">1.2 Simultaneous EEG and eye-tracking for cognitive dataset</h3>
<p>Building and refining powerful neural decoding models require large-scale and high-quality cognitive data. A powerful way to capture such cognitive data is to combine eye-tracking with electroencephalography (EEG). Eye-tracking captures gaze positions, which link visual input to specific task stimuli or words at given time. EEG, meanwhile, provides non-invasive and relatively low-cost recordings of brain dynamics with high temporal resolution.</p>
<h3 id="13-what-are-some-challenges">1.3 What are some challenges?</h3>
<h4 id="131-the-rarity-of-datasets-on-natural-reading">1.3.1 The rarity of datasets on natural reading</h4>
<p>Most existing simultaneous EEG and eye-tracking datasets focus on non-linguistic tasks. For example, EEGEyeNet provides large-scale recordings of eye movements while participants view simple visual symbols <d-cite key="Kastrati2021"></d-cite>, and EEGET-RSOD <d-cite key="He2025"></d-cite> records brain and eye activity as participants search for targets in remote sensing images. For reading specifically, the most well-known datasets are ZuCo <d-cite key="Hollenstein2018"></d-cite> and ZuCo 2.0 <d-cite key="Hollenstein2019"></d-cite>, which combine EEG and eye-tracking during sentence-level reading (i.e., each stimulus is a single sentence). These reading resources have become popular and invaluable for advancing EEG-to-text decoding and related computational models.</p>
<h4 id="132-frequent-mind-wandering-off-task-thoughts-during-reading">1.3.2 Frequent mind-wandering (off-task thoughts) during reading</h4>
<p>You might argue that existing datasets could be sufficient for neural decoding tasks if we apply data augmentation techniques. But there’s another, even more fundamental challenge: human attention as a confunding factor. Just as “Attention is all you need” in computational models <d-cite key="Vaswani2017"></d-cite>, attention is also critical in humans. Our attention is not static: it fluctuates with arousal, fatigue, mood, and motivation <d-cite key="Shen2024, Smallwood2009"></d-cite>, and these states alter how we engage with language and modulate behaviors and neural activity <d-cite key="Smallwood2011, Unsworth2013"></d-cite>. One particularly common and impactful attention state is <strong>mind-wandering (MW)</strong>, when our focus drifts from the task at hand to unrelated thoughts. <strong>People spend 30% to 60% of their daily lives mind-wandering</strong> <d-cite key="Killingsworth2010"></d-cite>, and it happens frequently during reading <d-cite key="Smallwood2011"></d-cite>. You might even find yourself mind-wandering while reading this blog post. While the exact cognitive processes behind MW are still unclear, one leading hypothesis, the perceptual decoupling hypothesis <d-cite key="Smallwood2006"></d-cite>, suggests that internal thoughts during MW divert resources away from external stimuli. This diversion can directly affect how language is processed. For these reasons, accounting for MW is not just interesting; it is essential for building models that can mechanistically and predictively capture real human language processing.</p>
<h3 id="14-mind-wandering-is-sneaky-so-how-do-we-study-it">1.4 Mind-wandering is sneaky, so how do we study it?</h3>
<h4 id="141-previous-approaches-and-their-drawbacks">1.4.1 Previous approaches and their drawbacks</h4>
<p>Now let’s take a brief detour into psychology. Studying constructs like MW can include unique challenges. Prior research has primarily relied on self-reports and thought probes to detect episodes of MW. Self-reports depend on participants’ subjective awareness: whenever individuals realize that their minds have drifted from the task, they are instructed to report it <d-cite key="Schooler2002"></d-cite>. Thought probes, by contrast, are randomly timed prompts that require participants to indicate whether they are currently on-task or mind-wandering <d-cite key="Giambra1995, Smallwood2006"></d-cite>. While both approaches are widely used in contexts such as reading <d-cite key="Reichle2010, Broadway2015, Faber2017"></d-cite>, they only mark when MW ends, making it difficult to capture its onset or to characterize how episodes naturally unfold over time, temporal features that are crucial for understanding and detecting MW.</p>
<h4 id="142-our-novel-remind-paradigm">1.4.2 Our novel ReMind paradigm</h4>
<p>So how did we solve this problem? I spent days reading research articles, and my own wandering mind made me reread passages over and over just to comprehend the material. Suddenly, it hit me: <strong>when we reread after getting lost, we are implicitly marking the parts of the text where our attention drifted.</strong> That observation inspired the ReMind paradigm, which estimates the onset and duration of MW episodes during reading by combining retrospective self-reports with eye-tracking. Participants indicate the words where they believe their mind started and stopped wandering for each episode. We then align these selections with gaze timestamps to estimate precise onset and offset times.</p>

<h3 id="15-introducing-the-roamm-dataset">1.5 Introducing the ROAMM dataset</h3>
<p>Using this approach, we created the Reading Observed at Mindless Moments (ROAMM) dataset, which contains simultaneous EEG and eye-tracking data from 44 participants (30 females, 9 males, and 5 non-binary) performing naturalistic reading in the ReMind paradigm. The dataset includes rich labels, such as the word at each fixation, attention state at each sample, and a comprehension question score for each page. By capturing attention states in a naturalistic and uninterrupted way, ROAMM provides a powerful resource for studying questions in language and cognition, and for developing language-based machine learning models that better reflect real human attention.</p>

<p>In the remainder of the post, we present the ROAMM Dataset including the participant demographics, task, data acquisition and pre-processing, and methodology for identifying mind wandering. We include figures of data validation that demonstrate the quality and temporal dynamics of the data. Finally, we describe the availability of the data and discuss potential applications including open questions for ML practitioners and examples of candidate models.</p>

<h2 id="2-roamm-dataset">2. ROAMM dataset</h2>
<h3 id="21-participants">2.1 Participants</h3>
<p>We recruited 58 participants from the University of Vermont who were fluent in English and reported no family history of neurological disorders or epilepsy. All participants underwent screening and provided informed consent before participation. The study protocol was approved by the university Institutional Review Board. 14 participants were excluded due to issues such as equipment difficulties, incomplete experimental runs, monocular-only eye-tracking data, or missing demographic information. The final dataset includes <strong>44 participants</strong>. The participants’ age ranged from 18 to 64 years (Mean = 22.6, SD = 7.8, Median = 20, Mode = 19). This indicates a relatively young but moderately varied sample. Our sample also includes labels for gender, handedness, and self-identified ADHD.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/demographics-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/demographics-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/demographics-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/demographics.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<h3 id="22-the-remind-paradigm">2.2 The ReMind paradigm</h3>
<p>Participants read five articles selected from Wikipedia (2015): <strong>Pluto</strong> (the dwarf planet), <strong>the Prisoner’s Dilemma</strong>, <strong>Serena Williams</strong>, <strong>the History of Film</strong>, and <strong>the Voynich Manuscript</strong>. These topics were chosen to be unfamiliar yet comprehensible without prior background knowledge. Each article was standardized by removing images and jargon, then divided into 10 pages (≈220 words per page). Pages were rendered using a custom Python script into 16 lines of black Courier-font text on a gray background. To encourage engagement and assess comprehension, we created one multiple-choice question per page. Each question was designed to require attention to that page alone.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<p>The reading task was programmed using PsychoPy <d-cite key="Peirce2019"></d-cite>, a platform for developing psychological experiments. Each experimental session consisted of five runs, one for each article. Articles were presented in a randomized order. Before the first run, participants received task instructions and an explicit definition of mind-wandering (<em>see above figure</em>). Participants read at their own pace with no time limit per page but could not re-read previous pages. If they noticed themselves mind-wandering, they pressed the “F” key to access a dedicated reporting screen. There, they clicked on the words marking where they believed the MW episode began and ended (highlighted onscreen for clarity). If the episode began on the prior page, they marked the first word of the current page. After submitting the report, they returned to the same page to resume reading. For consistency, only one MW report was permitted per page.</p>

<h3 id="23-data-acquisition-and-preprocessing">2.3 Data acquisition and preprocessing</h3>
<h4 id="231-eye-tracking">2.3.1 Eye-tracking</h4>
<p>The experiment was conducted in a soundproof booth to minimize distractions. We used an <strong>SR Research EyeLink 1000 Plus eye tracker</strong> to record binocular eye movements and pupil area at 1000 Hz. Each run began with calibration, repeated until EyeLink reported good accuracy (worst error &lt; 1.5°, average error &lt; 1.0°). Once calibration was complete, we recorded eye movements in sync with EEG while participants performed the reading task. PsychoPy sent page-onset triggers to both systems to keep the data streams aligned.</p>

<p>EyeLink automatically detected fixations, saccades, and blinks using default thresholds. These events were parsed into data frames, and fixations were mapped to individual words on the screen using spatial coordinates. Because pupil size data can be unreliable around blinks (due to eyelid occlusion), we corrected for this by linearly interpolating pupil size using values from the saccades surrounding each blink.</p>

<h4 id="232-eeg">2.3.2 EEG</h4>
<p>We recorded simultaneous EEG using a <strong>BioSemi ActiveTwo 64-channel system</strong> at 2048 Hz. Before starting the task, we ensured all electrodes had stable connections by checking impedances and correcting any channels with unusually high values. Collected data were preprocessed in EEGLAB <d-cite key="Delorme2004"></d-cite>: resampled to 256 Hz, re-referenced, filtered (0.5–50 Hz), and channels with poor signal quality interpolated. Eye and muscle artifacts were removed using independent component analysis (ICA) with standard EEGLAB parameters.</p>

<h3 id="24-dataset-format">2.4 Dataset format</h3>
<p>We made the ROAMM dataset easy to work with by aligning eye-tracking data to 64-channel EEG at 256 Hz. We downsampled the eye-tracking data using the real-time arrays: for each EEG time point, we identified the closest corresponding eye-tracking sample and used the pupil size at that moment. Fixations, saccades, and blinks were directly mapped using their start and end times relative to the EEG time array.</p>

<p>All data are stored in pandas DataFrames (.pkl format, <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.8+-blue.svg" alt="Python 3.8+" /></a>) for fast loading and smaller file size (compared to .csv format). Each participant has one .pkl file per run, with a total of 5 runs. Eye-tracking events like fixations, saccades, and blinks are expanded across their start-to-end times. For example, if a fixation occurs from 10 to 11 seconds, all samples within that 1-second window are annotated with <code class="language-plaintext highlighter-rouge">is_fixation = 1</code>, <code class="language-plaintext highlighter-rouge">fix_tStart = 10</code>, <code class="language-plaintext highlighter-rouge">fix_tEnd = 11</code>, <code class="language-plaintext highlighter-rouge">fix_duration = 1</code>, etc. Time stamps, page boundaries, and mind-wandering episodes are all included, along with metadata such as sampling frequency, run numbers, page numbers, and story names.</p>

<p>To maintain clarity and focus on natural reading, we defined <strong>first-pass reading</strong> as the period when participants were initially reading the text. Activities such as reading task instructions, marking mind-wandering pages, rereading after a mind-wandering report, or answering comprehension questions were excluded from this category. Each sample is labeled for first-pass reading, mind-wandering, and fixated words. Each fixated word also includes a key linking it to the original text, making it easy to generate embeddings or other vectorized representations within the context of the reading corpus for computational modeling. Additional information, including <strong>subject demographic information</strong> and <strong>comprehension question scores</strong> (with corresponding  run and page numbers), and <strong>EEG channel locations</strong> is saved in separate files for easy access.</p>

<table>
  <thead>
    <tr>
      <th>Data Category</th>
      <th>Column Count</th>
      <th>Description</th>
      <th>Example Columns</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>EEG</strong></td>
      <td>64</td>
      <td>64 electrode EEG signals</td>
      <td><code class="language-plaintext highlighter-rouge">Fp1, AF7, AF3, F1, F3...</code></td>
    </tr>
    <tr>
      <td><strong>Eye-Tracking</strong></td>
      <td>38</td>
      <td>Gaze position, pupil size, fixations, saccades, and blinks</td>
      <td><code class="language-plaintext highlighter-rouge">is_fixation, fix_eye, fix_tStart, fix_tEnd, fix_duration...</code></td>
    </tr>
    <tr>
      <td><strong>Time</strong></td>
      <td>8</td>
      <td>Timestamps, page boundaries, durations, and MW episode info</td>
      <td><code class="language-plaintext highlighter-rouge">time, page_start, page_end, page_dur, mw_onset...</code></td>
    </tr>
    <tr>
      <td><strong>Others</strong></td>
      <td>4</td>
      <td>Sampling frequency, run and page numbers, story name</td>
      <td><code class="language-plaintext highlighter-rouge">sfreq, page_num, run_num, story_name</code></td>
    </tr>
    <tr>
      <td><strong>Labels</strong></td>
      <td>4</td>
      <td>First-pass reading, MW, and fixated word annotations</td>
      <td><code class="language-plaintext highlighter-rouge">first_pass_reading, is_mw, fix_fixed_word, fix_fixed_word_key</code></td>
    </tr>
  </tbody>
</table>

<h3 id="25-dataset-scale">2.5 Dataset scale</h3>
<p>The ROAMM dataset is large and rich. It contains over 46 million recorded samples from 44 participants, totaling more than 50 hours of simultaneous EEG and eye-tracking data. Of these, over 26 million samples (around 30 hours) correspond to first-pass reading, which includes fixated word information at each sample. Across the 2,200 pages read, participants reported 998 mind-wandering episodes, corresponding to 45.4% of the pages. These episodes had a median duration of 5.92 seconds and a mean of 7.79 seconds and resulted in a total of 2.2 hours of reading time annotated as mind-wandering.</p>

<table>
  <thead>
    <tr>
      <th>Data Type</th>
      <th>Total Sample Count</th>
      <th>Total Time (h)</th>
      <th>Subject Avg Time (m)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Total Recording</strong></td>
      <td>46,371,584</td>
      <td>50.3</td>
      <td>68.6</td>
    </tr>
    <tr>
      <td><strong>First-Pass Reading</strong></td>
      <td>26,691,014</td>
      <td>29.0</td>
      <td>39.5</td>
    </tr>
    <tr>
      <td><strong>Mind-Wandering</strong></td>
      <td>2,045,021</td>
      <td>2.2</td>
      <td>3.0</td>
    </tr>
    <tr>
      <td><strong>Fixation</strong></td>
      <td>38,324,700</td>
      <td>41.6</td>
      <td>56.7</td>
    </tr>
    <tr>
      <td><strong>Saccade</strong></td>
      <td>9,326,633</td>
      <td>10.1</td>
      <td>13.8</td>
    </tr>
    <tr>
      <td><strong>Blink</strong></td>
      <td>2,290,418</td>
      <td>2.5</td>
      <td>3.4</td>
    </tr>
  </tbody>
</table>

<p>The histograms below illustrate the distribution of data across participants for each attribute in the ROAMM dataset. While all participants contributed, individual differences are evident in the distributions. This highlights the real-world variability in human data and underscores the importance of carefully considering modeling approaches, whether developing a general model across participants or an individualized classifier tailored to each person.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/data_scale-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/data_scale-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/data_scale-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/data_scale.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<h3 id="26-data-validation">2.6 Data validation</h3>
<p>We validated the ROAMM dataset to ensure high recording quality, precise alignment between EEG and eye-tracking data streams, accurate fixation-to-word mappings, and reliable labeling of MW episodes.</p>
<h4 id="261-eeg-and-eye-tracking-recoding-quality">2.6.1 EEG and eye-tracking recoding quality</h4>
<p>To assess recording quality, we inspected EEG and eye-tracking signals in parallel. As demonstrated below using a randomly selected 10-second window, preprocessed EEG from selected channels show <strong>clean activity with minimal muscle and eye artifacts</strong>. Eye-tracking features behave as expected: <strong>fixations are followed by saccades, blinks appeared distinctly</strong>, and <strong>gaze position traces reveal typical reading patterns</strong>. Specifically, x-coordinates increase left to right across each line, while y-coordinates step down across successive lines, confirming naturalistic line-by-line reading. For context, the top left of the displayed reading page is (x,y) = (258, 5.4) and the bottom right is (x,y) = (1662, 1074.6).</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<h4 id="262-eeg-and-eye-tracking-alignment">2.6.2 EEG and eye-tracking alignment</h4>
<p>Next, we validated alignment between EEG and eye-tracking streams. Using the unfold toolbox, we deconvolved fixation-related potentials (FRPs) during periods of reading without MW. <strong>The resulting FRPs and P1 topography replicated patterns reported using the ZuCo 2.0 dataset</strong> <d-cite key="Hollenstein2019"></d-cite>, providing strong evidence for the temporal precision of our co-registered recordings.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<h4 id="263-fixation-to-word-mapping">2.6.3 Fixation-to-word mapping</h4>
<p>We also validated fixation-to-word mappings by plotting gaze traces directly on reading pages. In one example, a participant read mindfully without reporting MW; in another, the same participant reported an MW episode. Onset and offset words of the MW episode were highlighted in red, while fixations appeared as colored dots. Larger dots indicated longer durations, and a purple-to-yellow gradient reflected temporal order. Fixations within MW episodes were additionally center-colored in red, and consecutive fixations were linked by red lines to mark saccades. <strong>Most fixations aligned neatly with words, and the gaze traces showed clear left-to-right reading flows, confirming the accuracy of fixation-to-word mapping.</strong></p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<h4 id="264-reliable-mw-onset">2.6.4 Reliable MW onset</h4>
<p>Finally, we validated MW onset labeling. In a paper currently under review, we demonstrated that incorporating MW onset information significantly improves the performance of linear regression classifiers trained to detect MW from eye-tracking features. A sliding-window analysis not only replicated prior findings of reduced fixation rates during MW episodes <d-cite key="Reichle2010"></d-cite>, but also revealed that these changes begin precisely at the reported MW onset. These findings demonstrate that the <strong>ReMind paradigm provides a powerful framework for capturing MW onset and its progression over time</strong>, ensuring that our attention state annotations are precise and grounded in reliable MW onset information.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eye_mwonset-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eye_mwonset-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eye_mwonset-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eye_mwonset.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
<h3 id="27-data-accessibility-and-availability">2.7 Data accessibility and availability</h3>
<p>The processed datasets are publicly available on the <a href="https://osf.io/kmvgb/overview">OSF</a>. Due to their large size, raw datasets are not hosted online but are available upon request. All preprocessing scripts used to generate the processed datasets are available in the <a href="https://github.com/GlassBrainLab/ROAMM.git">GitHub repository</a>.</p>

<h3 id="28-how-to-use-roamm">2.8 How to use ROAMM</h3>
<p>We put a lot of effort into making ROAMM easy to use, even if you’ve never worked with EEG or eye-tracking data before. Everything is pre-aligned and stored in <strong>pandas DataFrames</strong>, so you can load it with just a few lines of Python.</p>

<p>Once you download the dataset from <a href="https://osf.io/kmvgb/overview">OSF</a>, here’s how you can get started:</p>

<p><strong>1. Import packages and set up paths</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># define data root
# this is the path to the ROAMM folder on local machine
</span><span class="n">roamm_root</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">/Users/~/ROAMM/</span><span class="sh">"</span> <span class="c1"># change this to your path
</span><span class="n">ml_data_root</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">roamm_root</span><span class="p">,</span> <span class="sh">'</span><span class="s">subject_ml_data</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>
<p><strong>2. Load a single run for one subject</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subject_id</span> <span class="o">=</span> <span class="sh">'</span><span class="s">s10014</span><span class="sh">'</span>
<span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
<span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s">_ml_data.pkl</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>3. Load all runs for one subject</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
    <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
    <span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_sub_all_runs</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
</code></pre></div></div>

<p><strong>4. Load all subjects, filtered to first-pass reading</strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load all runs for all subjects
</span><span class="n">all_subjects</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">d</span><span class="p">))]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">subject_id</span> <span class="ow">in</span> <span class="n">all_subjects</span><span class="p">:</span>
    <span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
    <span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>

    <span class="c1"># make sure each subject has 5 runs of data
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span><span class="si">}</span><span class="s"> runs instead of 5</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
        <span class="c1"># I highly recommend you to filter out reading runs that are not the first pass reading
</span>        <span class="c1"># to save memory
</span>        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># add subject id to the dataframe   
</span>        <span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">subject_id</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">subject_id</span>
        <span class="c1"># convert bool col explicitly to avoid pandas warning
</span>        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">is_blink</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_saccade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_fixation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_mw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]:</span>
            <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">==</span> <span class="bp">True</span>
        <span class="c1"># append to the dataframe
</span>        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has been loaded.</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="3-open-questions-for-ml-practitioners-using-roamm">3. Open questions for ML practitioners using ROAMM</h2>

<p>The ROAMM dataset is rich in scope, combining multiple valuable modalities: <strong>eye-tracking</strong> (gaze position and pupil size), <strong>brain signals</strong> (i.e., EEG), <strong>human attention states</strong>, and <strong>linguistic content</strong> (the reading text itself). This multimodal design provides countless opportunities for machine learning practitioners to explore how these signals interact. Below, we highlight 4 open questions that showcase the potential of ROAMM for advancing both cognitive science and computational modeling.</p>

<div class="l-page">
  <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/roamm_modalities-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/roamm_modalities-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/roamm_modalities-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/roamm_modalities.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>

<h3 id="31-learn-shared-representation-from-eeg-and-eye-tracking">3.1 Learn shared representation from EEG and eye-tracking</h3>
<p>Eyes, particularly pupil size, reveal much about internal cognitive states and arousal <d-cite key="Castellotti2025"></d-cite>. With a dataset at the scale of ROAMM, we can now ask whether eye-tracking and EEG share common patterns of variance that allow them to be tightly linked.</p>

<p>One way to explore this question is to use a multi-modal embedding method like CLIP <d-cite key="Radford2021"></d-cite>, which was originally applied to learn joint representations of images and text via contrastive learning. A <strong>CLIP-style</strong> model could be trained on ROAMM by contrasting matched and mismatched EEG and eye-tracking segments. If two modalities indeed have features that share significant variance, the resulting embeddings would enable accurate cross-modal classification. Beyond classification, shared representation opens the door to conditional decoding from one modality to the other. For example, one can train a <strong>diffusion transformer model (DiT)</strong> <d-cite key="Peebles2022"></d-cite> to reconstruct a subject’s EEG signals using eye-tracking data as a generative prior. Since there exists a shared representation space of EEG and eye-tracking, the DiT can fuse the information from eye-tracking data through cross-attention.</p>

<h3 id="32-build-a-momentary-human-attention-decoder">3.2 Build a momentary human attention decoder</h3>
<p>EEG and eye-tracking are not the only modalities in ROAMM that may exhibit strong associations. Previous studies have trained models to detect attention states from EEG and eye-tracking separately. We hypothesize that, when combined together, they can provide a more reliable estimate of a subject’s attention state during reading.</p>

<p>To evaluate this hypothesis, one could train simple classifiers such as regression models, SVM, gradient boosting, or neural models that takes the shared representations of EEG and eye-tracking at a given moment to predict the subject’s attention state at that same time step. However, attention is not merely a transient experience; it unfolds dynamically over time. Thus, signals from a single moment are unlikely to provide sufficient information for accurate prediction. To capture attention’s temporal dynamics, one can train sequential models like <strong>long short-term memory networks</strong>, <strong>temporal convolutional networks</strong>, or <strong>generative transformers</strong>, which have demonstrated superior performance in modeling text data. These models can be trained to predict future attention states from EEG, eye-tracking, and attention states from both current and prior windows, enabling moment-to-moment attention decoding.</p>

<h3 id="33-is-human-attention-what-we-need-for-neural-decoding">3.3 Is human attention what we need for neural decoding?</h3>
<p>Decoding neural signals has become a popular topic, with several studies attempting to use EEG to decode text <d-cite key="Liu2024, Wang2024"></d-cite>. However, their main focus was on the attention mechanisms in transformers, but they largely neglected fluctuations in human attention during the task. This can be problematic: during mind-wandering, readers still maintain the outward behavior of reading, moving their eyes from left to right and line by line, but their visual input is disrupted by internal thoughts. Cognitive resources that should be allocated to word recognition and comprehension are instead consumed by spontaneous mental activity. In other words, the brain itself cannot fully follow what the eyes are reading during mind-wandering. This raises a fascinating question: <strong>does knowledge of human attention states improve neural decoding performance?</strong></p>

<p>A straightforward way to address this question is to train an EEG2text decoder separately on data segments from normal reading versus mind-wandering. <strong>Performance differences between these conditions would reveal whether filtering out periods of “mindless reading” provides a decoding advantage.</strong> If we observe better performance when training only on attentive reading, the result would be intuitive: how could a model recover information that the brain itself fails to process?</p>

<p>However, the more intriguing possibility is if decoding performance remains similar regardless of attention state. This would suggest that information about the text is encoded at lower levels of the visual or sensory hierarchy. In such a scenario, the model may be able to extract signals from early visual or pre-attentive neural activity that are not available to conscious awareness. This opens up provocative implications: <strong>machine learning models could potentially reveal implicit or subliminal processing of linguistic information in the brain.</strong> In other words, this is not a mind-reader but something at another level: a <strong>subconscious-reader</strong> that can uncover information from your brain even when you are not aware. It’s as if your neurons are whispering secrets that only the model can hear.</p>

<p>Thus, testing whether attention modulates neural decoding performance not only has practical implications for building better brain–computer interfaces, but also addresses fundamental cognitive neuroscience questions about the boundary between unconscious encoding and conscious comprehension.</p>

<h3 id="34-use-eeg-eye-tracking-human-attention-and-reading-text-to-predict-comprehension">3.4 Use EEG, eye-tracking, human attention, and reading text to predict comprehension</h3>
<p>The previous questions focused on the link between pairs of modalities in our dataset. But as the saying goes, <em>“only children make choices, adults want it all!”</em> For machine learning experts who are not satisfied with pairwise associations and eager to showcase the full power of multimodal modeling, the next challenge is to use all available modalities from ROAMM: EEG, eye-tracking, attention states, and reading text itself. The task we propose is to predict reading comprehension, using ROAMM’s page-level comprehension scores. While page-level labels are coarse and may not perfectly reflect moment-to-moment understanding, they still provide a valuable proxy for comprehension that can anchor multimodal learning.</p>

<p>This problem requires complex model architectures and training frameworks that can integrate heterogeneous data streams. One can opt for the <strong>traditional fusion methods like early fusion</strong> in which features from all modalities are concatenated and processed jointly within a single parameterized model. Although considered a traditional technique, early fusion remains prevalent in recent large-scale multimodal systems for text and images (e.g., <d-cite key="ChameleonTeam2024, Lin2024"></d-cite> early fusion via concatenation, late fusion via ensembling, or hybrid fusion). Besides fusion, we can also train individual models to embed each modality. Examples of this <strong>late fusion include CLIP and Imagine Bind</strong> <d-cite key="Radford2021, Girdhar2023"></d-cite> which trains transformer encoders to map multi-modal data across into an embedding space. Downstream tasks, such as comprehension prediction, can be done by training lightweight classification on top of the shared embeddings. When applying late fusion on ROAMM, one can follow the existing practice, using the same architecture to embed data from all modalities. Alternatively, they can use a specific architecture with inductive bias that accommodates the invariance present in the data (e.g., graph neural network (GNN) for EEG data <d-cite key="Klepl2023"></d-cite>).</p>

<p>A model that successfully predicts comprehension from this rich multimodal space would not only advance cognitive modeling, but also contribute to the emerging field of Foundation Models for the Brain and Body (<em>Yes, another workshop hosted this year</em>). By integrating physiological, behavioral, and linguistic signals into a single predictive framework, we move closer to general-purpose models of human cognition. A recent Nature study <d-cite key="Binz2025"></d-cite> shows that large-scale multimodal learning can capture human behavior across a wide range of domains. Extending these ideas to ROAMM provides an opportunity to build neurocognitive foundation models during naturalistic reading environments.</p>

<h2 id="4-conclusions">4. Conclusions</h2>

<p>In this work, we introduced the <strong>Reading Observed At Mindless Moments (ROAMM)</strong> dataset, a large-scale, multimodal resource capturing simultaneous EEG and eye-tracking data during naturalistic reading. By using the ReMind paradigm, ROAMM stands out among existing reading datasets by providing a highly naturalistic reading environment, temporally resolved attention labels, and precise alignment between neural and behavioral signals.</p>

<p>We provided an overview of the dataset’s acquisition, preprocessing, and structure, highlighting its <strong>scale, richness, and quality</strong>. Validation analyses confirmed <strong>high-fidelity recordings</strong>, <strong>accurate fixation-to-word mappings</strong>, and <strong>reliable labeling of mind-wandering episodes</strong>, making ROAMM suitable for rigorous cognitive and computational modeling.</p>

<p>Beyond describing the dataset, we outlined a set of open questions that illustrate its potential for advancing both neuroscience and machine learning. These include 1) learning shared representations between EEG and eye-tracking, 2) building moment-to-moment attention decoders, 3) investigating the role of attention in neural decoding, and 4) predicting reading comprehension using fully multimodal data. ROAMM thus provides a unique opportunity to explore the interactions between brain signals, eye movements, attention, and language, enabling development of models that better reflect real human cognition.</p>

<p>In summary, ROAMM not only offers a rich resource for fundamental research on attention and reading but also serves as a platform for developing advanced multimodal machine learning models. By bridging cognitive science and computational modeling, it paves the way toward neurocognitive foundation models capable of capturing complex and naturalistic human behavior.</p>]]></content><author><name>Haorui Sun</name></author><summary type="html"><![CDATA[ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking during naturalistic reading, with precise annotations of mind-wandering episodes to advance research in attention, language, and machine learning.]]></summary></entry><entry><title type="html">Sample Blog Post</title><link href="https://data-brain-mind.github.io/tutorials/distill-example/" rel="alternate" type="text/html" title="Sample Blog Post" /><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example/"><![CDATA[<p>Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code class="language-plaintext highlighter-rouge">$$</code>, like <code class="language-plaintext highlighter-rouge">$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code class="language-plaintext highlighter-rouge">$$</code> and place it as a separate paragraph.
Here is an example:</p>

\[\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)\]

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> 
that brought a significant improvement to the loading and rendering speed, which is now 
<a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<h2 id="images-and-figures">Images and Figures</h2>

<p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
To include images in your submission in this way, you must do something like the following:</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}
</code></pre></div></div>

<p>which results in the following image:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>To ensure that there are no namespace conflicts, you must save your asset to your unique directory
<code class="language-plaintext highlighter-rouge">/assets/img/2025-04-28-[SUBMISSION NAME]</code> within your submission.</p>

<p>Please avoid using the direct markdown method of embedding images; they may not be properly resized.
Some more complex ways to load images (note the different styles of the shapes/shadows):</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/9-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/9.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/8-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/8.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/10-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/10.jpg" class="img-fluid z-depth-2" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/11-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/11.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/12-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/12.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

    </div>
</div>

<h3 id="interactive-figures">Interactive Figures</h3>

<p>Here’s how you could embed interactive figures that have been exported as HTML files.
Note that we will be using plotly for this demo, but anything built off of HTML should work
(<strong>no extra javascript is allowed!</strong>).
All that’s required is for you to export your figure into HTML format, and make sure that the file
exists in the <code class="language-plaintext highlighter-rouge">assets/html/[SUBMISSION NAME]/</code> directory in this repository’s root directory.
To embed it into any page, simply insert the following code anywhere into your page.</p>

<div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% include [FIGURE_NAME].html %} 
</code></pre></div></div>

<p>For example, the following code can be used to generate the figure underneath it.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">plotly.express</span> <span class="k">as</span> <span class="n">px</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="sh">'</span><span class="s">https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv</span><span class="sh">'</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">px</span><span class="p">.</span><span class="nf">density_mapbox</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">lat</span><span class="o">=</span><span class="sh">'</span><span class="s">Latitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="sh">'</span><span class="s">Longitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="sh">'</span><span class="s">Magnitude</span><span class="sh">'</span><span class="p">,</span> <span class="n">radius</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">center</span><span class="o">=</span><span class="nf">dict</span><span class="p">(</span><span class="n">lat</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">lon</span><span class="o">=</span><span class="mi">180</span><span class="p">),</span> <span class="n">zoom</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">mapbox_style</span><span class="o">=</span><span class="sh">"</span><span class="s">stamen-terrain</span><span class="sh">"</span><span class="p">)</span>
<span class="n">fig</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="n">fig</span><span class="p">.</span><span class="nf">write_html</span><span class="p">(</span><span class="sh">'</span><span class="s">./assets/html/2025-04-28-distill-example/plotly_demo_1.html</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<p>And then include it with the following:</p>

<div class="language-html highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;div</span> <span class="na">class=</span><span class="s">"l-page"</span><span class="nt">&gt;</span>
  <span class="nt">&lt;iframe</span> <span class="na">src=</span><span class="s">"{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}"</span> <span class="na">frameborder=</span><span class="s">'0'</span> <span class="na">scrolling=</span><span class="s">'no'</span> <span class="na">height=</span><span class="s">"600px"</span> <span class="na">width=</span><span class="s">"100%"</span><span class="nt">&gt;&lt;/iframe&gt;</span>
<span class="nt">&lt;/div&gt;</span>
</code></pre></div></div>

<p>Voila!</p>

<div class="l-page">
  <iframe src="/tutorials/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder="0" scrolling="no" height="600px" width="100%"></iframe>
</div>

<h2 id="citations">Citations</h2>

<p>Citations are then used in the article body with the <code class="language-plaintext highlighter-rouge">&lt;d-cite&gt;</code> tag.
The key attribute is a reference to the id provided in the bibliography.
The key attribute can take multiple ids, separated by commas.</p>

<p>The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
If you have an appendix, a bibliography is automatically created and populated in it.</p>

<p>Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well — the authors are human and it’s nice for them to have the community associate them with their work.</p>

<hr />

<h2 id="footnotes">Footnotes</h2>

<p>Just wrap the text you would like to show up in a footnote in a <code class="language-plaintext highlighter-rouge">&lt;d-footnote&gt;</code> tag.
The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote></p>

<hr />

<h2 id="code-blocks">Code Blocks</h2>

<p>This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
It supports more than 100 languages.
This example is in C++.
All you have to do is wrap your code in a liquid tag:</p>

<p>{% highlight c++ linenos %}  <br /> code code code <br /> {% endhighlight %}</p>

<p>The keyword <code class="language-plaintext highlighter-rouge">linenos</code> triggers display of line numbers. You can try toggling it on or off yourself below:</p>

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>

<hr />

<h2 id="diagrams">Diagrams</h2>

<p>This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> plugin.
Below, we generate a few examples of such diagrams using languages such as <a href="https://mermaid-js.github.io/mermaid/" target="\_blank">mermaid</a>, <a href="https://plantuml.com/" target="\_blank">plantuml</a>, <a href="https://vega.github.io/vega-lite/" target="\_blank">vega-lite</a>, etc.</p>

<p><strong>Note:</strong> different diagram-generation packages require external dependencies to be installed on your machine.
Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW.
For any other details, please refer to <a href="https://github.com/zhustec/jekyll-diagrams" target="\_blank">jekyll-diagrams</a> README.</p>

<p><strong>Note:</strong> This is not supported for local rendering!</p>

<p>The diagram below was generated by the following code:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice-&gt;&gt;John: Hello John, how are you?
    John--&gt;&gt;Alice: Great!
{% endmermaid %}
</code></pre></div></div>

<div class="jekyll-diagrams diagrams mermaid">
  <svg id="mermaid-1765319089551" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1765319089551 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1765319089551 .node circle,#mermaid-1765319089551 .node ellipse,#mermaid-1765319089551 .node polygon,#mermaid-1765319089551 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1765319089551 .node.clickable{cursor:pointer}#mermaid-1765319089551 .arrowheadPath{fill:#333}#mermaid-1765319089551 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1765319089551 .edgeLabel{background-color:#e8e8e8}#mermaid-1765319089551 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1765319089551 .cluster text{fill:#333}#mermaid-1765319089551 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1765319089551 .actor{stroke:#ccf;fill:#ececff}#mermaid-1765319089551 text.actor{fill:#000;stroke:none}#mermaid-1765319089551 .actor-line{stroke:grey}#mermaid-1765319089551 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1765319089551 .messageLine0,#mermaid-1765319089551 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1765319089551 #arrowhead{fill:#333}#mermaid-1765319089551 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1765319089551 .messageText{fill:#333;stroke:none}#mermaid-1765319089551 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1765319089551 .labelText,#mermaid-1765319089551 .loopText{fill:#000;stroke:none}#mermaid-1765319089551 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1765319089551 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1765319089551 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1765319089551 .section{stroke:none;opacity:.2}#mermaid-1765319089551 .section0{fill:rgba(102,102,255,.49)}#mermaid-1765319089551 .section2{fill:#fff400}#mermaid-1765319089551 .section1,#mermaid-1765319089551 .section3{fill:#fff;opacity:.2}#mermaid-1765319089551 .sectionTitle0,#mermaid-1765319089551 .sectionTitle1,#mermaid-1765319089551 .sectionTitle2,#mermaid-1765319089551 .sectionTitle3{fill:#333}#mermaid-1765319089551 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1765319089551 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1765319089551 .grid path{stroke-width:0}#mermaid-1765319089551 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1765319089551 .task{stroke-width:2}#mermaid-1765319089551 .taskText{text-anchor:middle;font-size:11px}#mermaid-1765319089551 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1765319089551 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1765319089551 .taskText0,#mermaid-1765319089551 .taskText1,#mermaid-1765319089551 .taskText2,#mermaid-1765319089551 .taskText3{fill:#fff}#mermaid-1765319089551 .task0,#mermaid-1765319089551 .task1,#mermaid-1765319089551 .task2,#mermaid-1765319089551 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1765319089551 .taskTextOutside0,#mermaid-1765319089551 .taskTextOutside1,#mermaid-1765319089551 .taskTextOutside2,#mermaid-1765319089551 .taskTextOutside3{fill:#000}#mermaid-1765319089551 .active0,#mermaid-1765319089551 .active1,#mermaid-1765319089551 .active2,#mermaid-1765319089551 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1765319089551 .activeText0,#mermaid-1765319089551 .activeText1,#mermaid-1765319089551 .activeText2,#mermaid-1765319089551 .activeText3{fill:#000!important}#mermaid-1765319089551 .done0,#mermaid-1765319089551 .done1,#mermaid-1765319089551 .done2,#mermaid-1765319089551 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1765319089551 .doneText0,#mermaid-1765319089551 .doneText1,#mermaid-1765319089551 .doneText2,#mermaid-1765319089551 .doneText3{fill:#000!important}#mermaid-1765319089551 .crit0,#mermaid-1765319089551 .crit1,#mermaid-1765319089551 .crit2,#mermaid-1765319089551 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1765319089551 .activeCrit0,#mermaid-1765319089551 .activeCrit1,#mermaid-1765319089551 .activeCrit2,#mermaid-1765319089551 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1765319089551 .doneCrit0,#mermaid-1765319089551 .doneCrit1,#mermaid-1765319089551 .doneCrit2,#mermaid-1765319089551 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1765319089551 .activeCritText0,#mermaid-1765319089551 .activeCritText1,#mermaid-1765319089551 .activeCritText2,#mermaid-1765319089551 .activeCritText3,#mermaid-1765319089551 .doneCritText0,#mermaid-1765319089551 .doneCritText1,#mermaid-1765319089551 .doneCritText2,#mermaid-1765319089551 .doneCritText3{fill:#000!important}#mermaid-1765319089551 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1765319089551 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1765319089551 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1765319089551 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1765319089551 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1765319089551 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1765319089551 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1765319089551 #compositionEnd,#mermaid-1765319089551 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1765319089551 #aggregationEnd,#mermaid-1765319089551 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1765319089551 #dependencyEnd,#mermaid-1765319089551 #dependencyStart,#mermaid-1765319089551 #extensionEnd,#mermaid-1765319089551 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1765319089551 .branch-label,#mermaid-1765319089551 .commit-id,#mermaid-1765319089551 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1765319089551 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>

<hr />

<h2 id="tweets">Tweets</h2>

<p>An example of displaying a tweet:</p>
<div class="jekyll-twitter-plugin"><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>

<p>An example of pulling from a timeline:</p>
<div class="jekyll-twitter-plugin"><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a></p>

<hr />

<h2 id="blockquotes">Blockquotes</h2>

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote>

<hr />

<h2 id="layouts">Layouts</h2>

<p>The main text column is referred to as the body.
It is the assumed layout of any direct descendants of the <code class="language-plaintext highlighter-rouge">d-article</code> element.</p>

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

<p>For images you want to display a little larger, try <code class="language-plaintext highlighter-rouge">.l-page</code>:</p>

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

<p>All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:</p>

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

<p>Occasionally you’ll want to use the full browser width.
For this, use <code class="language-plaintext highlighter-rouge">.l-screen</code>.
You can also inset the element a little from the edge of the browser by using the inset variant.</p>

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

<p>The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of <code class="language-plaintext highlighter-rouge">.l-body</code>-sized text except on mobile screen sizes.</p>

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>

<hr />

<h2 id="other-typography">Other Typography?</h2>

<p>Emphasis, aka italics, with <em>asterisks</em> (<code class="language-plaintext highlighter-rouge">*asterisks*</code>) or <em>underscores</em> (<code class="language-plaintext highlighter-rouge">_underscores_</code>).</p>

<p>Strong emphasis, aka bold, with <strong>asterisks</strong> or <strong>underscores</strong>.</p>

<p>Combined emphasis with <strong>asterisks and <em>underscores</em></strong>.</p>

<p>Strikethrough uses two tildes. <del>Scratch this.</del></p>

<ol>
  <li>First ordered list item</li>
  <li>Another item
    <ul>
      <li>Unordered sub-list.</li>
    </ul>
  </li>
  <li>Actual numbers don’t matter, just that it’s a number
    <ol>
      <li>Ordered sub-list</li>
    </ol>
  </li>
  <li>
    <p>And another item.</p>

    <p>You can have properly indented paragraphs within list items. Notice the blank line above, and the leading spaces (at least one, but we’ll use three here to also align the raw Markdown).</p>

    <p>To have a line break without a paragraph, you will need to use two trailing spaces.
Note that this line is separate, but within the same paragraph.
(This is contrary to the typical GFM line break behavior, where trailing spaces are not required.)</p>
  </li>
</ol>

<ul>
  <li>Unordered lists can use asterisks</li>
  <li>Or minuses</li>
  <li>Or pluses</li>
</ul>

<p><a href="https://www.google.com">I’m an inline-style link</a></p>

<p><a href="https://www.google.com" title="Google's Homepage">I’m an inline-style link with title</a></p>

<p><a href="https://www.mozilla.org">I’m a reference-style link</a></p>

<p><a href="../blob/master/LICENSE">I’m a relative reference to a repository file</a></p>

<p><a href="http://slashdot.org">You can use numbers for reference-style link definitions</a></p>

<p>Or leave it empty and use the <a href="http://www.reddit.com">link text itself</a>.</p>

<p>URLs and URLs in angle brackets will automatically get turned into links. 
http://www.example.com or <a href="http://www.example.com">http://www.example.com</a> and sometimes 
example.com (but not on Github, for example).</p>

<p>Some text to show that the reference links can follow later.</p>

<p>Here’s our logo (hover to see the title text):</p>

<p>Inline-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 1" /></p>

<p>Reference-style: 
<img src="https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png" alt="alt text" title="Logo Title Text 2" /></p>

<p>Inline <code class="language-plaintext highlighter-rouge">code</code> has <code class="language-plaintext highlighter-rouge">back-ticks around</code> it.</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kd">var</span> <span class="nx">s</span> <span class="o">=</span> <span class="dl">"</span><span class="s2">JavaScript syntax highlighting</span><span class="dl">"</span><span class="p">;</span>
<span class="nf">alert</span><span class="p">(</span><span class="nx">s</span><span class="p">);</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Python syntax highlighting</span><span class="sh">"</span>
<span class="nf">print</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>No language indicated, so no syntax highlighting. 
But let's throw in a &lt;b&gt;tag&lt;/b&gt;.
</code></pre></div></div>

<p>Colons can be used to align columns.</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>

<p>There must be at least 3 dashes separating each header cell.
The outer pipes (|) are optional, and you don’t need to make the 
raw Markdown line up prettily. You can also use inline Markdown.</p>

<table>
  <thead>
    <tr>
      <th>Markdown</th>
      <th>Less</th>
      <th>Pretty</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><em>Still</em></td>
      <td><code class="language-plaintext highlighter-rouge">renders</code></td>
      <td><strong>nicely</strong></td>
    </tr>
    <tr>
      <td>1</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Blockquotes are very handy in email to emulate reply text.
This line is part of the same quote.</p>
</blockquote>

<p>Quote break.</p>

<blockquote>
  <p>This is a very long line that will still be quoted properly when it wraps. Oh boy let’s keep writing to make sure this is long enough to actually wrap for everyone. Oh, you can <em>put</em> <strong>Markdown</strong> into a blockquote.</p>
</blockquote>

<p>Here’s a line for us to start with.</p>

<p>This line is separated from the one above by two newlines, so it will be a <em>separate paragraph</em>.</p>

<p>This line is also a separate paragraph, but…
This line is only separated by a single newline, so it’s a separate line in the <em>same paragraph</em>.</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry><entry><title type="html">Sample Blog Post (HTML version)</title><link href="https://data-brain-mind.github.io/tutorials/distill-example2/" rel="alternate" type="text/html" title="Sample Blog Post (HTML version)" /><published>2025-04-28T00:00:00+08:00</published><updated>2025-04-28T00:00:00+08:00</updated><id>https://data-brain-mind.github.io/tutorials/distill-example2</id><content type="html" xml:base="https://data-brain-mind.github.io/tutorials/distill-example2/"><![CDATA[<p>
  This is a sample blog post written in HTML (while the other <a href="/tutorials/distill-example/">sample post</a> is written in Markdown). Authors have the choice to write in HTML or Markdown. While Markdown is easier to write, HTML gives you more control over the layout of your post. Furthermore, Markdown often interacts in unexpected ways with MathJax and other HTML widgets. If you are having trouble with Markdown, try writing in HTML instead.
</p>

<p>
  Note: please use the table of contents as defined in the front matter rather than the traditional markdown styling.
</p>

<h2 id="equations">Equations</h2>

<p>This theme supports rendering beautiful math in inline and display modes using <a href="https://www.mathjax.org/">MathJax 3</a> engine.
You just need to surround your math expression with <code>$$</code>, like <code>$$ E = mc^2 $$</code>.
If you leave it inside a paragraph, it will produce an inline expression, just like \(E = mc^2\).</p>

<p>To use display mode, again surround your expression with <code>$$</code> and place it as a separate paragraph.
Here is an example:
$$
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
$$
</p>

<p>Note that MathJax 3 is <a href="https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html">a major re-write of MathJax</a> 
that brought a significant improvement to the loading and rendering speed, which is now 
<a href="http://www.intmath.com/cg5/katex-mathjax-comparison.php">on par with KaTeX</a>.</p>

<h2 id="images-and-figures">Images and Figures</h2>

<p>Its generally a better idea to avoid linking to images hosted elsewhere - links can break and you
might face losing important information in your blog post.
You can display images from this repository using the following code:</p>

<pre><code>{% include figure.html path="assets/img/2025-04-28-distill-example/iclr.png" class="img-fluid" %}</code></pre>

<p>which results in the following image:</p>

<figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/iclr-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/iclr.png"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>



<p>
  To ensure that there are no namespace conflicts, you must save your asset to your unique directory
  `/assets/img/2025-04-28-[SUBMISSION NAME]` within your submission.  
</p>

<p>
  Please avoid using the direct HTML method of embedding images; they may not be properly resized.
  Some below complex ways to load images (note the different styles of the shapes/shadows):  
</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/9-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/9.jpg"
      class="img-fluid rounded z-depth-1"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg"
      class="img-fluid rounded z-depth-1"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>
<div class="caption">
    A simple, elegant caption looks good between image rows, after each row, or doesn't have to be there at all.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/8-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/8.jpg"
      class="img-fluid z-depth-2"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/10-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/10.jpg"
      class="img-fluid z-depth-2"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/11-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/11.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/12-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/12.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
    <div class="col-sm mt-3 mt-md-0">
        <figure>

  <picture>
    
    <source 
        class="responsive-img-srcset"
        media="(max-width: 480px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-480.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 800px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-800.webp"
      />
    <source 
        class="responsive-img-srcset"
        media="(max-width: 1400px)" 
        srcset="/tutorials/assets/img/2025-04-28-distill-example/7-1400.webp"
      />
    

    <!-- Fallback to the original file -->
    <img 
      src="/tutorials/assets/img/2025-04-28-distill-example/7.jpg"
      class="img-fluid"  
      width="auto" 
      height="auto" 
       
       
       
       
       
       
      
      onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
    />

  </picture>

</figure>

    </div>
</div>

<h3>Interactive Figures</h3>

<p>
  Here's how you could embed interactive figures that have been exported as HTML files.
  Note that we will be using plotly for this demo, but anything built off of HTML should work.
  All that's required is for you to export your figure into HTML format, and make sure that the file
  exists in the `assets/html/[SUBMISSION NAME]/` directory in this repository's root directory.
  To embed it into any page, simply insert the following code anywhere into your page.  
</p>

<pre><code>{% include [FIGURE_NAME].html %}</code></pre>

<p>
For example, the following code can be used to generate the figure underneath it.
</p>

<pre><code class="language-python">import pandas as pd
import plotly.express as px

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/earthquakes-23k.csv')

fig = px.density_mapbox(
    df, lat='Latitude', lon='Longitude', z='Magnitude', radius=10,
    center=dict(lat=0, lon=180), zoom=0, mapbox_style="stamen-terrain")
fig.show()

fig.write_html('./assets/html/2025-04-28-distill-example/plotly_demo_1.html')
</code></pre>

And then include it with the following:

<pre><code class="language-html">&lt;div class="l-page"&gt;
  &lt;iframe src="{{ 'assets/html/2025-04-28-distill-example/plotly_demo_1.html' | relative_url }}" frameborder='0' scrolling='no' height="600px" width="100%"&gt;&lt;/iframe&gt;
&lt;/div&gt;
</code></pre>

Voila!

<div class="l-page">
  <iframe src="/tutorials/assets/html/2025-04-28-distill-example/plotly_demo_1.html" frameborder='0' scrolling='no' height="600px" width="100%"></iframe>
</div>


<h2 id="citations">Citations</h2>


<p>
  Citations are then used in the article body with the <code>&lt;d-cite&gt;</code> tag.
    The key attribute is a reference to the id provided in the bibliography.
    The key attribute can take multiple ids, separated by commas.    
</p>

<p>
  The citation is presented inline like this: <d-cite key="gregor2015draw"></d-cite> (a number that displays more information on hover).
  If you have an appendix, a bibliography is automatically created and populated in it.  
</p>

<p>
  Distill chose a numerical inline citation style to improve readability of citation dense articles and because many of the benefits of longer citations are obviated by displaying more information on hover.
  However, we consider it good style to mention author last names if you discuss something at length and it fits into the flow well - the authors are human and it's nice for them to have the community associate them with their work.  
</p>


<h2 id="footnotes">Footnotes</h2>

<p>
  Just wrap the text you would like to show up in a footnote in a <code>&lt;d-footnote&gt;</code> tag.
    The number of the footnote will be automatically generated.<d-footnote>This will become a hoverable footnote.</d-footnote>
</p>


<h2 id="code-blocks">Code Blocks</h2>

<p>
  This theme implements a built-in Jekyll feature, the use of Rouge, for syntax highlighting.
  It supports more than 100 languages.
  This example is in C++.
  All you have to do is wrap your code in a liquid tag as follows:  
</p>

<pre><code>
{% highlight c++ linenos %}  <br/> code code code <br/> {% endhighlight %}

</code></pre>

The keyword `linenos` triggers display of line numbers. You can try toggling it on or off yourself below:

<figure class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
<span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span></code></pre></figure>



<h2 id="diagrams">Diagrams</h2>

<p>
  This theme supports generating various diagrams from a text description using <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> plugin.
  Below, we generate a few examples of such diagrams using languages such as <a href="http://mermaid.js.org/">mermaid</a>, <a href="https://plantuml.com/">plantuml</a>, <a href="https://vega.github.io/vega-lite/">vega-lite</a>, etc.  
</p>

<p>
  <b>Note</b>different diagram-generation packages require external dependencies to be installed on your machine.
  Also, be mindful of that because of diagram generation the first time you build your Jekyll website after adding new diagrams will be SLOW.
  For any other details, please refer to the <a href="https://github.com/zhustec/jekyll-diagrams">jekyll-diagrams</a> README.  
</p>

<p>
  <b>Note:</b> This is not supported for local rendering!
</p>

<p>
  The diagram below was generated by the following code:
</p>

<pre><code>{% mermaid %}
sequenceDiagram
    participant John
    participant Alice
    Alice->>John: Hello John, how are you?
    John-->>Alice: Great!
{% endmermaid %}

</code></pre>

<div class='jekyll-diagrams diagrams mermaid'>
  <svg id="mermaid-1765319090190" width="100%" xmlns="http://www.w3.org/2000/svg" height="100%" style="max-width:450px;" viewBox="-50 -10 450 231"><style>#mermaid-1765319090190 .label{font-family:trebuchet ms,verdana,arial;color:#333}#mermaid-1765319090190 .node circle,#mermaid-1765319090190 .node ellipse,#mermaid-1765319090190 .node polygon,#mermaid-1765319090190 .node rect{fill:#ececff;stroke:#9370db;stroke-width:1px}#mermaid-1765319090190 .node.clickable{cursor:pointer}#mermaid-1765319090190 .arrowheadPath{fill:#333}#mermaid-1765319090190 .edgePath .path{stroke:#333;stroke-width:1.5px}#mermaid-1765319090190 .edgeLabel{background-color:#e8e8e8}#mermaid-1765319090190 .cluster rect{fill:#ffffde!important;stroke:#aa3!important;stroke-width:1px!important}#mermaid-1765319090190 .cluster text{fill:#333}#mermaid-1765319090190 div.mermaidTooltip{position:absolute;text-align:center;max-width:200px;padding:2px;font-family:trebuchet ms,verdana,arial;font-size:12px;background:#ffffde;border:1px solid #aa3;border-radius:2px;pointer-events:none;z-index:100}#mermaid-1765319090190 .actor{stroke:#ccf;fill:#ececff}#mermaid-1765319090190 text.actor{fill:#000;stroke:none}#mermaid-1765319090190 .actor-line{stroke:grey}#mermaid-1765319090190 .messageLine0{marker-end:"url(#arrowhead)"}#mermaid-1765319090190 .messageLine0,#mermaid-1765319090190 .messageLine1{stroke-width:1.5;stroke-dasharray:"2 2";stroke:#333}#mermaid-1765319090190 #arrowhead{fill:#333}#mermaid-1765319090190 #crosshead path{fill:#333!important;stroke:#333!important}#mermaid-1765319090190 .messageText{fill:#333;stroke:none}#mermaid-1765319090190 .labelBox{stroke:#ccf;fill:#ececff}#mermaid-1765319090190 .labelText,#mermaid-1765319090190 .loopText{fill:#000;stroke:none}#mermaid-1765319090190 .loopLine{stroke-width:2;stroke-dasharray:"2 2";marker-end:"url(#arrowhead)";stroke:#ccf}#mermaid-1765319090190 .note{stroke:#aa3;fill:#fff5ad}#mermaid-1765319090190 .noteText{fill:#000;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:14px}#mermaid-1765319090190 .section{stroke:none;opacity:.2}#mermaid-1765319090190 .section0{fill:rgba(102,102,255,.49)}#mermaid-1765319090190 .section2{fill:#fff400}#mermaid-1765319090190 .section1,#mermaid-1765319090190 .section3{fill:#fff;opacity:.2}#mermaid-1765319090190 .sectionTitle0,#mermaid-1765319090190 .sectionTitle1,#mermaid-1765319090190 .sectionTitle2,#mermaid-1765319090190 .sectionTitle3{fill:#333}#mermaid-1765319090190 .sectionTitle{text-anchor:start;font-size:11px;text-height:14px}#mermaid-1765319090190 .grid .tick{stroke:#d3d3d3;opacity:.3;shape-rendering:crispEdges}#mermaid-1765319090190 .grid path{stroke-width:0}#mermaid-1765319090190 .today{fill:none;stroke:red;stroke-width:2px}#mermaid-1765319090190 .task{stroke-width:2}#mermaid-1765319090190 .taskText{text-anchor:middle;font-size:11px}#mermaid-1765319090190 .taskTextOutsideRight{fill:#000;text-anchor:start;font-size:11px}#mermaid-1765319090190 .taskTextOutsideLeft{fill:#000;text-anchor:end;font-size:11px}#mermaid-1765319090190 .taskText0,#mermaid-1765319090190 .taskText1,#mermaid-1765319090190 .taskText2,#mermaid-1765319090190 .taskText3{fill:#fff}#mermaid-1765319090190 .task0,#mermaid-1765319090190 .task1,#mermaid-1765319090190 .task2,#mermaid-1765319090190 .task3{fill:#8a90dd;stroke:#534fbc}#mermaid-1765319090190 .taskTextOutside0,#mermaid-1765319090190 .taskTextOutside1,#mermaid-1765319090190 .taskTextOutside2,#mermaid-1765319090190 .taskTextOutside3{fill:#000}#mermaid-1765319090190 .active0,#mermaid-1765319090190 .active1,#mermaid-1765319090190 .active2,#mermaid-1765319090190 .active3{fill:#bfc7ff;stroke:#534fbc}#mermaid-1765319090190 .activeText0,#mermaid-1765319090190 .activeText1,#mermaid-1765319090190 .activeText2,#mermaid-1765319090190 .activeText3{fill:#000!important}#mermaid-1765319090190 .done0,#mermaid-1765319090190 .done1,#mermaid-1765319090190 .done2,#mermaid-1765319090190 .done3{stroke:grey;fill:#d3d3d3;stroke-width:2}#mermaid-1765319090190 .doneText0,#mermaid-1765319090190 .doneText1,#mermaid-1765319090190 .doneText2,#mermaid-1765319090190 .doneText3{fill:#000!important}#mermaid-1765319090190 .crit0,#mermaid-1765319090190 .crit1,#mermaid-1765319090190 .crit2,#mermaid-1765319090190 .crit3{stroke:#f88;fill:red;stroke-width:2}#mermaid-1765319090190 .activeCrit0,#mermaid-1765319090190 .activeCrit1,#mermaid-1765319090190 .activeCrit2,#mermaid-1765319090190 .activeCrit3{stroke:#f88;fill:#bfc7ff;stroke-width:2}#mermaid-1765319090190 .doneCrit0,#mermaid-1765319090190 .doneCrit1,#mermaid-1765319090190 .doneCrit2,#mermaid-1765319090190 .doneCrit3{stroke:#f88;fill:#d3d3d3;stroke-width:2;cursor:pointer;shape-rendering:crispEdges}#mermaid-1765319090190 .activeCritText0,#mermaid-1765319090190 .activeCritText1,#mermaid-1765319090190 .activeCritText2,#mermaid-1765319090190 .activeCritText3,#mermaid-1765319090190 .doneCritText0,#mermaid-1765319090190 .doneCritText1,#mermaid-1765319090190 .doneCritText2,#mermaid-1765319090190 .doneCritText3{fill:#000!important}#mermaid-1765319090190 .titleText{text-anchor:middle;font-size:18px;fill:#000}#mermaid-1765319090190 g.classGroup text{fill:#9370db;stroke:none;font-family:trebuchet ms,verdana,arial;font-size:10px}#mermaid-1765319090190 g.classGroup rect{fill:#ececff;stroke:#9370db}#mermaid-1765319090190 g.classGroup line{stroke:#9370db;stroke-width:1}#mermaid-1765319090190 .classLabel .box{stroke:none;stroke-width:0;fill:#ececff;opacity:.5}#mermaid-1765319090190 .classLabel .label{fill:#9370db;font-size:10px}#mermaid-1765319090190 .relation{stroke:#9370db;stroke-width:1;fill:none}#mermaid-1765319090190 #compositionEnd,#mermaid-1765319090190 #compositionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1765319090190 #aggregationEnd,#mermaid-1765319090190 #aggregationStart{fill:#ececff;stroke:#9370db;stroke-width:1}#mermaid-1765319090190 #dependencyEnd,#mermaid-1765319090190 #dependencyStart,#mermaid-1765319090190 #extensionEnd,#mermaid-1765319090190 #extensionStart{fill:#9370db;stroke:#9370db;stroke-width:1}#mermaid-1765319090190 .branch-label,#mermaid-1765319090190 .commit-id,#mermaid-1765319090190 .commit-msg{fill:#d3d3d3;color:#d3d3d3}</style><style>#mermaid-1765319090190 {
    color: rgb(0, 0, 0);
    font: normal normal 400 normal 16px / normal "Times New Roman";
  }</style><g></g><g><line id="actor0" x1="75" y1="5" x2="75" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="0" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><line id="actor1" x1="275" y1="5" x2="275" y2="220" class="actor-line" stroke-width="0.5px" stroke="#999"></line><rect x="200" y="0" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="32.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g><defs><marker id="arrowhead" refX="5" refY="2" markerWidth="6" markerHeight="4" orient="auto"><path d="M 0,0 V 4 L6,2 Z"></path></marker></defs><defs><marker id="crosshead" markerWidth="15" markerHeight="8" orient="auto" refX="16" refY="4"><path fill="black" stroke="#000000" stroke-width="1px" d="M 9,2 V 6 L16,4 Z" style="stroke-dasharray: 0, 0;"></path><path fill="none" stroke="#000000" stroke-width="1px" d="M 0,1 L 6,7 M 6,1 L 0,7" style="stroke-dasharray: 0, 0;"></path></marker></defs><g><text x="175" y="93" class="messageText" style="text-anchor: middle;">Hello John, how are you?</text><line x1="275" y1="100" x2="75" y2="100" class="messageLine0" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="fill: none;"></line></g><g><text x="175" y="128" class="messageText" style="text-anchor: middle;">Great!</text><line x1="75" y1="135" x2="275" y2="135" class="messageLine1" stroke-width="2" stroke="black" marker-end="url(#arrowhead)" style="stroke-dasharray: 3, 3; fill: none;"></line></g><g><rect x="0" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="75" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="75" dy="0">John</tspan></text></g><g><rect x="200" y="155" fill="#eaeaea" stroke="#666" width="150" height="65" rx="3" ry="3" class="actor"></rect><text x="275" y="187.5" dominant-baseline="central" alignment-baseline="central" class="actor" style="text-anchor: middle;"><tspan x="275" dy="0">Alice</tspan></text></g></svg>
</div>



<h2 id="tweets">Tweets</h2>

<p>
  An example of displaying a tweet:
  <div class='jekyll-twitter-plugin'><blockquote class="twitter-tweet"><p lang="sv" dir="ltr">jekyll-twitter-plugin (1.0.0): A Liquid tag plugin for Jekyll that renders Tweets from Twitter API <a href="http://t.co/m4EIQPM9h4">http://t.co/m4EIQPM9h4</a></p>&mdash; RubyGems (@rubygems) <a href="https://twitter.com/rubygems/status/518821243320287232?ref_src=twsrc%5Etfw">October 5, 2014</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

</div>  
</p>

<p>
  An example of pulling from a timeline:
  <div class='jekyll-twitter-plugin'><a class="twitter-timeline" data-width="500" data-tweet-limit="3" href="https://twitter.com/jekyllrb?ref_src=twsrc%5Etfw">Tweets by jekyllrb</a>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>  
</p>

<p>
  For more details on using the plugin visit: <a href="https://github.com/rob-murray/jekyll-twitter-plugin">jekyll-twitter-plugin</a> 
</p>


<h2 id="blockquotes">Blockquotes</h2>

<blockquote>
    We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
    —Anais Nin
</blockquote>


<h2 id="layouts">Layouts</h2>

The main text column is referred to as the body.
It's the assumed layout of any direct descendants of the `d-article` element.

<div class="fake-img l-body">
  <p>.l-body</p>
</div>

For images you want to display a little larger, try `.l-page`:

<div class="fake-img l-page">
  <p>.l-page</p>
</div>

All of these have an outset variant if you want to poke out from the body text a little bit.
For instance:

<div class="fake-img l-body-outset">
  <p>.l-body-outset</p>
</div>

<div class="fake-img l-page-outset">
  <p>.l-page-outset</p>
</div>

Occasionally you'll want to use the full browser width.
For this, use `.l-screen`.
You can also inset the element a little from the edge of the browser by using the inset variant.

<div class="fake-img l-screen">
  <p>.l-screen</p>
</div>
<div class="fake-img l-screen-inset">
  <p>.l-screen-inset</p>
</div>

The final layout is for marginalia, asides, and footnotes.
It does not interrupt the normal flow of `.l-body`-sized text except on mobile screen sizes.

<div class="fake-img l-gutter">
  <p>.l-gutter</p>
</div>


<h2 id="other-typography">Other Typography?</h2>

<p>
  Emphasis, aka italics, with the <code>&lt;i&gt;&lt;/i&gt;</code> tag <i>emphasis</i>.
</p>

<p>
  Strong emphasis, aka bold, with <code>&lt;b&gt;&lt;/b&gt;</code> tag <b>bold</b>.
</p>

<p>
  Strikethrough ca be accomplished with the <code>&lt;s&gt;&lt;/s&gt;</code> tag. <s>Scratch this.</s>
</p>

<ul>
  <li>First ordered list item</li>
  <li>Another item</li>
  <ol>
    <li>Unordered sub-list. </li>
  </ol>
  <li>And another item.</li>
</ul>



<p>
  For code, the language can be specified in the class. For example, use <q>language-javascript</q> for Javascript and <q>language-python</q> for Python code.
</p>

<pre><code class="language-javascript">var s = "JavaScript syntax highlighting";
  alert(s);</code></pre>
 
<pre><code class="language-python">s = "Python syntax highlighting"
  print(s)</code></pre>

<pre><code class="language-python">No language indicated, so no syntax highlighting.</code></pre>

<p>
  A table can be created with the <code>&lt;table&gt;</code> element. Below is an example
</p>

<table>
  <thead>
    <tr>
      <th>Tables</th>
      <th style="text-align: center">Are</th>
      <th style="text-align: right">Cool</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>col 3 is</td>
      <td style="text-align: center">right-aligned</td>
      <td style="text-align: right">$1600</td>
    </tr>
    <tr>
      <td>col 2 is</td>
      <td style="text-align: center">centered</td>
      <td style="text-align: right">$12</td>
    </tr>
    <tr>
      <td>zebra stripes</td>
      <td style="text-align: center">are neat</td>
      <td style="text-align: right">$1</td>
    </tr>
  </tbody>
</table>


<p>
  <blockquote>Blockquotes can be defined with the &gt;blockquote&lt; tag.</blockquote>
</p>]]></content><author><name>Albert Einstein</name></author><summary type="html"><![CDATA[Your blog post's abstract. Please add your abstract or summary here and not in the main body of your text. Do not include math/latex or hyperlinks.]]></summary></entry></feed>