<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),o=a[0][0],r=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"November 24, 2025"),i="An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute",l="";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(o+"2025"+i.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${i}},\n  abstract = {${l}},\n  booktitle = {NeurIPS 2025 Workshop on "Data on the Brain & Mind" Tutorials Track},\n  year = {2025},\n  date = {${r}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]);e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0];let t=`\n${e}, "${i}", NeurIPS 2025 Workshop on "Data on the Brain & Mind" Tutorials Track, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute | Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</title> <meta name="author" content=" "> <meta name="description" content="Home to the Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025) "> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/tutorials/assets/img/brain-icon.svg"> <link rel="stylesheet" href="/tutorials/assets/css/main.css"> <link rel="canonical" href="https://data-brain-mind.github.io/tutorials/an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/tutorials/assets/js/theme.js"></script> <script src="/tutorials/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/tutorials/assets/js/distillpub/template.v2.js"></script> <script src="/tutorials/assets/js/distillpub/transforms.v2.js"></script> <script src="/tutorials/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">{
      "title": "An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute",
      "description": "",
      "published": "November 24, 2025",
      "authors": [
        {
          "author": "Corbett Bennett",
          "authorURL": "https://alleninstitute.org/person/corbett-bennett/",
          "affiliations": [
            {
              "name": "Allen Institute",
              "url": ""
            }
          ]
        },
        {
          "author": "Su-Yee J Lee",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Allen Institute",
              "url": ""
            }
          ]
        },
        {
          "author": "Josh Siegle",
          "authorURL": "",
          "affiliations": [
            {
              "name": "Allen Institute",
              "url": ""
            }
          ]
        },
        {
          "author": "Marina Garrett",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "Saskia EJ de Vries",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "Shawn R Olsen",
          "authorURL": "https://scholar.google.com/citations?user=huDkgmYAAAAJ&hl=en",
          "affiliations": [
            {
              "name": "Allen Institute",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/tutorials/">Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/tutorials/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/call/">call for tutorial</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/blog/index.html">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>An Overview of the Neuropixels Visual Behavior Dataset From the Allen Institute</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction">Introduction</a></div> </nav> </d-contents> <p>The following tutorial will provide a brief introduction to this dataset then demonstrate how to load an example session and do some simple analysis. We’ll cover the following topics:</p> <ul> <li>Introduction to the experiment</li> <li>Accessing the dataset</li> <li>Reading the project metadata</li> <li>Loading an experiment for analysis</li> <li>Plotting neural activity aligned to stimuli</li> <li>Using optotagging to infer cell type</li> <li>Accessing LFP data</li> <li>Analyzing behavioral data during the visual change detection task</li> </ul> <h2 id="introduction-to-the-experiment">Introduction to the experiment</h2> <p>Our ability to perceive the sensory environment and flexibly interact with the world requires the coordinated action of neuronal populations distributed throughout the brain. To further our understanding of the neural basis of behavior, the Visual Behavior project leveraged the Allen Brain Observatory pipeline (diagrammed below; gray panels refer to the companion <a href="https://portal.brain-map.org/circuits-behavior/visual-behavior-2p" rel="external nofollow noopener noopener noreferrer" target="_blank">Visual Behavior Optical Physiology dataset</a>) to collect a large-scale, highly standardized dataset consisting of recordings of neural activity in mice that have learned to perform a visual chang detection task. This dataset can be used to investigate how patterns of spiking activity across the visual cortex and thalamus are related to behavior and also how these activity dynamics are influenced by task-engagement and prior visual experience. </p> <p>The Visual Behavior Neuropixels dataset includes 153 sessions from 81 mice. These data are made openly accessible, with all recorded timeseries, behavioral events, and experimental metadata conveniently packaged in Neurodata Without Borders (NWB) files that can be accessed and analyzed using our open Python software package, the  <a href="https://github.com/AllenInstitute/AllenSDK" rel="external nofollow noopener noopener noreferrer" target="_blank">AllenSDK</a>.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline-1400.webp"></source> <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/pipeline.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="the-visual-change-detection-task">The visual change detection task</h3> <p>The Visual Behavior Optical Physiology and Visual Behavior Neuropixels projects are built upon a change detection behavioral task. Briefly, in this go/no-go task, mice are shown a continuous series of briefly presented visual images and they earn water rewards by correctly reporting when the identity of the image changes (diagrammed below). Five percent of images are omitted, allowing for analysis of expectation signals.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task-1400.webp"></source> <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/change_detection_task.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="neuropixels-recordings-throughout-visual-cortex-and-thalamus">Neuropixels recordings throughout visual cortex and thalamus</h3> <p>This dataset includes multi-regional Neuropixels recordings from up to 6 probes at once. The probes target six visual cortical areas including VISp, VISl, VISal, VISrl, VISam, and VISpm. In addition, multiple subcortical areas are also typically measured, including visual thalamic areas LGd and LP as well as units in the hippocampus and midbrain. Note that for the first release, NWB files will include spike times for units recorded in these structures, but LFP data will not be available.</p> <p>Recordings were made in three genotypes: C57BL6J, Sst-IRES-Cre; Ai32, and Vip-IRES-Cre; Ai32. By crossing Sst and Vip lines to the Ai32 ChR2 reporter mouse, we were able to activate putative Sst+ and Vip+ cortical interneurons by stimulating the cortical surface with blue light during an optotagging protocol at the end of each session.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-1400.webp"></source> <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <h3 id="investigating-the-impact-of-stimulus-novelty-on-neural-responses-and-behavior">Investigating the impact of stimulus novelty on neural responses and behavior</h3> <p>To allow analysis of stimulus novelty on neural responses and behavior, two different images sets were used in the recording sessions: G and H (diagrammed below). Both image sets comprised 8 natural images. Two images were shared across the two image sets (purple in diagram), enabling within session analysis of novelty effects. Mice took one of the following three trajectories through training and the two days of recording:</p> <p>1) Train on G; see G on the first recording day; see H on the second recording day</p> <p>2) Train on G; see H on the first recording day; see G on the second recording day</p> <p>3) Train on H; see H on the first recording day; see G on the second recording day</p> <p>The numbers in the <em>Training and Recording Workflow</em> bubble below give the total recording sessions of each type in the dataset.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy-1400.webp"></source> <img src="/tutorials/assets/img/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute/recording_strategy.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><a href="https://anonymous.4open.science/r/neurips-vbn-tutorial-7B18/vbn_neurips_tutorial.ipynb" rel="external nofollow noopener noopener noreferrer" target="_blank">Tutorial link</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/tutorials/assets/bibliography/2025-11-24-an-overview-of-the-neuropixels-visual-behavior-dataset-from-the-allen-institute.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>