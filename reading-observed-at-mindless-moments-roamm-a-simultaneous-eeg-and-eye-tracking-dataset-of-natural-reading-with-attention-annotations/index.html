<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>let thunk=()=>{let e=e=>e.trim(),t=e=>e.innerText,n=e=>{let t=e.split(" "),n=t.slice(0,-1).join(" ");return[t.at(-1),n]},a=Array.from(document.getElementsByClassName("author")).map(t).map(e).map(n),i=a[0][0],o=(Array.from(document.getElementsByClassName("affiliation")).filter(e=>"P"===e.nodeName).map(t).map(e),"November 24, 2025"),r="Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations",l="ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes.";{let e=a.map(e=>`${e[0]}, ${e[1]}`).join(" and "),t=`\n@inproceedings{${(i+"2025"+r.split(" ").slice(0,3).join("")).replace(" ","").replace(/[\p{P}$+<=>^`|~]/gu,"").toLowerCase().trim()},\n  author = {${e}},\n  title = {${r}},\n  abstract = {${l}},\n  booktitle = {NeurIPS 2025 Workshop on "Data on the Brain & Mind" Tutorials Track},\n  year = {2025},\n  date = {${o}},\n  note = {${window.location.href}},\n  url  = {${window.location.href}}\n}\n  `.trim();document.getElementById("bibtex-box").innerText=t}{let e=a.map(e=>e[0]);e=e.length>2?e[0]+", et al.":2==e.length?e[0]+" & "+e[1]:e[0];let t=`\n${e}, "${r}", NeurIPS 2025 Workshop on "Data on the Brain & Mind" Tutorials Track, 2025.\n`.trim();document.getElementById("bibtex-academic-attribution").innerText=t}};document.addEventListener("readystatechange",function(){"complete"===document.readyState&&thunk()});</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations | Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</title> <meta name="author" content=" "> <meta name="description" content="ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes."> <meta name="keywords" content="machine-learning, ml, deep-learning, reinforcement-learning"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/tutorials/assets/img/brain-icon.svg"> <link rel="stylesheet" href="/tutorials/assets/css/main.css"> <link rel="canonical" href="https://data-brain-mind.github.io/tutorials/reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/tutorials/assets/js/theme.js"></script> <script src="/tutorials/assets/js/dark_mode.js"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/tutorials/assets/js/distillpub/template.v2.js"></script> <script src="/tutorials/assets/js/distillpub/transforms.v2.js"></script> <script src="/tutorials/assets/js/distillpub/overrides.js"></script> <d-front-matter> <script async type="text/json">{
      "title": "Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations",
      "description": "ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes.",
      "published": "November 24, 2025",
      "authors": [
        {
          "author": "Haorui Sun",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of Vermont",
              "url": ""
            }
          ]
        },
        {
          "author": "Ardyn Vivienne Olszko",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of Vermont",
              "url": ""
            }
          ]
        },
        {
          "author": "Yida Chen",
          "authorURL": "https://yc015.github.io/",
          "affiliations": [
            {
              "name": "Harvard University, Harvard University",
              "url": ""
            }
          ]
        },
        {
          "author": "Hazen Kellner",
          "authorURL": "",
          "affiliations": [
            {
              "name": "University of Vermont",
              "url": ""
            }
          ]
        },
        {
          "author": "Niharika Singh",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "Lincoln Lewisequerre",
          "authorURL": "",
          "affiliations": [
            {
              "name": "",
              "url": ""
            }
          ]
        },
        {
          "author": "David C. Jangraw",
          "authorURL": "https://www.uvm.edu/~brainlab",
          "affiliations": [
            {
              "name": "University of Vermont",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> </head> <body class="fixed-top-nav"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/tutorials/">Data on the Brain &amp; Mind Tutorial Track (NeurIPS 2025)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/tutorials/about/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/call/">call for tutorial</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/submitting/">submitting</a> </li> <li class="nav-item "> <a class="nav-link" href="/tutorials/blog/index.html">blog</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="post distill"> <d-title> <h1>Reading Observed At Mindless Moments (ROAMM): A Simultaneous EEG and Eye-Tracking Dataset of Natural Reading with Attention Annotations</h1> <p>ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div><a href="#introduction-and-motivation">Introduction and motivation</a></div> <ul> <li><a href="#neural-decoding-models-from-science-fiction-to-reality">Neural decoding models: From science fiction to reality</a></li> <li><a href="#simulatenous-eeg-and-eye-tracking-for-cognitive-dataset">Simulatenous EEG and eye-tracking for cognitive dataset</a></li> <li><a href="#what-are-some-challenges">What are some challenges?</a></li> <li><a href="#mind-wandering-is-sneaky-so-how-do-we-study-it">Mind-wandering is sneaky, so how do we study it?</a></li> <li><a href="#introduce-roamm-dataset">Introduce ROAMM dataset</a></li> </ul> <div><a href="#roamm-dataset">ROAMM dataset</a></div> <ul> <li><a href="#participants">Participants</a></li> <li><a href="#the-remind-paradigm">The ReMind paradigm</a></li> <li><a href="#data-acquisition-and-preprocessing">Data acquisition and preprocessing</a></li> <li><a href="#dataset-format">Dataset format</a></li> <li><a href="#dataset-scale">Dataset scale</a></li> <li><a href="#data-validation">Data validation</a></li> <li><a href="#data-accessibility-and-availability">Data accessibility and availability</a></li> <li><a href="#how-to-use-roamm">How to use ROAMM</a></li> </ul> <div><a href="#open-questions-for-ml-practitioners-using-roamm">Open questions for ML practitioners using ROAMM</a></div> <ul> <li><a href="#learn-shared-representation-from-eeg-and-eye-tracking">Learn shared representation from EEG and eye-tracking</a></li> <li><a href="#build-a-momentary-human-attention-decoder">Build a momentary human attention decoder</a></li> <li><a href="#is-human-attention-what-we-need-for-neural-decoding">Is human attention what we need for neural decoding?</a></li> <li><a href="#use-eeg-eye-tracking-human-attention-and-reading-text-to-predict-comprehension">Use EEG, eye-tracking, human attention, and reading text to predict comprehension</a></li> </ul> <div><a href="#conclusions">Conclusions</a></div> </nav> </d-contents> <h2 id="tldr">TL;DR</h2> <p><strong>ROAMM (Reading Observed At Mindless Moments) is a large-scale multimodal dataset of EEG and eye-tracking data during naturalistic reading, with precise annotations of mind-wandering episodes to advance research in attention, language, and machine learning.</strong></p> <h2 id="1-introduction-and-motivation">1. Introduction and motivation</h2> <h3 id="11-neural-decoding-models-from-science-fiction-to-reality">1.1 Neural decoding models: from science fiction to reality</h3> <p>Many of us have seen movies or read stories where people can “mind read,” such as Professor X, an exceptionally powerful telepath who can read and control the thoughts of others. While this belongs to science fiction, advances in technology and computation are bringing us closer to decoding aspects of human thought in real life. Neural decoding models powered by modern natural language processing and large language models have begun to approximate how humans process and generate language. These models, in turn, have helped researchers to understand how human brains accomplish the same tasks.</p> <h3 id="12-simultaneous-eeg-and-eye-tracking-for-cognitive-dataset">1.2 Simultaneous EEG and eye-tracking for cognitive dataset</h3> <p>Building and refining powerful neural decoding models require large-scale and high-quality cognitive data. A powerful way to capture such cognitive data is to combine eye-tracking with electroencephalography (EEG). Eye-tracking captures gaze positions, which link visual input to specific task stimuli or words at given time. EEG, meanwhile, provides non-invasive and relatively low-cost recordings of brain dynamics with high temporal resolution.</p> <h3 id="13-what-are-some-challenges">1.3 What are some challenges?</h3> <h4 id="131-the-rarity-of-datasets-on-natural-reading">1.3.1 The rarity of datasets on natural reading</h4> <p>Most existing simultaneous EEG and eye-tracking datasets focus on non-linguistic tasks. For example, EEGEyeNet provides large-scale recordings of eye movements while participants view simple visual symbols <d-cite key="Kastrati2021"></d-cite>, and EEGT-RSOD <d-cite key="He2025"></d-cite> records brain and eye activity as participants search for targets in remote sensing images. For reading specifically, the most well-known datasets are ZuCo <d-cite key="Hollenstein2018"></d-cite> and ZuCo 2.0 <d-cite key="Hollenstein2019"></d-cite>, which combine EEG and eye-tracking during sentence-level reading (i.e., each stimulus is a single sentence). These natural (like) reading resources have become popular and invaluable for advancing EEG-to-text decoding and related computational models.</p> <h4 id="132-frequent-mind-wandering-off-task-thoughts-during-reading">1.3.2 Frequent mind-wandering (off-task thoughts) during reading</h4> <p>You might argue that existing datasets could be sufficient for neural decoding tasks if we apply data augmentation techniques. But there’s another, even more fundamental challenge: human attention as a confunding factor. Just as “Attention is all you need” in computational models <d-cite key="Vaswani2017"></d-cite>, attention is also critical in humans. Our attention is not static: it fluctuates with arousal, fatigue, mood, and motivation <d-cite key="Shen2024, Smallwood2009"></d-cite>, and these states alter how we engage with language and modulate behaviors and neural activity <d-cite key="Smallwood2011, Unsworth2013"></d-cite>. One particularly common and impactful attention state is <strong>mind-wandering (MW)</strong>, when our focus drifts from the task at hand to unrelated thoughts. <strong>People spend 30% to 60% of their daily lives mind-wandering</strong> <d-cite key="Killingsworth2010"></d-cite>, and it happens frequently during reading <d-cite key="Smallwood2011"></d-cite>. You might even find yourself mind-wandering while reading this blog post. While the exact cognitive processes behind MW are still unclear, one leading hypothesis, the perceptual decoupling hypothesis <d-cite key="Smallwood2006"></d-cite>, suggests that internal thoughts during MW divert resources away from external stimuli. This diversion can directly affect how language is processed. For these reasons, accounting for MW is not just interesting; it is essential for building models that can mechanistically and predictively capture real human language processing.</p> <h3 id="14-mind-wandering-is-sneaky-so-how-do-we-study-it">1.4 Mind-wandering is sneaky, so how do we study it?</h3> <h4 id="141-previous-approaches-and-their-drawbacks">1.4.1 Previous approaches and their drawbacks</h4> <p>Now let’s take a brief detour into psychology. Studying constructs like MW can include unique challenges. Prior research has primarily relied on self-reports and thought probes to detect episodes of MW. Self-reports depend on participants’ subjective awareness: whenever individuals realize that their minds have drifted from the task, they are instructed to report it <d-cite key="Schooler2002"></d-cite>. Thought probes, by contrast, are random prompts that require participants to indicate whether they are currently on-task or mind-wandering <d-cite key="Giambra1995, Smallwood2006"></d-cite>. While both approaches are widely used in contexts such as reading <d-cite key="Reichle2010, Broadway2015, Faber2017"></d-cite>, they only mark when MW ends, making it difficult to capture its onset or to characterize how episodes naturally unfold over time, temporal features that are crucial for understanding and detecting MW.</p> <h4 id="142-the-remind-paradigm">1.4.2 The ReMind paradigm</h4> <p>So how did we solve this problem? I spent days reading research articles, and my own wandering mind made me reread passages over and over just to comprehend the material. Suddenly, it hit me: when we reread after getting lost, we are implicitly marking the parts of the text where our attention drifted. That observation inspired the ReMind paradigm, which estimates the onset and duration of MW episodes during reading by combining retrospective self-reports with eye-tracking. Participants indicate the words where they believe their mind started and stopped wandering for each episode. We then align these selections with gaze timestamps to estimate precise onset and offset times.</p> <p><img src="assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/intro_pic.png" alt="intro_pic"> <em>Figure created by Sora.</em></p> <h3 id="15-introduce-roamm-dataset">1.5 Introduce ROAMM dataset</h3> <p>Using this approach, we created the Reading Observed at Mindless Moments (ROAMM) dataset, which contains simultaneous EEG and eye-tracking data from 44 participants (30 females, 9 males, and 5 non-binary) performing naturalistic reading in the ReMind paradigm. The dataset includes rich labels, such as the word at each fixation, attention state at each sample, and a comprehension question score for each page. By capturing attention states in a naturalistic and uninterrupted way, ROAMM provides a powerful resource for studying questions in language and cognition, and for developing language-based machine learning models that better reflect real human attention.</p> <p>In the remainder of the post, we present the ROAMM Dataset including the participant demographics, task, data acquisition and pre-processing, and methodology for identifying mind wandering. We include figures of data validation that demonstrate the quality and temporal dynamics of the data. Finally, we describe the availability of the data and discuss potential applications including open questions for ML practitioners and examples of candidate models.</p> <h2 id="2-roamm-dataset">2. ROAMM dataset</h2> <h3 id="21-participants">2.1 Participants</h3> <p>We recruited 58 participants from a university in the northeastern United State who were fluent in English and reported no family history of neurological disorders or epilepsy. All participants underwent screening and provided informed consent before participation. The study protocol was approved by the university Institutional Review Board. 14 participants were excluded due to issues such as equipment difficulties, incomplete experimental runs, monocular-only eye-tracking data, or missing demographic information. The final dataset includes <strong>44 participants</strong>. The participants’ age ranged from 18 to 64 years (Mean = 22.6, SD = 7.8, Median = 20, Mode = 19). This indicates a relatively young but moderately varied sample. Our sample also includes labels for gender, handedness, and self-identified ADHD.</p> <p><img src="demographics.png" alt="demographics" style="zoom:45%;"></p> <h3 id="22-the-remind-paradigm">2.2 The ReMind paradigm</h3> <p>Participants read five articles selected from Wikipedia (2015): <strong>Pluto</strong> (the dwarf planet), <strong>the Prisoner’s Dilemma</strong>, <strong>Serena Williams</strong>, <strong>the History of Film</strong>, and <strong>the Voynich Manuscript</strong>. These topics were chosen to be unfamiliar yet comprehensible without prior background knowledge. Each article was standardized by removing images and jargon, then divided into 10 pages (≈220 words per page). Pages were rendered using a custom Python script into 16 lines of black Courier-font text on a gray background. To encourage engagement and assess comprehension, we created one multiple-choice question per page. Each question was designed to require attention to that page alone.</p> <p><img src="assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/remind_task.png" alt="remind_task"></p> <p>The reading task was programmed using PsychoPy <d-cite key="Peirce2019"></d-cite>, a platform for developing psychological experiments. Each experimental session consisted of five runs, one for each article. Articles were presented in a randomized order. Before the first run, participants received task instructions and an explicit definition of mind-wandering (<em>see above figure</em>). Participants read at their own pace with no time limit per page but could not re-read previous pages. If they noticed themselves mind-wandering, they pressed the “F” key to access a dedicated reporting screen. There, they clicked on the words marking where they believed the MW episode began and ended (highlighted onscreen for clarity). If the episode began on the prior page, they marked the first word of the current page. After submitting the report, they returned to the same page to resume reading. For consistency, only one MW report was permitted per page.</p> <h3 id="23-data-acquisition-and-preprocessing">2.3 Data acquisition and preprocessing</h3> <h4 id="231-eye-tracking">2.3.1 Eye-tracking</h4> <p>The experiment was conducted in a soundproof booth to minimize distractions. We used an <strong>SR Research EyeLink 1000 Plus eye tracker</strong> to record binocular eye movements and pupil area at 1000 Hz. Each run began with calibration, repeated until EyeLink reported good accuracy (worst error &lt; 1.5°, average error &lt; 1.0°). Once calibration was complete, we recorded eye movements in sync with EEG while participants performed the reading task. PsychoPy sent page-onset triggers to both systems to keep the data streams aligned.</p> <p>EyeLink automatically detected fixations, saccades, and blinks using default thresholds. These events were parsed into data frames, and fixations were mapped to individual words on the screen using spatial coordinates. Because pupil size data can be unreliable around blinks (due to eyelid occlusion), we corrected for this by applying linear interpolation.</p> <h4 id="232-eeg">2.3.2 EEG</h4> <p>We recorded simultaneous EEG using a <strong>BioSemi ActiveTwo 64-channel system</strong> at 2048 Hz. Before starting the task, we ensured all electrodes had stable connections by checking impedances and correcting any channels with unusually high values. Collected data were preprocessed in EEGLAB <d-cite key="Delorme2004"></d-cite>: resampled to 256 Hz, re-referenced, filtered (0.5–50 Hz), and bad channels interpolated. Eye and muscle artifacts were removed using independent component analysis (ICA) with standard EEGLAB parameters.</p> <h3 id="24-dataset-format">2.4 Dataset format</h3> <p>We made the ROAMM dataset easy to work with by aligning eye-tracking data to 64-channel EEG at 256 Hz. We downsampled the eye-tracking data using the real-time arrays: for each EEG time point, we identified the closest corresponding eye-tracking sample and used the pupil size at that moment. Fixations, saccades, and blinks were directly mapped using their start and end times relative to the EEG time array.</p> <p>All data are stored in pandas DataFrames (.pkl format, [<figure></figure></p> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/figure_1-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/figure_1-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/figure_1-1400.webp"></source> <img src="/tutorials/assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/figure_1.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <p>&lt;/figure&gt; ](https://www.python.org/downloads/)) for fast loading and smaller file size (compared to .csv format). Each participant has one .pkl file per run, with a total of 5 runs. Eye-tracking events like fixations, saccades, and blinks are expanded across their start-to-end times. For example, if a fixation occurs from 10 to 11 seconds, all samples within that 1-second window are annotated with <code class="language-plaintext highlighter-rouge">is_fixation = 1</code>, <code class="language-plaintext highlighter-rouge">fix_tStart = 10</code>, <code class="language-plaintext highlighter-rouge">fix_tEnd = 11</code>, <code class="language-plaintext highlighter-rouge">fix_duration = 1</code>, etc. Time stamps, page boundaries, and mind-wandering episodes are all included, along with metadata such as sampling frequency, run numbers, page numbers, and story names.</p> <p>To maintain clarity and focus on natural reading, we defined <strong>first-pass reading</strong> as the period when participants were initially reading the text. Activities such as reading task instructions, marking mind-wandering pages, rereading after a mind-wandering report, or answering comprehension questions were excluded from this category. Each sample is labeled for first-pass reading, mind-wandering, and fixated words. Each fixated word also includes a key linking it to the original text, making it easy to generate embeddings or other vectorized representations within the context of the reading corpus for computational modeling. Additional information, including <strong>subject demographic information</strong> and <strong>comprehension question scores</strong> (with corresponding run and page numbers), and <strong>EEG channel locations</strong> is saved in separate files for easy access.</p> <table> <thead> <tr> <th>Data Category</th> <th>Column Count</th> <th>Description</th> <th>Example Columns</th> </tr> </thead> <tbody> <tr> <td><strong>EEG</strong></td> <td>64</td> <td>64 electrode EEG signals</td> <td><code class="language-plaintext highlighter-rouge">Fp1, AF7, AF3, F1, F3...</code></td> </tr> <tr> <td><strong>Eye-Tracking</strong></td> <td>38</td> <td>Gaze position, pupil size, fixations, saccades, and blinks</td> <td><code class="language-plaintext highlighter-rouge">is_fixation, fix_eye, fix_tStart, fix_tEnd, fix_duration...</code></td> </tr> <tr> <td><strong>Time</strong></td> <td>8</td> <td>Timestamps, page boundaries, durations, and MW episode info</td> <td><code class="language-plaintext highlighter-rouge">time, page_start, page_end, page_dur, mw_onset...</code></td> </tr> <tr> <td><strong>Others</strong></td> <td>4</td> <td>Sampling frequency, run and page numbers, story name</td> <td><code class="language-plaintext highlighter-rouge">sfreq, page_num, run_num, story_name</code></td> </tr> <tr> <td><strong>Labels</strong></td> <td>4</td> <td>First-pass reading, MW, and fixated word annotations</td> <td><code class="language-plaintext highlighter-rouge">first_pass_reading, is_mw, fix_fixed_word, fix_fixed_word_key</code></td> </tr> </tbody> </table> <h3 id="25-dataset-scale">2.5 Dataset scale</h3> <p>The ROAMM dataset is large and rich. It contains over 46 million recorded samples from 44 participants, totaling more than 50 hours of simultaneous EEG and eye-tracking data. Of these, over 26 million samples (around 30 hours) correspond to first-pass reading, which includes fixated word information at each sample. Across the 2,200 pages read, participants reported 998 mind-wandering episodes, corresponding to 45.4% of the pages. These episodes had a median duration of 5.92 seconds and a mean of 7.79 seconds and resulted in a total of 2.2 hours of reading time annotated as mind-wandering.</p> <table> <thead> <tr> <th>Data Type</th> <th>Total Sample Count</th> <th>Total Time (h)</th> <th>Subject Avg Time (m)</th> </tr> </thead> <tbody> <tr> <td><strong>Total Recording</strong></td> <td>46,371,584</td> <td>50.3</td> <td>68.6</td> </tr> <tr> <td><strong>First-Pass Reading</strong></td> <td>26,691,014</td> <td>29.0</td> <td>39.5</td> </tr> <tr> <td><strong>Mind-Wandering</strong></td> <td>2,045,021</td> <td>2.2</td> <td>3.0</td> </tr> <tr> <td><strong>Fixation</strong></td> <td>38,324,700</td> <td>41.6</td> <td>56.7</td> </tr> <tr> <td><strong>Saccade</strong></td> <td>9,326,633</td> <td>10.1</td> <td>13.8</td> </tr> <tr> <td><strong>Blink</strong></td> <td>2,290,418</td> <td>2.5</td> <td>3.4</td> </tr> </tbody> </table> <p>The histograms below illustrate the distribution of data across participants for each attribute in the ROAMM dataset. While all participants contributed, individual differences are evident in the distributions. This highlights the real-world variability in human data and underscores the importance of carefully considering modeling approaches, whether developing a general model across participants or an individualized classifier tailored to each person.</p> <p><img src="data_scale.png" alt="data_scale" style="zoom:20%;"></p> <h3 id="26-data-validation">2.6 Data validation</h3> <p>We validated the ROAMM dataset to ensure high recording quality, precise alignment between EEG and eye-tracking data streams, accurate fixation-to-word mappings, and reliable labeling of MW episodes.</p> <h4 id="261-eeg-and-eye-tracking-recoding-quality">2.6.1 EEG and eye-tracking recoding quality</h4> <p>To assess recording quality, we inspected EEG and eye-tracking signals in parallel. As demonstrated below using a randomly selected 10-second window, preprocessed EEG from selected channels show <strong>clean activity with minimal muscle and eye artifacts</strong>. Eye-tracking features behave as expected: <strong>fixations are followed by saccades, blinks appeared distinctly</strong>, and <strong>gaze position traces reveal typical reading patterns</strong>. Specifically, x-coordinates increase left to right across each line, while y-coordinates step down across successive lines, confirming naturalistic line-by-line reading.</p> <p><img src="assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/eeg_eye_valid.png" alt="eeg_eye_valid"></p> <h4 id="262-eeg-and-eye-tracking-alignment">2.6.2 EEG and eye-tracking alignment</h4> <p>Next, we validated alignment between EEG and eye-tracking streams. Using the unfold toolbox, we deconvolved fixation-related potentials (FRPs) during periods of reading without MW. <strong>The resulting FRPs and P1 topography replicated patterns reported using the ZuCo 2.0 dataset</strong> <d-cite key="Hollenstein2019"></d-cite>, providing strong evidence for the temporal precision of our co-registered recordings.</p> <p><img src="assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/frp.png" alt="frp"></p> <h4 id="263-fixation-to-word-mapping">2.6.3 Fixation-to-word mapping</h4> <p>We also validated fixation-to-word mappings by plotting gaze traces directly on reading pages. In one example, a participant read mindfully without reporting MW; in another, the same participant reported an MW episode. Onset and offset words of the MW episode were highlighted in red, while fixations appeared as colored dots. Larger dots indicated longer durations, and a purple-to-yellow gradient reflected temporal order. Fixations within MW episodes were additionally center-colored in red, and consecutive fixations were linked by red lines to mark saccades. <strong>All fixations aligned neatly with words, and the gaze traces showed clear left-to-right reading flows, confirming the accuracy of fixation-to-word mapping.</strong></p> <p><img src="assets/img/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations/reading_page_full.png" alt="reading_page_full"></p> <h4 id="264-reliable-mw-onset">2.6.4 Reliable MW onset</h4> <p>Finally, we validated MW onset labeling. In a paper currently under review, we demonstrated that incorporating MW onset information significantly improves the performance of linear regression classifiers trained to detect MW from eye-tracking features. A sliding-window analysis not only replicated prior findings of reduced fixation rates during MW episodes <d-cite key="Reichle2010"></d-cite>, but also revealed that these changes begin precisely at the reported MW onset. These findings demonstrate that the <strong>ReMind paradigm provides a powerful framework for capturing MW onset and its progression over time</strong>, ensuring that our attention state annotations are precise and grounded in reliable MW onset information.</p> <p><img src="eye_mwonset.png" alt="eye_mwonset" style="zoom:40%;"></p> <h3 id="27-data-accessibility-and-availability">2.7 Data accessibility and availability</h3> <p>The processed datasets are publicly available on the <a href="https://osf.io/kmvgb/?view_only=688b268d5b784ff39eba5b73bc10171e" rel="external nofollow noopener noopener noreferrer" target="_blank">OSF</a>. Due to their large size, raw datasets are not hosted online but are available upon request. All preprocessing scripts used to generate the processed datasets are available in the <a href="https://anonymous.4open.science/r/ROAMM-6E8C/README.md" rel="external nofollow noopener noopener noreferrer" target="_blank">GitHub repository</a>.</p> <h3 id="28-how-to-use-roamm">2.8 How to use ROAMM</h3> <p>We put a lot of effort into making ROAMM easy to use, even if you’ve never worked with EEG or eye-tracking data before. Everything is pre-aligned and stored in <strong>pandas DataFrames</strong>, so you can load it with just a few lines of Python.</p> <p>Once you download the dataset from <a href="https://osf.io/kmvgb/?view_only=688b268d5b784ff39eba5b73bc10171e" rel="external nofollow noopener noopener noreferrer" target="_blank">OSF</a>, here’s how you can get started:</p> <p><strong>1. Import packages and set up paths</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span>
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># define data root
# this is the path to the ROAMM folder on local machine
</span><span class="n">roamm_root</span> <span class="o">=</span> <span class="sa">r</span><span class="sh">"</span><span class="s">/Users/~/ROAMM/</span><span class="sh">"</span> <span class="c1"># change this to your path
</span><span class="n">ml_data_root</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">roamm_root</span><span class="p">,</span> <span class="sh">'</span><span class="s">subject_ml_data</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p><strong>2. Load a single run for one subject</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">subject_id</span> <span class="o">=</span> <span class="sh">'</span><span class="s">s10014</span><span class="sh">'</span>
<span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
<span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">'</span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s">_run</span><span class="si">{</span><span class="n">run_number</span><span class="si">}</span><span class="s">_ml_data.pkl</span><span class="sh">'</span><span class="p">))</span>
</code></pre></div></div> <p><strong>3. Load all runs for one subject</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>
<span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
    <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
    <span class="n">df_sub_all_runs</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df_sub_all_runs</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
</code></pre></div></div> <p><strong>4. Load all subjects, filtered to first-pass reading</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load all runs for all subjects
</span><span class="n">all_subjects</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">)</span> <span class="k">if</span> <span class="n">d</span><span class="p">.</span><span class="nf">startswith</span><span class="p">(</span><span class="sh">'</span><span class="s">s</span><span class="sh">'</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">d</span><span class="p">))]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">subject_id</span> <span class="ow">in</span> <span class="n">all_subjects</span><span class="p">:</span>
    <span class="n">subject_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ml_data_root</span><span class="p">,</span> <span class="n">subject_id</span><span class="p">)</span>
    <span class="n">pkl_files</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span> <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">os</span><span class="p">.</span><span class="nf">listdir</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">)</span> <span class="k">if</span> <span class="n">f</span><span class="p">.</span><span class="nf">endswith</span><span class="p">(</span><span class="sh">'</span><span class="s">.pkl</span><span class="sh">'</span><span class="p">)]</span>

    <span class="c1"># make sure each subject has 5 runs of data
</span>    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has </span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">pkl_files</span><span class="p">)</span><span class="si">}</span><span class="s"> runs instead of 5</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">pkl_file</span> <span class="ow">in</span> <span class="n">pkl_files</span><span class="p">:</span>
        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_pickle</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">subject_dir</span><span class="p">,</span> <span class="n">pkl_file</span><span class="p">))</span>
        <span class="c1"># I highly recommend you to filter out reading runs that are not the first pass reading
</span>        <span class="c1"># to save memory
</span>        <span class="n">df_sub_single_run</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>
        <span class="c1"># add subject id to the dataframe   
</span>        <span class="n">df_sub_single_run</span><span class="p">[</span><span class="sh">'</span><span class="s">subject_id</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="n">subject_id</span>
        <span class="c1"># convert bool col explicitly to avoid pandas warning
</span>        <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">is_blink</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_saccade</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_fixation</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">is_mw</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">first_pass_reading</span><span class="sh">'</span><span class="p">]:</span>
            <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_sub_single_run</span><span class="p">[</span><span class="n">col</span><span class="p">]</span> <span class="o">==</span> <span class="bp">True</span>
        <span class="c1"># append to the dataframe
</span>        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">df_sub_single_run</span><span class="p">])</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Subject </span><span class="si">{</span><span class="n">subject_id</span><span class="si">}</span><span class="s"> has been loaded.</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <h2 id="3-open-questions-for-ml-practitioners-using-roamm">3. Open questions for ML practitioners using ROAMM</h2> <p>The ROAMM dataset is rich in scope, combining multiple valuable modalities: <strong>eye-tracking</strong> (gaze position and pupil size), <strong>brain signals</strong> (i.e., EEG), <strong>human attention states</strong>, and <strong>linguistic content</strong> (the reading text itself). This multimodal design provides countless opportunities for machine learning practitioners to explore how these signals interact. Below, we highlight 4 open questions that showcase the potential of ROAMM for advancing both cognitive science and computational modeling.</p> <p><img src="roamm_modalities.png" alt="roamm_modalities" style="zoom:40%;"></p> <h3 id="31-learn-shared-representation-from-eeg-and-eye-tracking">3.1 Learn shared representation from EEG and eye-tracking</h3> <blockquote> <b>The eyes are the windows to the soul.</b> — William Shakespeare, “King Richard III” Act V, Sc.3 </blockquote> <p>Eyes, particularly pupil size, reveal much about internal cognitive states and arousal <d-cite key="Castellotti2025"></d-cite>. With a dataset at the scale of ROAMM, we can now ask whether eye-tracking and EEG share common patterns of variance that allow them to be tightly linked.</p> <p>One way to explore this question is to use the established multi-modal embedding method like CLIP <d-cite key="Radford2021"></d-cite>, which was originally applied to learn joint representations of images and text via contrastive learning. A <strong>CLIP-style</strong> model could be trained on ROAMM by contrasting matched and mismatched EEG and eye-tracking segments. If two modalities indeed have features that share significant variance, the resulting embeddings would enable accurate cross-modal classification. Beyond classification, shared representation opens the door to conditional decoding from one modality to the other. For example, one can train a <strong>diffusion transformer model (DiT)</strong> <d-cite key="Peebles2022"></d-cite> to reconstruct a subject’s EEG signals using eye-tracking data as a generative prior. Since there exists a shared representation space of EEG and eye-tracking, the DiT can fuse the information from eye-tracking data through cross-attention.</p> <h3 id="32-build-a-momentary-human-attention-decoder">3.2 Build a momentary human attention decoder</h3> <p>EEG and eye-tracking are not the only modalities in ROAMM that may exhibit strong associations. Previous studies have trained models to detect attention states from EEG and eye-tracking separately. We hypothesize that, when combined together, they can provide a more reliable estimate of a subject’s attention state during reading.</p> <p>To evaluate this hypothesis, one could train simple classifiers such as regression models, SVM, gradient boosting, or neural models that takes the shared representations of EEG and eye-tracking at a given moment to predict the subject’s attention state at that same time step. However, attention is not merely a transient experience; it unfolds dynamically over time. Thus, signals from a single moment are unlikely to provide sufficient information for accurate prediction. To capture attention’s temporal dynamics, one can train sequential models like <strong>long short-term memory networks</strong>, <strong>temporal convolutional networks</strong>, or <strong>generative transformers</strong>, which have demonstrated superior performance in modeling text data. These models can be trained to predict future attention states from EEG, eye-tracking, and attention states from both current and prior windows, enabling moment-to-moment attention decoding.</p> <h3 id="33-is-human-attention-what-we-need-for-neural-decoding">3.3 Is human attention what we need for neural decoding?</h3> <p>Decoding neural signals has become a popular topic, with several studies attempting to use EEG to decode text <d-cite key="Liu2024, Wang2024"></d-cite>. However, their main focus was on the attention mechanisms in transformers, but they largely neglected fluctuations in human attention during the task. This can be problematic: during mind-wandering, readers still maintain the outward behavior of reading, moving their eyes from left to right and line by line, but their visual input is disrupted by internal thoughts. Cognitive resources that should be allocated to word recognition and comprehension are instead consumed by spontaneous mental activity. In other words, the brain itself cannot fully follow what the eyes are reading during mind-wandering. This raises a fascinating question: <strong>does knowledge of human attention states improve neural decoding performance?</strong></p> <p>A straightforward way to address this question is to train an EEG2text decoder separately on data segments from normal reading versus mind-wandering. <strong>Performance differences between these conditions would reveal whether filtering out periods of “mindless reading” provides a decoding advantage.</strong> If we observe better performance when training only on attentive reading, the result would be intuitive: how could a model recover information that the brain itself fails to process?</p> <p>However, the more intriguing possibility is if decoding performance remains similar regardless of attention state. This would suggest that information about the text is encoded at lower levels of the visual or sensory hierarchy. In such a scenario, the model may be able to extract signals from early visual or pre-attentive neural activity that are not available to conscious awareness. This opens up provocative implications: <strong>machine learning models could potentially reveal implicit or subliminal processing of linguistic information in the brain.</strong> In other words, this is not a mind-reader but something at another level: a <strong>subconscious-reader</strong> that can uncover information from your brain even when you are not aware. It’s as if your neurons are whispering secrets that only the model can hear.</p> <p>Thus, testing whether attention modulates neural decoding performance not only has practical implications for building better brain–computer interfaces, but also addresses fundamental cognitive neuroscience questions about the boundary between unconscious encoding and conscious comprehension.</p> <h3 id="34-use-eeg-eye-tracking-human-attention-and-reading-text-to-predict-comprehension">3.4 Use EEG, eye-tracking, human attention, and reading text to predict comprehension</h3> <p>The previous questions focused on the link between pairs of modalities in our dataset. But as the saying goes, <em>“only children make choices, adults want it all!”</em> For machine learning experts who are not satisfied with pairwise associations and eager to showcase the full power of multimodal modeling, the next challenge is to use all available modalities from ROAMM: EEG, eye-tracking, attention states, and reading text itself. The task we propose is to predict reading comprehension, using ROAMM’s page-level comprehension scores. While page-level labels are coarse and may not perfectly reflect moment-to-moment understanding, they still provide a valuable proxy for comprehension that can anchor multimodal learning.</p> <p>This problem requires complex model architectures and training frameworks that can integrate heterogeneous data streams. One can opt for the <strong>traditional fusion methods like early fusion</strong> in which features from all modalities are concatenated and processed jointly within a single parameterized model. Although considered a traditional technique, early fusion remains prevalent in recent large-scale multimodal systems for text and images (e.g., <d-cite key="ChameleonTeam2024, Lin2024"></d-cite>) (early fusion via concatenation, late fusion via ensembling, or hybrid fusion). Besides fusion, we can also train individual models to embed each modality. Examples of this <strong>late fusion include CLIP and Imagine Bind</strong> <d-cite key="Radford2021, Girdhar2023"></d-cite> which trains transformer encoders to map multi-modal data across into an embedding space. Downstream tasks, such as comprehension prediction, can be done by training lightweight classification on top of the shared embeddings. When applying late fusion on ROAMM, one can follow the existing practice, using the same architecture to embed data from all modalities. Alternatively, they can use specific architecture with inductive bias that accommodates the invariance present in the data (e.g., graph neural network (GNN) for EEG data <d-cite key="Klepl2023"></d-cite>).</p> <p>A model that successfully predicts comprehension from this rich multimodal space would not only advance cognitive modeling, but also contribute to the emerging field of Foundation Models for the Brain and Body (<em>Yes, another workshop hosted this year</em>). By integrating physiological, behavioral, and linguistic signals into a single predictive framework, we move closer to general-purpose models of human cognition. A recent Nature study <d-cite key="Binz2025"></d-cite> showes that large-scale multimodal learning can capture human behavior across a wide range of domains. Extending these ideas to ROAMM provides an opportunity to build neurocognitive foundation models during naturalistic reading environments.</p> <h2 id="4-conclusions">4. Conclusions</h2> <p>In this work, we introduced the <strong>Reading Observed At Mindless Moments (ROAMM)</strong> dataset, a large-scale, multimodal resource capturing simultaneous EEG and eye-tracking data during naturalistic reading. By using ReMind paradigm, ROAMM stands out among existing reading datasets by providing a highly naturalistic reading environment, temporally-resolved attention labels, and precise alignment between neural and behavioral signals.</p> <p>We provided an overview of the dataset’s acquisition, preprocessing, and structure, highlighting its <strong>scale, richness, and quality</strong>. Validation analyses confirmed <strong>high-fidelity recordings</strong>, <strong>accurate fixation-to-word mappings</strong>, and <strong>reliable labeling of mind-wandering episodes</strong>, making ROAMM suitable for rigorous cognitive and computational modeling.</p> <p>Beyond describing the dataset, we outlined a set of open questions that illustrate its potential for advancing both neuroscience and machine learning. These include 1) learning shared representations between EEG and eye-tracking, 2) building moment-to-moment attention decoders, 3) investigating the role of attention in neural decoding, and 4) predicting reading comprehension using fully multimodal data. ROAMM thus provides a unique opportunity to explore the interactions between brain, eye movements, attention, and language, enabling development of models that better reflect real human cognition.</p> <p>In summary, ROAMM not only offers a rich resource for fundamental research on attention and reading but also serves as a platform for developing advanced multimodal machine learning models. By bridging cognitive science and computational modeling, it paves the way toward neurocognitive foundation models capable of capturing complex and naturalistic human behavior.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> </div> <d-bibliography src="/tutorials/assets/bibliography/2025-11-24-reading-observed-at-mindless-moments-roamm-a-simultaneous-eeg-and-eye-tracking-dataset-of-natural-reading-with-attention-annotations.bib"></d-bibliography> <d-article id="bibtex-container" class="related highlight"> For attribution in academic contexts, please cite this work as <pre id="bibtex-academic-attribution">
        PLACEHOLDER FOR ACADEMIC ATTRIBUTION
  </pre> BibTeX citation <pre id="bibtex-box">
        PLACEHOLDER FOR BIBTEX
  </pre> </d-article> <script src="https://utteranc.es/client.js" repo="iclr-blogposts/2025" issue-term="pathname" theme="github-light" crossorigin="anonymous" async> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> </body> </html>